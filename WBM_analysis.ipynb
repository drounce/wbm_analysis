{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e866ed68-288a-486a-9966-93415486f962",
   "metadata": {},
   "source": [
    "# PyGEM-WBM Sea Level Analysis\n",
    "Analyze WBM to determine fraction of glacier mass loss/runoff that reaches ocean and contributes to sea-level rise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cartopy\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.patches import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.spatial import ConvexHull\n",
    "from shapely.geometry import Point\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e368c159-7b46-494e-abbb-90eceb7fdb2f",
   "metadata": {},
   "source": [
    "### Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a925473-bab9-4818-886b-292751c0e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = os.getcwd()\n",
    "\n",
    "basin_csv_fn = 'MERIT_plus_15min_v1_IDs_super.csv'\n",
    "basin_tif_fn = 'MERIT_plus_15min_v1_IDs_wcrs.tif'\n",
    "rgiids_wbasins_fn = 'RGI60_wMERIT_IDs.csv'\n",
    "\n",
    "rgi_shp_fn = '/Users/drounce/Documents/HiMAT/PyGEM-analysis/wbm_results/qgis/rgi60_all_simplified2_robinson.shp'\n",
    "rgi_regions_fn = '/Users/drounce/Documents/HiMAT/PyGEM-analysis/wbm_results/qgis/rgi60_regions_robinson-v2.shp'\n",
    "\n",
    "wbm_fp_primary = '/Volumes/TOSHIBA-WBM/NSLCT/'\n",
    "pygem_output_fp = '/Users/drounce/Documents/HiMAT/spc_backup/nsidc/glacier_stats/'\n",
    "\n",
    "results_fp = 'wbm_results/'\n",
    "if not os.path.exists(results_fp):\n",
    "    os.makedirs(results_fp)\n",
    "\n",
    "# scenarios = ['ssp126']\n",
    "scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "gcm_names = ['BCC-CSM2-MR', 'CESM2', 'CESM2-WACCM', 'EC-Earth3', 'EC-Earth3-Veg', 'FGOALS-f3-L', \n",
    "             'GFDL-ESM4', 'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM']\n",
    "# gcm_names = ['BCC-CSM2-MR', 'CESM2', 'CESM2-WACCM', 'EC-Earth3', 'EC-Earth3-Veg', 'FGOALS-f3-L', \n",
    "#              'GFDL-ESM4', 'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0']\n",
    "\n",
    "vns = ['discharge_m3s_pg', 'discharge', 'glMelt']\n",
    "\n",
    "# RGI data\n",
    "rgi_fp = main_directory + '/../RGI/rgi60/00_rgi60_attribs/'\n",
    "rgi_O1Id_colname = 'glacno'\n",
    "rgi_glacno_float_colname = 'RGIId_float'\n",
    "rgi_cols_drop = ['GLIMSId','BgnDate','EndDate','Status','Linkages']\n",
    "\n",
    "# Exorheic Option for fraction reaching ocean (1 = average over time period; 2 = individual values, capped to ensure realistic)\n",
    "  # - they result in the same answer\n",
    "  # However, should use option 2 because some basins (e.g., Ganges) see a drastic change in the fraction reaching the ocean over time.\n",
    "  # Hence, you want to captuion this.\n",
    "exor_frac_option = 2\n",
    "\n",
    "# Plotting details\n",
    "ssp_name_dict = {'ssp126':'SSP1-2.6',\n",
    "                 'ssp245':'SSP2-4.5',\n",
    "                 'ssp370':'SSP3-7.0',\n",
    "                 'ssp585':'SSP5-8.5',\n",
    "                }\n",
    "# ssp_colordict = {'ssp119':'#081d58', 'ssp126':'#1d91c0', 'ssp245':'#7fcdbb', 'ssp370':'#F47A20', 'ssp585':'#ED2024'}\n",
    "# ssp_colordict = {'ssp126':'#2D446B', 'ssp245':'#DE944B', 'ssp370':'#CD4644', 'ssp585':'#7E2825'} # IPCC AR6 colors\n",
    "ssp_colordict = {'ssp126':'#DE944B', 'ssp245':'#D25B4E', 'ssp370':'#9F2C2A', 'ssp585':'#7E2825'} # Yellow-Red\n",
    "\n",
    "rgi_reg_dict = {'all':'Global',\n",
    "                'global':'Global',\n",
    "                1:'Alaska',\n",
    "                2:'W Canada & US',\n",
    "                3:'Arctic Canada North',\n",
    "                4:'Arctic Canada South',\n",
    "                5:'Greenland Periphery',\n",
    "                6:'Iceland',\n",
    "                7:'Svalbard',\n",
    "                8:'Scandinavia',\n",
    "                9:'Russian Arctic',\n",
    "                10:'North Asia',\n",
    "                11:'Central Europe',\n",
    "                12:'Caucasus & Middle East',\n",
    "                13:'Central Asia',\n",
    "                14:'South Asia West',\n",
    "                15:'South Asia East',\n",
    "                16:'Low Latitudes',\n",
    "                17:'Southern Andes',\n",
    "                18:'New Zealand',\n",
    "                19:'Antarctic & Subantarctic'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754b6ad-4ee1-40c9-9dfa-0fde6d8dd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_to_mmSLE(mass, mass_bsl, rho_ice=900, rho_water=1000, area_ocean=362.5*1e12):\n",
    "    \"\"\" Calculate annual SLR accounting for the ice below sea level following Rounce et al. (2023), \n",
    "     which is consistent with OGGM (https://oggm.org/2021/03/04/slr-bsl/).\"\"\"\n",
    "    # Volume of water that will contribute to sea-level rise\n",
    "    vol_water = mass / rho_water \n",
    "    # Volume of ice (i.e., water displaced currently) that must be considered\n",
    "    vol_bsl_ice = mass_bsl / rho_ice\n",
    "\n",
    "    mass_mmSLE = (vol_water - vol_bsl_ice) / area_ocean * 1000\n",
    "\n",
    "    return mass_mmSLE\n",
    "\n",
    "def slr_mmSLEyr(reg_vol, reg_vol_bsl, rho_ice=900, rho_water=1000, area_ocean=362.5*1e12):\n",
    "    \"\"\" Calculate annual SLR accounting for the ice below sea level following Farinotti et al. (2019) \"\"\"\n",
    "    # Farinotti et al. (2019)\n",
    "#    reg_vol_asl = reg_vol - reg_vol_bsl\n",
    "#    return (-1*(reg_vol_asl[:,1:] - reg_vol_asl[:,0:-1]) * \n",
    "#            pygem_prms.density_ice / pygem_prms.density_water / pygem_prms.area_ocean * 1000)\n",
    "    \n",
    "    # OGGM new approach\n",
    "    if len(reg_vol.shape) == 2:\n",
    "        return (-1*(((reg_vol[:,1:] - reg_vol[:,0:-1]) * rho_ice / rho_water - \n",
    "                 (reg_vol_bsl[:,1:] - reg_vol_bsl[:,0:-1])) / area_ocean * 1000))\n",
    "    elif len(reg_vol.shape) == 3:\n",
    "        return (-1*(((reg_vol[:,:,1:] - reg_vol[:,:,0:-1]) * rho_ice / rho_water - \n",
    "                 (reg_vol_bsl[:,:,1:] - reg_vol_bsl[:,:,0:-1])) / area_ocean * 1000))\n",
    "    else:\n",
    "        assert 1==0, 'must code other shapes'\n",
    "        return 1\n",
    "\n",
    "def selectglaciersrgitable(glac_no=None, rgi_regionsO1=None, rgi_regionsO2='all', rgi_glac_number='all',\n",
    "                           rgi_fp=rgi_fp, \n",
    "                           rgi_cols_drop=rgi_cols_drop,\n",
    "                           rgi_O1Id_colname=rgi_O1Id_colname,\n",
    "                           rgi_glacno_float_colname=rgi_glacno_float_colname,\n",
    "                           indexname='GlaNo',\n",
    "                           include_landterm=True,include_laketerm=True,include_tidewater=True,\n",
    "                           glac_no_skip=None,\n",
    "                           min_glac_area_km2=0):\n",
    "    \"\"\"\n",
    "    Select all glaciers to be used in the model run according to the regions and glacier numbers defined by the RGI\n",
    "    glacier inventory. This function returns the rgi table associated with all of these glaciers.\n",
    "\n",
    "    glac_no : list of strings\n",
    "        list of strings of RGI glacier numbers (e.g., ['1.00001', '13.00001'])\n",
    "    rgi_regionsO1 : list of integers\n",
    "        list of integers of RGI order 1 regions (e.g., [1, 13])\n",
    "    rgi_regionsO2 : list of integers or 'all'\n",
    "        list of integers of RGI order 2 regions or simply 'all' for all the order 2 regions\n",
    "    rgi_glac_number : list of strings\n",
    "        list of RGI glacier numbers without the region (e.g., ['00001', '00002'])\n",
    "\n",
    "    Output: Pandas DataFrame of the glacier statistics for each glacier in the model run\n",
    "    (rows = GlacNo, columns = glacier statistics)\n",
    "    \"\"\"\n",
    "    if glac_no is not None:\n",
    "        glac_no_byregion = {}\n",
    "        rgi_regionsO1 = [int(i.split('.')[0]) for i in glac_no]\n",
    "        rgi_regionsO1 = list(set(rgi_regionsO1))\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = []\n",
    "        for i in glac_no:\n",
    "            region = i.split('.')[0]\n",
    "            glac_no_only = i.split('.')[1]\n",
    "            glac_no_byregion[int(region)].append(glac_no_only)\n",
    "\n",
    "        for region in rgi_regionsO1:\n",
    "            glac_no_byregion[region] = sorted(glac_no_byregion[region])\n",
    "\n",
    "    # Create an empty dataframe\n",
    "    rgi_regionsO1 = sorted(rgi_regionsO1)\n",
    "    glacier_table = pd.DataFrame()\n",
    "    for region in rgi_regionsO1:\n",
    "\n",
    "        if glac_no is not None:\n",
    "            rgi_glac_number = glac_no_byregion[region]\n",
    "\n",
    "#        if len(rgi_glac_number) < 50:\n",
    "\n",
    "        for i in os.listdir(rgi_fp):\n",
    "            if i.startswith(str(region).zfill(2)) and i.endswith('.csv'):\n",
    "                rgi_fn = i\n",
    "        try:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp + rgi_fn)\n",
    "        except:\n",
    "            csv_regionO1 = pd.read_csv(rgi_fp + rgi_fn, encoding='latin1')\n",
    "        \n",
    "        # Populate glacer_table with the glaciers of interest\n",
    "        if rgi_regionsO2 == 'all' and rgi_glac_number == 'all':\n",
    "            # print(\"All glaciers within region(s) %s are included in this model run.\" % (region))\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1\n",
    "            else:\n",
    "                glacier_table = pd.concat([glacier_table, csv_regionO1], axis=0)\n",
    "        elif rgi_regionsO2 != 'all' and rgi_glac_number == 'all':\n",
    "            # print(\"All glaciers within subregion(s) %s in region %s are included in this model run.\" %\n",
    "                  # (rgi_regionsO2, region))\n",
    "            for regionO2 in rgi_regionsO2:\n",
    "                if glacier_table.empty:\n",
    "                    glacier_table = csv_regionO1.loc[csv_regionO1['O2Region'] == regionO2]\n",
    "                else:\n",
    "                    glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[csv_regionO1['O2Region'] ==\n",
    "                                                                                regionO2]], axis=0))\n",
    "        else:\n",
    "            # if len(rgi_glac_number) < 20:\n",
    "            #     print(\"%s glaciers in region %s are included: %s\" % (len(rgi_glac_number), region, rgi_glac_number))\n",
    "            # else:\n",
    "            #     print(\"%s glaciers in region %s are included\" % (len(rgi_glac_number), region))\n",
    "                \n",
    "            rgiid_subset = ['RGI60-' + str(region).zfill(2) + '.' + x for x in rgi_glac_number] \n",
    "            rgiid_all = list(csv_regionO1.RGIId.values)\n",
    "            rgi_idx = [rgiid_all.index(x) for x in rgiid_subset if x in rgiid_all]\n",
    "            if glacier_table.empty:\n",
    "                glacier_table = csv_regionO1.loc[rgi_idx]\n",
    "            else:\n",
    "                glacier_table = (pd.concat([glacier_table, csv_regionO1.loc[rgi_idx]],\n",
    "                                           axis=0))\n",
    "                    \n",
    "    glacier_table = glacier_table.copy()\n",
    "    # reset the index so that it is in sequential order (0, 1, 2, etc.)\n",
    "    glacier_table.reset_index(inplace=True)\n",
    "    # drop connectivity 2 for Greenland and Antarctica\n",
    "    glacier_table = glacier_table.loc[glacier_table['Connect'].isin([0,1])]\n",
    "    glacier_table.reset_index(drop=True, inplace=True)\n",
    "    # change old index to 'O1Index' to be easier to recall what it is\n",
    "    glacier_table.rename(columns={'index': 'O1Index'}, inplace=True)\n",
    "    # Record the reference date\n",
    "    glacier_table['RefDate'] = glacier_table['BgnDate']\n",
    "    # if there is an end date, then roughly average the year\n",
    "    enddate_idx = glacier_table.loc[(glacier_table['EndDate'] > 0), 'EndDate'].index.values\n",
    "    glacier_table.loc[enddate_idx,'RefDate'] = (\n",
    "            np.mean((glacier_table.loc[enddate_idx,['BgnDate', 'EndDate']].values / 10**4).astype(int),\n",
    "                    axis=1).astype(int) * 10**4 + 9999)\n",
    "    # drop columns of data that is not being used\n",
    "    glacier_table.drop(rgi_cols_drop, axis=1, inplace=True)\n",
    "    # add column with the O1 glacier numbers\n",
    "    glacier_table[rgi_O1Id_colname] = (\n",
    "            glacier_table['RGIId'].str.split('.').apply(pd.Series).loc[:,1].astype(int))\n",
    "    glacier_table['rgino_str'] = [x.split('-')[1] for x in glacier_table.RGIId.values]\n",
    "#    glacier_table[rgi_glacno_float_colname] = (np.array([np.str.split(glacier_table['RGIId'][x],'-')[1]\n",
    "#                                                    for x in range(glacier_table.shape[0])]).astype(float))\n",
    "    glacier_table[rgi_glacno_float_colname] = (np.array([x.split('-')[1] for x in glacier_table['RGIId']]\n",
    "#            [np.str.split(glacier_table['RGIId'][x],'-')[1]\n",
    "#                                                    for x in range(glacier_table.shape[0])]\n",
    "            ).astype(float))\n",
    "    # set index name\n",
    "    glacier_table.index.name = indexname\n",
    "    # Longitude between 0-360deg (no negative)\n",
    "    glacier_table['CenLon_360'] = glacier_table['CenLon']\n",
    "    glacier_table.loc[glacier_table['CenLon'] < 0, 'CenLon_360'] = (\n",
    "            360 + glacier_table.loc[glacier_table['CenLon'] < 0, 'CenLon_360'])\n",
    "    # Subset glaciers based on their terminus type\n",
    "    termtype_values = []\n",
    "    if include_landterm:\n",
    "        termtype_values.append(0)\n",
    "        # assume dry calving, regenerated, and not assigned are land-terminating\n",
    "        termtype_values.append(3)\n",
    "        termtype_values.append(4)\n",
    "        termtype_values.append(9)\n",
    "    if include_tidewater:\n",
    "        termtype_values.append(1)\n",
    "        # assume shelf-terminating glaciers are tidewater\n",
    "        termtype_values.append(5)\n",
    "    if include_laketerm:\n",
    "        termtype_values.append(2)\n",
    "    glacier_table = glacier_table.loc[glacier_table['TermType'].isin(termtype_values)]\n",
    "    glacier_table.reset_index(inplace=True, drop=True)\n",
    "    # Glacier number with no trailing zeros\n",
    "    glacier_table['glacno'] = [str(int(x.split('-')[1].split('.')[0])) + '.' + x.split('-')[1].split('.')[1]\n",
    "                               for x in glacier_table.RGIId]\n",
    "    \n",
    "    # Remove glaciers below threshold\n",
    "    glacier_table = glacier_table.loc[glacier_table['Area'] > min_glac_area_km2,:]\n",
    "    glacier_table.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Remove glaciers that are meant to be skipped\n",
    "    if glac_no_skip is not None:\n",
    "        glac_no_all = list(glacier_table['glacno'])\n",
    "        glac_no_unique = [x for x in glac_no_all if x not in glac_no_skip]\n",
    "        unique_idx = [glac_no_all.index(x) for x in glac_no_unique]\n",
    "        glacier_table = glacier_table.loc[unique_idx,:]\n",
    "        glacier_table.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # print(\"This study is focusing on %s glaciers in region %s\" % (glacier_table.shape[0], rgi_regionsO1))\n",
    "\n",
    "    return glacier_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d0977-d2e2-4ec6-9af3-4a5effaec14b",
   "metadata": {},
   "source": [
    "#### Key notes:\n",
    "The aggregation step (i.e., whether you use annual time series of the fractions that reach the ocean or just use the full time period's average) makes no difference on the impact to the glacier's contributions to sea-level rise.\n",
    "\n",
    "Basins where the fraction is greater than 1 doesn't have an impact on the results either since these are often very small values above 1 or the actual discharge is very small.\n",
    "\n",
    "Mass gain (i.e., negative sea-level rise) is multiplied by fractions just like mass loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a56d8-caa0-46d6-aac4-b703b8addfbd",
   "metadata": {},
   "source": [
    "# Confirmation of SLR Estimates with Rounce et al. (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c781240-130b-49e0-8ce7-16ccceaab228",
   "metadata": {},
   "outputs": [],
   "source": [
    "glac_vns = ['mass_annual', 'mass_bsl_annual']\n",
    "# glac_vns = ['mass_annual']\n",
    "\n",
    "regions = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "\n",
    "ds_vn_dict = {'area_annual': 'glac_area_annual', \n",
    "              'mass_annual': 'glac_mass_annual',\n",
    "              'mass_bsl_annual': 'glac_mass_bsl_annual',\n",
    "              'runoff_monthly': 'glac_runoff_fixed_monthly'}\n",
    "\n",
    "global_mass_dict = {}\n",
    "global_mass_bsl_dict = {}\n",
    "nontw_idx_dict = {}\n",
    "reg_mass_dict = {}\n",
    "# Load and aggregate data\n",
    "for scenario in scenarios:\n",
    "    reg_mass_dict[scenario] = {}\n",
    "    for glac_vn in glac_vns:\n",
    "        print(scenario, glac_vn)\n",
    "        for nreg, reg in enumerate(regions):\n",
    "\n",
    "            # print(reg, scenario, glac_vn)\n",
    "            \n",
    "            # ----- NETCDF FILEPATHS AND FILENAMES -----\n",
    "            # Filenames\n",
    "            fp_reg_vn = pygem_output_fp + glac_vn + '/' + str(reg).zfill(2) + '/'\n",
    "            \n",
    "            fn_batches = []\n",
    "            for fn in os.listdir(fp_reg_vn):\n",
    "                if glac_vn in fn and scenario in fn:\n",
    "                    fn_batches.append(fn)\n",
    "     \n",
    "            # Load data\n",
    "            vn_scenario = None\n",
    "            for fn in fn_batches:\n",
    "\n",
    "                ds_batch = xr.open_dataset(fp_reg_vn + fn)\n",
    "                \n",
    "                ds_vn = ds_vn_dict[glac_vn]\n",
    "                \n",
    "                reg_vn_scenario_raw = ds_batch[ds_vn].values\n",
    "\n",
    "                # Select specific GCM data\n",
    "                gcm_dict = {}\n",
    "                for i in ds_batch.Climate_Model.attrs.keys():\n",
    "                    try:\n",
    "                        # print(int(i), ds_batch.Climate_Model.attrs[i])\n",
    "                        gcm_dict[int(i)] = ds_batch.Climate_Model.attrs[i]\n",
    "                    except:\n",
    "                        pass\n",
    "                ds_gcm_names = []\n",
    "                for i in ds_batch.Climate_Model.values:\n",
    "                    ds_gcm_names.append(gcm_dict[i])\n",
    "                \n",
    "                ds_gcm_idxs = []\n",
    "                for gcm_name in gcm_names:\n",
    "                    ds_gcm_idxs.append(ds_gcm_names.index(gcm_name))\n",
    "                ds_gcm_idxs\n",
    "                \n",
    "                reg_vn_scenario = reg_vn_scenario_raw[ds_gcm_idxs,:,:]\n",
    "\n",
    "                # Aggregate\n",
    "                if vn_scenario is None:\n",
    "                    vn_scenario = reg_vn_scenario\n",
    "                    vn_rgiids = list(ds_batch.RGIId.values)\n",
    "                    years = ds_batch.year.values\n",
    "                else:\n",
    "                    vn_scenario = np.concatenate((vn_scenario, reg_vn_scenario), axis=1)\n",
    "                    vn_rgiids.extend(list(ds_batch.RGIId.values))\n",
    "\n",
    "            # Load Glaciers to identify tidewater glaciers for bsl correction\n",
    "            if glac_vn == 'mass_bsl_annual':\n",
    "                if reg in list(nontw_idx_dict.keys()):\n",
    "                    nontw_idx = nontw_idx_dict[reg]\n",
    "                else:\n",
    "                    glacno_list = [x.split('-')[1] for x in vn_rgiids]\n",
    "                    main_glac_rgi_reg = selectglaciersrgitable(glac_no=glacno_list)\n",
    "                    if main_glac_rgi_reg.TermType.sum() == 0:\n",
    "                        tw_idx = []\n",
    "                    else:\n",
    "                        tw_idx = np.where((main_glac_rgi_reg.TermType.values == 1) | (main_glac_rgi_reg.TermType.values == 5))[0]\n",
    "                        if len(tw_idx) == 0:\n",
    "                            tw_idx = []\n",
    "                    nontw_idx = [x for x in main_glac_rgi_reg.index.values if x not in tw_idx]\n",
    "                    nontw_idx_dict[reg] = nontw_idx\n",
    "    \n",
    "                vn_scenario[:,nontw_idx,:] = 0\n",
    "            \n",
    "            vn_scenario_reg = vn_scenario.sum(1)\n",
    "\n",
    "            if nreg == 0:\n",
    "                vn_scenario_all = vn_scenario_reg\n",
    "            else:\n",
    "                vn_scenario_all = vn_scenario_all + vn_scenario_reg\n",
    "\n",
    "            if glac_vn == 'mass_annual':\n",
    "                reg_mass_dict[scenario][reg] = vn_scenario_reg\n",
    "                mass_years = ds_batch.year.values\n",
    "\n",
    "        if glac_vn == 'mass_annual':\n",
    "            mass_annual_all = vn_scenario_all\n",
    "        elif glac_vn == 'mass_bsl_annual':\n",
    "            mass_bsl_annual_all = vn_scenario_all\n",
    "\n",
    "    global_mass_dict[scenario] = mass_annual_all\n",
    "    global_mass_bsl_dict[scenario] = mass_bsl_annual_all\n",
    "\n",
    "    reg_mass_dict[scenario]['all'] = mass_annual_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e6f96-19a1-44fa-9165-4348fb72c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sea Level Rise from 2015 to 2100 (mm SLE)')\n",
    "print('  scenario - SLR - SLR (uncorrected)')\n",
    "for scenario in scenarios:\n",
    "    mass_annual_all = global_mass_dict[scenario]\n",
    "    mass_bsl_annual_all = global_mass_bsl_dict[scenario]\n",
    "\n",
    "    vol_all = mass_annual_all / 900\n",
    "    vol_bsl_all = mass_bsl_annual_all / 900\n",
    "    \n",
    "    slr_all = slr_mmSLEyr(vol_all, vol_bsl_all)\n",
    "    slr_all_nobsl = slr_mmSLEyr(vol_all, np.zeros(vol_all.shape))\n",
    "    print('   ', scenario)\n",
    "    print('     ',int(np.round(np.median(slr_all[:,16:].sum(1)))), '+/-', int(np.round(1.96*np.std(slr_all[:,16:].sum(1))))) \n",
    "    print('     ',int(np.round(np.median(slr_all_nobsl[:,16:].sum(1)))), '+/-', int(np.round(1.96*np.std(slr_all_nobsl[:,16:].sum(1)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236cce2-788b-47c4-b221-f7cf7989f9fc",
   "metadata": {},
   "source": [
    "# Basin-wide Applications\n",
    "- Remove Greenland basins\n",
    "- Aggregate to single dataset\n",
    "\n",
    "Note: values should only be used for the reduction in sea-level rise. Note that below sea level values adjustments are less important in this framework as marine-terminating glaciers contribute 100% regardless, and the RGI Region analysis is done separately below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac995948-5d8f-4f9b-a8c3-f9e4d946dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi60_basin_df = pd.read_csv('RGI60_wMERIT_IDs.csv')\n",
    "rgi60_basin_df_greenland = rgi60_basin_df.loc[rgi60_basin_df['O1Region'] == 5]\n",
    "print(rgi60_basin_df_greenland.shape[0], 'glaciers in Greenland')\n",
    "\n",
    "basin_ids_greenland = list(np.unique(rgi60_basin_df_greenland.MeritID.values))\n",
    "print(len(basin_ids_greenland), 'basins in Greenland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb9de7-5bd0-4b84-a99a-14e5103e8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three datasets (all together, uncommon, and common)\n",
    "uncommon_basin_slr_mmSLE = {}\n",
    "uncommon_basin_slr_mmSLE_raw = {}\n",
    "\n",
    "common_basin_slr_mmSLE = {}\n",
    "common_basin_slr_mmSLE_raw = {}\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(scenario)\n",
    "\n",
    "    basin_ids_all = []\n",
    "    uncommon_basin_ids_all = []\n",
    "    common_basin_ids_all = []\n",
    "\n",
    "    # ----- Load Data -----\n",
    "    ds_wbm_fp = 'wbm_processed/basin_components_yearly-wibt-common/'\n",
    "    ds_wbm_fn = 'wbm_' + scenario + '_2016_2099_v4-basin_components_yearly-wibt-common.nc'\n",
    "    ds_wbm = xr.open_dataset(ds_wbm_fp + ds_wbm_fn)\n",
    "\n",
    "    ds_pygem_fp = 'pygem_processed/basin_data/'\n",
    "    ds_pygem_fn = 'pygem_' + scenario + '_2000_2100-basin_annual-wibt-common.nc'\n",
    "    ds_pygem = xr.open_dataset(ds_pygem_fp + ds_pygem_fn)\n",
    "\n",
    "    ds_pygem_fn_uncommon = 'pygem_' + scenario + '_2000_2100-basin_annual-wibt-uncommon-w19.nc'\n",
    "    ds_pygem_uncommon = xr.open_dataset(ds_pygem_fp + ds_pygem_fn_uncommon)\n",
    "\n",
    "    glmelt = ds_wbm.glmelt.values\n",
    "    q_pg = ds_wbm.discharge_pg.values\n",
    "\n",
    "    # Basin database (including interbasin transfers) prepared by Stanley Gliddon 8/14/2024\n",
    "    df_basins = pd.read_csv('MERIT_plus_15min_v1_IDs_super.csv')\n",
    "    df_basin_ids = list(df_basins.ID.values)\n",
    "    \n",
    "    # Subset to common years\n",
    "    years_wbm = ds_wbm.year.values\n",
    "    years_mass = ds_pygem.year.values\n",
    "    year_idx_start = np.where(years_mass == years_wbm[0])[0][0]\n",
    "    year_idx_end = np.where(years_mass == years_wbm[-1]+1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f6f57-fac8-4715-ba35-45e421570e54",
   "metadata": {},
   "source": [
    "## Process datasets for sea level contributions and export\n",
    "Note: for exor_frac_option == 2, when annual values are used, this can result in very unrealisitc values. This occurs when:\n",
    "- glmelt is zero (i.e., dividing by zero)\n",
    "- glmelt_exor is tiny and thus fraction is huge (note: if glmelt is small, then q_pg_exor should also be small)\n",
    "\n",
    "Therefore, when this option is used, we do two things. We define unrealistic values as those where\n",
    "- glmelt is zero\n",
    "- frac_exor_2ocean > 2\n",
    "\n",
    "We then replace them with the average for each gcm and basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f155c09-bfec-4d40-9d62-86a48b0fbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pygem_fp = 'pygem_processed/basin_data/'\n",
    "ds_slr_fn = 'pygem_2000_2100-basin_annual_slr_mmSLE.nc'\n",
    "if not os.path.exists(ds_pygem_fp + ds_slr_fn):\n",
    "# for batman in [0]:\n",
    "\n",
    "    # Three datasets (all together, uncommon, and common)\n",
    "    uncommon_basin_slr_mmSLE = {}\n",
    "    uncommon_basin_slr_mmSLE_raw = {}\n",
    "    \n",
    "    common_basin_slr_mmSLE = {}\n",
    "    common_basin_slr_mmSLE_raw = {}\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(scenario)\n",
    "    \n",
    "        basin_ids_all = []\n",
    "        uncommon_basin_ids_all = []\n",
    "        common_basin_ids_all = []\n",
    "    \n",
    "        # ----- Load Data -----\n",
    "        ds_wbm_fp = 'wbm_processed/basin_components_yearly-wibt-common/'\n",
    "        ds_wbm_fn = 'wbm_' + scenario + '_2016_2099_v4-basin_components_yearly-wibt-common.nc'\n",
    "        ds_wbm = xr.open_dataset(ds_wbm_fp + ds_wbm_fn)\n",
    "    \n",
    "        ds_pygem_fp = 'pygem_processed/basin_data/'\n",
    "        ds_pygem_fn = 'pygem_' + scenario + '_2000_2100-basin_annual-wibt-common.nc'\n",
    "        ds_pygem = xr.open_dataset(ds_pygem_fp + ds_pygem_fn)\n",
    "    \n",
    "        ds_pygem_fn_uncommon = 'pygem_' + scenario + '_2000_2100-basin_annual-wibt-uncommon-w19.nc'\n",
    "        ds_pygem_uncommon = xr.open_dataset(ds_pygem_fp + ds_pygem_fn_uncommon)\n",
    "    \n",
    "        glmelt = ds_wbm.glmelt.values\n",
    "        q_pg = ds_wbm.discharge_pg.values\n",
    "    \n",
    "        # Basin database (including interbasin transfers) prepared by Stanley Gliddon 8/14/2024\n",
    "        df_basins = pd.read_csv('MERIT_plus_15min_v1_IDs_super.csv')\n",
    "        df_basin_ids = list(df_basins.ID.values)\n",
    "        \n",
    "        # Subset to common years\n",
    "        years_wbm = ds_wbm.year.values\n",
    "        years_mass = ds_pygem.year.values\n",
    "        year_idx_start = np.where(years_mass == years_wbm[0])[0][0]\n",
    "        year_idx_end = np.where(years_mass == years_wbm[-1]+1)[0][0]\n",
    "    \n",
    "        \n",
    "        # ----- UNCOMMON BASINS -----\n",
    "        # Contribute 100% to sea-level rise\n",
    "        # mass_uncommon = ds_pygem_uncommon.mass_annual.values[:,:,year_idx_start:year_idx_end+1]\n",
    "        # mass_bsl_uncommon = ds_pygem_uncommon.mass_bsl_annual.values[:,:,year_idx_start:year_idx_end+1]\n",
    "        mass_uncommon = ds_pygem_uncommon.mass_annual.values\n",
    "        mass_bsl_uncommon = ds_pygem_uncommon.mass_bsl_annual.values\n",
    "\n",
    "        # Convert to mm SLE\n",
    "        mass_mmSLE_uncommon = mass_to_mmSLE(mass_uncommon, mass_bsl_uncommon)\n",
    "        \n",
    "        # Compute SLR contribution for each GCM and year\n",
    "        slr_mmSLE_uncommon = mass_mmSLE_uncommon[:,:,0:-1] - mass_mmSLE_uncommon[:,:,1:]\n",
    "\n",
    "        # Record to eventually create dataset\n",
    "        uncommon_basin_slr_mmSLE[scenario] = slr_mmSLE_uncommon\n",
    "        uncommon_basin_slr_mmSLE_raw[scenario] = slr_mmSLE_uncommon\n",
    "        uncommon_basin_ids_all.extend(list(ds_pygem_uncommon.basin_ids.values))\n",
    "\n",
    "\n",
    "        # ----- ENDORHEIC AND EXORHEIC BASINS -----\n",
    "        # These basin are \"common\" (shared) between PyGEM and WBM\n",
    "        # mass_common = ds_pygem.mass_annual.values[:,:,year_idx_start:year_idx_end+1]\n",
    "        # mass_bsl_common = ds_pygem.mass_bsl_annual.values[:,:,year_idx_start:year_idx_end+1]\n",
    "        mass_common = ds_pygem.mass_annual.values\n",
    "        mass_bsl_common = ds_pygem.mass_bsl_annual.values\n",
    "    \n",
    "        # Select the basins that are common\n",
    "        basin_ids_first = [int(x.split('-')[0]) for x in list(ds_pygem.basin_ids.values)]\n",
    "        basin_idxs = [df_basin_ids.index(x) for x in basin_ids_first]\n",
    "        df_basins_common = df_basins.iloc[basin_idxs,:]\n",
    "        df_basins_common.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # ----- REMOVE GREENLAND BASINS -----\n",
    "        basin_ids_first_nogreenland = []\n",
    "        basin_ids_first_greenland = []\n",
    "        for id in basin_ids_first:\n",
    "            if id in basin_ids_greenland:\n",
    "                basin_ids_first_greenland.append(id)\n",
    "            else:\n",
    "                basin_ids_first_nogreenland.append(id)\n",
    "\n",
    "        # select greenland and non-greenland indices\n",
    "        basins_common_raw = list(df_basins_common.ID.values)\n",
    "        # - greenland\n",
    "        basin_idx_greenland = [basins_common_raw.index(x) for x in basin_ids_first_greenland]\n",
    "        df_basins_common_greenland = df_basins_common.loc[basin_idx_greenland,:]\n",
    "        # - non-greenland\n",
    "        basin_idx_nogreenland = [basins_common_raw.index(x) for x in basin_ids_first_nogreenland]\n",
    "        df_basins_common_nogreenland = df_basins_common.loc[basin_idx_nogreenland,:]\n",
    "        \n",
    "        # Compute mass in mm SLE for each\n",
    "        mass_mmSLE_common = mass_to_mmSLE(mass_common, mass_bsl_common)\n",
    "        mass_mmSLE_greenland = mass_mmSLE_common[:,basin_idx_greenland,:]\n",
    "        mass_mmSLE_nogreenland = mass_mmSLE_common[:,basin_idx_nogreenland,:]\n",
    "\n",
    "        # -- add Greenland to \"uncommon\" dataset --\n",
    "        # Compute SLR contribution for each GCM and year\n",
    "        slr_mmSLE_greenland = mass_mmSLE_greenland[:,:,0:-1] - mass_mmSLE_greenland[:,:,1:]\n",
    "\n",
    "        # update datasets\n",
    "        uncommon_basin_slr_mmSLE[scenario] = np.concatenate((uncommon_basin_slr_mmSLE[scenario], \n",
    "                                                             slr_mmSLE_greenland), axis=1)\n",
    "        uncommon_basin_slr_mmSLE_raw[scenario] = np.concatenate((uncommon_basin_slr_mmSLE_raw[scenario], \n",
    "                                                                 slr_mmSLE_greenland), axis=1)\n",
    "        uncommon_basin_ids_all.extend(basin_ids_first_greenland)\n",
    "\n",
    "        # ----- NON-GREENLAND BASINS -----\n",
    "        # Separate endorheic and exorheic basins\n",
    "        df_basins_common_nogreenland.reset_index(inplace=True, drop=True)\n",
    "        endo_idx = list(df_basins_common_nogreenland.loc[df_basins_common_nogreenland['Endorheic'] == 1].index.values)\n",
    "        df_basins_endo = df_basins_common_nogreenland.iloc[endo_idx,:]\n",
    "\n",
    "        exor_idx = list(df_basins_common_nogreenland.loc[df_basins_common_nogreenland['Endorheic'] == 0].index.values)\n",
    "        df_basins_exor = df_basins_common_nogreenland.iloc[exor_idx,:]\n",
    "\n",
    "        # Mass in mm SLE for each\n",
    "        mass_mmSLE_endo = mass_mmSLE_nogreenland[:,endo_idx,:]\n",
    "        mass_mmSLE_exor = mass_mmSLE_nogreenland[:,exor_idx,:]\n",
    "\n",
    "        # -- ENDORHEIC Contributions --\n",
    "        slr_mmSLE_endo = mass_mmSLE_endo[:,:,0:-1] - mass_mmSLE_endo[:,:,1:]\n",
    "\n",
    "        # Append to larger dataset\n",
    "        # - append only zeros for Endorheic basins because they don't contribute, but keep the raw values too per previous calcs\n",
    "        common_basin_slr_mmSLE[scenario] = np.zeros(slr_mmSLE_endo.shape)\n",
    "        common_basin_slr_mmSLE_raw[scenario] = slr_mmSLE_endo\n",
    "        common_basin_ids_all.extend(list(df_basins_endo.ID.values))\n",
    "    \n",
    "        # -- EXORHEIC Contributions --\n",
    "        # Must adjust contributions based on WBM tracking\n",
    "        # - subset based on non-greenland basins\n",
    "        glmelt_nogreenland = glmelt[:,basin_idx_nogreenland,:]\n",
    "        q_pg_nogreenland = q_pg[:,basin_idx_nogreenland,:]\n",
    "        # - subset based on endorheic and exorheic\n",
    "        glmelt_exor = glmelt_nogreenland[:,exor_idx,:]\n",
    "        q_pg_exor = q_pg_nogreenland[:,exor_idx,:]\n",
    "        \n",
    "        # Compute SLR contribution for each GCM and year\n",
    "        slr_mmSLE_exor_raw = mass_mmSLE_exor[:,:,0:-1] - mass_mmSLE_exor[:,:,1:]\n",
    "\n",
    "        # Consider only fraction makes it to ocean\n",
    "        if exor_frac_option == 1:\n",
    "            # Option 1: Adustment assuming that fraction is constant over the entire period\n",
    "            frac_exor_2ocean = np.sum(q_pg_exor, axis=2) / np.sum(glmelt_exor, axis=2)\n",
    "            slr_mmSLE_exor = slr_mmSLE_exor_raw * frac_exor_2ocean[:,:,np.newaxis]\n",
    "    \n",
    "        elif exor_frac_option == 2:\n",
    "            # Option 2: Adjustment assuming fraction changes each year\n",
    "            # - small values give fractions exceeding 1e20 which are not plausible. Limit individual values to 2.\n",
    "            # - fraction when glacier melt 0 causes error; assume amount that reaches after glacier lost is negligible \n",
    "            # replace them with the average for each gcm and basin\n",
    "            \n",
    "            # Find basin average\n",
    "            frac_exor_2ocean_raw = np.zeros(glmelt_exor.shape)\n",
    "            frac_exor_2ocean_raw[:,:,:] = np.nan\n",
    "            frac_exor_2ocean_raw[glmelt_exor > 0] = q_pg_exor[glmelt_exor > 0] / glmelt_exor[glmelt_exor > 0]\n",
    "            frac_exor_2ocean_raw[frac_exor_2ocean_raw > 2] = np.nan\n",
    "            frac_exor_2ocean_gcm_basin_avg = np.nanmean(frac_exor_2ocean_raw, axis=2)\n",
    "            \n",
    "            # Fill in poor values\n",
    "            frac_exor_2ocean_raw_filled = np.zeros(frac_exor_2ocean_raw.shape)\n",
    "            for x in np.arange(glmelt_exor.shape[2]):\n",
    "                frac_exor_2ocean_raw_filled[:,:,x] = frac_exor_2ocean_gcm_basin_avg\n",
    "            frac_exor_2ocean_raw_filled[~np.isnan(frac_exor_2ocean_raw)] =  frac_exor_2ocean_raw[~np.isnan(frac_exor_2ocean_raw)]\n",
    "            \n",
    "            # Extend values to 2000-2015 and 2100 which aren't modeled by WBM simulations\n",
    "            frac_exor_2ocean = np.zeros(slr_mmSLE_exor_raw.shape)\n",
    "            # start with calculated values\n",
    "            frac_exor_2ocean[:,:,year_idx_start:year_idx_end] = frac_exor_2ocean_raw_filled\n",
    "            # fill in values at start with first 10-yr average for each basin and GCM\n",
    "            frac_exor_start_avg = frac_exor_2ocean_raw_filled[:,:,:10].mean(2)\n",
    "            frac_exor_2ocean[:,:,:year_idx_start] = frac_exor_start_avg[:,:,np.newaxis]\n",
    "            # fill in values at end iwth last 10-yr average for each basin and GCM\n",
    "            frac_exor_end_avg = frac_exor_2ocean_raw_filled[:,:,-10:].mean(2)\n",
    "            frac_exor_2ocean[:,:,year_idx_end:] = frac_exor_start_avg[:,:,np.newaxis]\n",
    "        \n",
    "            # Compute the SLR contribution\n",
    "            slr_mmSLE_exor = slr_mmSLE_exor_raw * frac_exor_2ocean\n",
    "\n",
    "        # Append to larger dataset\n",
    "        common_basin_slr_mmSLE[scenario] = np.concatenate((common_basin_slr_mmSLE[scenario], slr_mmSLE_exor), axis=1)\n",
    "        common_basin_slr_mmSLE_raw[scenario] = np.concatenate((common_basin_slr_mmSLE_raw[scenario], slr_mmSLE_exor_raw), axis=1)\n",
    "        common_basin_ids_all.extend(list(df_basins_exor.ID.values))\n",
    "\n",
    "    \n",
    "    # ------ EXPORT DATA INTO A NEW XARRAY DATASET -----\n",
    "    uncommon_basin_slr_mmSLE_ssps = None\n",
    "    common_basin_slr_mmSLE_ssps = None\n",
    "    common_basin_slr_mmSLE_raw_ssps = None\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        if common_basin_slr_mmSLE_ssps is None:\n",
    "            uncommon_basin_slr_mmSLE_ssps = uncommon_basin_slr_mmSLE[scenario][np.newaxis,:,:,:]\n",
    "            common_basin_slr_mmSLE_ssps = common_basin_slr_mmSLE[scenario][np.newaxis,:,:,:]\n",
    "            common_basin_slr_mmSLE_raw_ssps = common_basin_slr_mmSLE_raw[scenario][np.newaxis,:,:,:]\n",
    "        else:\n",
    "            uncommon_basin_slr_mmSLE_ssps = np.concatenate((uncommon_basin_slr_mmSLE_ssps, \n",
    "                                                            uncommon_basin_slr_mmSLE[scenario][np.newaxis,:,:,:]), axis=0)\n",
    "            common_basin_slr_mmSLE_ssps = np.concatenate((common_basin_slr_mmSLE_ssps, \n",
    "                                                          common_basin_slr_mmSLE[scenario][np.newaxis,:,:,:]), axis=0)\n",
    "            common_basin_slr_mmSLE_raw_ssps = np.concatenate((common_basin_slr_mmSLE_raw_ssps, \n",
    "                                                              common_basin_slr_mmSLE_raw[scenario][np.newaxis,:,:,:]), axis=0)\n",
    "    \n",
    "    # ----- Reorder data so IDs are numerically ordered -----\n",
    "    common_basin_slr_mmSLE_ssps_sorted = np.zeros(common_basin_slr_mmSLE_ssps.shape)\n",
    "    common_basin_slr_mmSLE_raw_ssps_sorted = np.zeros(common_basin_slr_mmSLE_raw_ssps.shape)\n",
    "    common_basin_endo_flag = np.zeros(len(common_basin_ids_all)).astype(int)\n",
    "    \n",
    "    common_basin_ids_all_sorted = sorted(common_basin_ids_all)\n",
    "    for nid, id in enumerate(common_basin_ids_all_sorted):\n",
    "        id_idx = common_basin_ids_all.index(id)\n",
    "        common_basin_slr_mmSLE_ssps_sorted[:,:,nid,:] = common_basin_slr_mmSLE_ssps[:,:,id_idx,:]\n",
    "        common_basin_slr_mmSLE_raw_ssps_sorted[:,:,nid,:] = common_basin_slr_mmSLE_raw_ssps[:,:,id_idx,:]\n",
    "    \n",
    "        if id in list(df_basins_endo.ID.values):\n",
    "            common_basin_endo_flag[nid] = 1\n",
    "\n",
    "    # Mannually check that this is correctly re-ordering data\n",
    "    #  - 10/10/2024 it does\n",
    "    # print(common_basin_slr_mmSLE_ssps_sorted[:,:,13,:].sum(), common_basin_slr_mmSLE_ssps[:,:,123,:].sum())\n",
    "    # print('\\nendo flag:', common_basin_endo_flag[123])\n",
    "    \n",
    "    # ----- Export netCDF -----\n",
    "    basins_common = np.arange(len(common_basin_ids_all_sorted))\n",
    "    uncommon_basin_ids = np.array([int(x) for x in uncommon_basin_ids_all])\n",
    "    basins_uncommon = np.arange(len(uncommon_basin_ids_all))\n",
    "    \n",
    "    ds_slr = xr.Dataset(\n",
    "                data_vars=dict(\n",
    "                    slr_mmSLE_common=([\"scenario\", \"gcm\", \"basin_common\", \"year\"], common_basin_slr_mmSLE_ssps_sorted),\n",
    "                    slr_mmSLE_common_raw=([\"scenario\", \"gcm\", \"basin_common\", \"year\"], common_basin_slr_mmSLE_raw_ssps_sorted),\n",
    "                    slr_mmSLE_uncommon=([\"scenario\", \"gcm\", \"basin_uncommon\", \"year\"], uncommon_basin_slr_mmSLE_ssps),\n",
    "                    basin_ids_common=([\"basin_common\"], common_basin_ids_all_sorted),\n",
    "                    basin_ids_uncommon=([\"basin_uncommon\"], uncommon_basin_ids),\n",
    "                    gcms=([\"gcm\"], gcm_names),\n",
    "                    scenarios=([\"scenario\"], scenarios),\n",
    "                    endo_flag = ([\"basin_common\"], common_basin_endo_flag),\n",
    "                ),\n",
    "                coords=dict(\n",
    "                    year=years_mass[:-1],\n",
    "                    basin_common=basins_common,\n",
    "                    basin_uncommon=basins_uncommon,\n",
    "                    gcm=gcm_names,\n",
    "                    scenario=scenarios,\n",
    "                ),\n",
    "                attrs=dict(description=\"Processed PyGEM data by WBM basin with inter-basin transfers merged together for common basins by their first (lowest) ID.\"),\n",
    "            )\n",
    "    \n",
    "    # Variable attributes\n",
    "    ds_slr.slr_mmSLE_common.attrs['long_name'] = 'basin sea level contribution adjusted for downstream hydrology'\n",
    "    ds_slr.slr_mmSLE_common.attrs['units'] = 'mm sea level equivalent'\n",
    "    ds_slr.slr_mmSLE_common.attrs['comment'] = 'only basins common to PyGEM and WBM'\n",
    "    \n",
    "    ds_slr.slr_mmSLE_common_raw.attrs['long_name'] = 'basin sea level contribution not adjusted for downstream hydrology'\n",
    "    ds_slr.slr_mmSLE_common_raw.attrs['units'] = 'mm sea level equivalent'\n",
    "    ds_slr.slr_mmSLE_common_raw.attrs['comment'] = 'only basins common to PyGEM and WBM'\n",
    "    \n",
    "    ds_slr.slr_mmSLE_uncommon.attrs['long_name'] = 'non-WBM basin sea level contribution'\n",
    "    ds_slr.slr_mmSLE_uncommon.attrs['units'] = 'mm sea level equivalent'\n",
    "    ds_slr.slr_mmSLE_uncommon.attrs['comment'] = 'all additional basins in PyGEM but not simulated by WBM'\n",
    "    \n",
    "    # Year attributes\n",
    "    ds_slr.year.attrs['long_name'] = 'years'\n",
    "    ds_slr.year.attrs['year_type'] = 'calendar year'\n",
    "    ds_slr.year.attrs['range'] = '2000 - 2101'\n",
    "    ds_slr.year.attrs['comment'] = 'years referring to the start of each year'\n",
    "    \n",
    "    # Basin attributes\n",
    "    ds_slr.basin_common.attrs['long_name'] = 'WBM basins derived from MERIT data'\n",
    "    ds_slr.basin_uncommon.attrs['long_name'] = 'WBM basins derived from MERIT data but not included in simulations. 99999 refers to RGI Region 19.'\n",
    "\n",
    "    # Dataset processing attributes\n",
    "    ds_slr.attrs['date_created'] = '10/10/2024'\n",
    "    ds_slr.attrs['exor_frac_option'] = '2'\n",
    "    \n",
    "    ds_slr.to_netcdf(ds_pygem_fp + ds_slr_fn)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    ds_slr = xr.open_dataset(ds_pygem_fp + ds_slr_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04096f-fec6-402b-b8d6-7556f6a1297a",
   "metadata": {},
   "source": [
    "# Merge the following basins for better plotting of endorheic basins spatially\n",
    "'W Kunlun Shan, E Pamir, C Tien Shan'\n",
    "33, 1105, 1255, 82, 1159, 148, 970, 1105, 1330, 1606, 1729, 1895, 2141, 2145, 2302, 2413, 2434, 2697, 3071, 3115, 3361\n",
    "\n",
    "'E Tien Shan'\n",
    "228, 175 (watch out for the one in Africa!) 244, 399, 477, 663, 754, 1029\n",
    "\n",
    "'Tibetan Interior Mountains'\n",
    "317, 772, 899, 1099, 1208, 1220, 1233, 1314, 1343, 1453, 1884, 2045, 2284, 3305, 3371, 1124, 1346, 1456, 1587, 1656, 1663, 1731, 1778, 1863, 1903, 2030, 2043, 2211, 2259, 2300, 2517, 2518, 2532, 2553, 2574, 2817, 2831, 2841, 2851, 2856, 2913, 2924, 2956, 2970, 3274, 3276, 3299, 3301, 3315, 3323, 3327, 3350, 3336, 3377, 3380 \n",
    "\n",
    "'E Kunlun Shan, Altun Shan, Qilian Shan'\n",
    "132, 148, 201, 341, 471, 571, 578, 737, 883, 896, 989, 1008, 1167, 1568, 1594, 2339, 2612, 2630, 2679, 3040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc5822-517a-41fa-b8ef-55e5504cf988",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pygem_fp = 'pygem_processed/basin_data/'\n",
    "ds_slr_endo_merged_fn = 'pygem_2000_2100-basin_annual_slr_mmSLE-endo_merged.nc'\n",
    "if not os.path.exists(ds_pygem_fp + ds_slr_endo_merged_fn):\n",
    "    \n",
    "    basins_2adjust = [33, 228, 317, 132]\n",
    "    basins_added = [1105, 1255, 82, 1159, 970, 1330, 1606, 1729, 1895, 2141, 2145, 2302, 2413, 2434, 2697, 3071, 3115, 3361, \n",
    "                    175, 244, 399, 477, 663, 754, 1029, 772, 899, 1099, 1208, 1220, 1233, 1314, 1343, 1453, 1884, 2045, 2284, 3305, 3371, \n",
    "                    1124, 1346, 1456, 1587, 1656, 1663, 1731, 1778, 1863, 1903, 2030, 2043, 2211, 2259, 2300, 2517, 2518, 2532, 2553, 2574, \n",
    "                    2817, 2831, 2841, 2851, 2856, 2913, 2924, 2956, 2970, 3274, 3276, 3299, 3301, 3315, 3323, 3327, 3350, 3336, 3377, 3380,\n",
    "                    148, 201, 341, 471, 571, 578, 737, 883, 896, 989, 1008, 1167, 1568, 1594, 2339, 2612, 2630, 2679, 3040]\n",
    "    basins_2add_dict = {33: [33, 1105, 1255, 82, 1159, 970, 1330, 1606, 1729, 1895, 2141, 2145, 2302, 2413, 2434, 2697, 3071, 3115, \n",
    "                             3361],\n",
    "                        228: [228, 175, 244, 399, 477, 663, 754, 1029],\n",
    "                        317: [317, 772, 899, 1099, 1208, 1220, 1233, 1314, 1343, 1453, 1884, 2045, 2284, 3305, 3371, 1124, 1346, 1456, 1587, \n",
    "                              1656, 1663, 1731, 1778, 1863, 1903, 2030, 2043, 2211, 2259, 2300, 2517, 2518, 2532, 2553, 2574, 2817, 2831, \n",
    "                              2841, 2851, 2856, 2913, 2924, 2956, 2970, 3274, 3276, 3299, 3301, 3315, 3323, 3327, 3350, 3336, 3377, 3380 ],\n",
    "                        132: [132, 148, 201, 341, 471, 571, 578, 737, 883, 896, 989, 1008, 1167, 1568, 1594, 2339, 2612, 2630, 2679, 3040]\n",
    "                       }\n",
    "    \n",
    "    slr_mmSLE_common_merged = None\n",
    "    slr_mmSLE_common_raw_merged = None\n",
    "    slr_mmSLE_common = ds_slr.slr_mmSLE_common.values\n",
    "    slr_mmSLE_common_raw = ds_slr.slr_mmSLE_common_raw.values\n",
    "    \n",
    "    common_endo_flag = ds_slr.endo_flag.values\n",
    "    common_endo_flag_merged = []\n",
    "    \n",
    "    basin_ids_common = list(ds_slr.basin_ids_common.values)\n",
    "    basin_ids_common_merged = []\n",
    "    for nid, basin_id in enumerate(basin_ids_common):\n",
    "        if basin_id not in basins_2adjust and basin_id not in basins_added:\n",
    "            if slr_mmSLE_common_merged is None:\n",
    "                slr_mmSLE_common_merged = slr_mmSLE_common[:,:,nid,:][:,:,np.newaxis,:]\n",
    "                slr_mmSLE_common_raw_merged = slr_mmSLE_common_raw[:,:,nid,:][:,:,np.newaxis,:]\n",
    "            else:\n",
    "                slr_mmSLE_common_merged = np.concatenate((slr_mmSLE_common_merged, \n",
    "                                                          slr_mmSLE_common[:,:,nid,:][:,:,np.newaxis,:]), axis=2)\n",
    "                slr_mmSLE_common_raw_merged = np.concatenate((slr_mmSLE_common_raw_merged, \n",
    "                                                              slr_mmSLE_common_raw[:,:,nid,:][:,:,np.newaxis,:]), axis=2)\n",
    "            basin_ids_common_merged.append(basin_id)\n",
    "            common_endo_flag_merged.append(common_endo_flag[nid])\n",
    "            \n",
    "        elif basin_id in basins_2adjust:\n",
    "            # Merge the basins together\n",
    "            basins_2merge = basins_2add_dict[basin_id]\n",
    "            slr_mmSLE_common_single = None\n",
    "            slr_mmSLE_common_raw_single = None\n",
    "            for basin_id_2merge in basins_2merge:\n",
    "                if basin_id_2merge in basin_ids_common:\n",
    "                    nidx = basin_ids_common.index(basin_id_2merge)\n",
    "    \n",
    "                    if slr_mmSLE_common_single is None:\n",
    "                        slr_mmSLE_common_single = slr_mmSLE_common[:,:,nidx,:][:,:,np.newaxis,:]\n",
    "                        slr_mmSLE_common_raw_single = slr_mmSLE_common_raw[:,:,nidx,:][:,:,np.newaxis,:]\n",
    "                    else:\n",
    "                        slr_mmSLE_common_single = slr_mmSLE_common_single + slr_mmSLE_common[:,:,nidx,:][:,:,np.newaxis,:]\n",
    "                        slr_mmSLE_common_raw_single = slr_mmSLE_common_raw_single + slr_mmSLE_common_raw[:,:,nidx,:][:,:,np.newaxis,:]\n",
    "                        \n",
    "            # Merge to full dataset\n",
    "            if slr_mmSLE_common_merged is None:\n",
    "                slr_mmSLE_common_merged = slr_mmSLE_common_single\n",
    "                slr_mmSLE_common_raw_merged = slr_mmSLE_common_raw_single\n",
    "            else:\n",
    "                slr_mmSLE_common_merged = np.concatenate((slr_mmSLE_common_merged, \n",
    "                                                          slr_mmSLE_common_single), axis=2)\n",
    "                slr_mmSLE_common_raw_merged = np.concatenate((slr_mmSLE_common_raw_merged, \n",
    "                                                              slr_mmSLE_common_raw_single), axis=2)\n",
    "    \n",
    "            basin_ids_common_merged.append(basin_id)\n",
    "            common_endo_flag_merged.append(1)\n",
    "\n",
    "    # ----- Export netCDF -----\n",
    "    basins_common = np.arange(len(basin_ids_common_merged))\n",
    "    uncommon_basin_ids = list(ds_slr.basin_ids_uncommon.values)\n",
    "    basins_uncommon = np.arange(len(uncommon_basin_ids))\n",
    "    \n",
    "    ds_slr_endo_merged = xr.Dataset(\n",
    "                data_vars=dict(\n",
    "                    slr_mmSLE_common=([\"scenario\", \"gcm\", \"basin_common\", \"year\"], slr_mmSLE_common_merged),\n",
    "                    slr_mmSLE_common_raw=([\"scenario\", \"gcm\", \"basin_common\", \"year\"], slr_mmSLE_common_raw_merged),\n",
    "                    slr_mmSLE_uncommon=([\"scenario\", \"gcm\", \"basin_uncommon\", \"year\"], ds_slr.slr_mmSLE_uncommon.values),\n",
    "                    basin_ids_common=([\"basin_common\"], basin_ids_common_merged),\n",
    "                    basin_ids_uncommon=([\"basin_uncommon\"], uncommon_basin_ids),\n",
    "                    gcms=([\"gcm\"], list(ds_slr.gcms.values)),\n",
    "                    scenarios=([\"scenario\"], list(ds_slr.scenarios.values)),\n",
    "                    endo_flag = ([\"basin_common\"], np.array(common_endo_flag_merged)),\n",
    "                ),\n",
    "                coords=dict(\n",
    "                    year=ds_slr.year.values,\n",
    "                    basin_common=basins_common,\n",
    "                    basin_uncommon=basins_uncommon,\n",
    "                    gcm=list(ds_slr.gcms.values),\n",
    "                    scenario=list(ds_slr.scenarios.values),\n",
    "                ),\n",
    "                attrs=dict(description=\"Processed PyGEM data by WBM basin with inter-basin transfers merged together for common basins by their first (lowest) ID. Endorheic basins in High Mountain Asia merged together by general glacier areas.\"),\n",
    "            )\n",
    "    \n",
    "    # Variable attributes\n",
    "    ds_slr_endo_merged.slr_mmSLE_common.attrs['long_name'] = 'basin sea level contribution adjusted for downstream hydrology'\n",
    "    ds_slr_endo_merged.slr_mmSLE_common.attrs['units'] = 'mm sea level equivalent'\n",
    "    ds_slr_endo_merged.slr_mmSLE_common.attrs['comment'] = 'only basins common to PyGEM and WBM'\n",
    "    \n",
    "    ds_slr_endo_merged.slr_mmSLE_common_raw.attrs['long_name'] = 'basin sea level contribution not adjusted for downstream hydrology'\n",
    "    ds_slr_endo_merged.slr_mmSLE_common_raw.attrs['units'] = 'mm sea level equivalent'\n",
    "    ds_slr_endo_merged.slr_mmSLE_common_raw.attrs['comment'] = 'only basins common to PyGEM and WBM'\n",
    "    \n",
    "    ds_slr_endo_merged.slr_mmSLE_uncommon.attrs['long_name'] = 'non-WBM basin sea level contribution'\n",
    "    ds_slr_endo_merged.slr_mmSLE_uncommon.attrs['units'] = 'mm sea level equivalent'\n",
    "    ds_slr_endo_merged.slr_mmSLE_uncommon.attrs['comment'] = 'all additional basins in PyGEM but not simulated by WBM'\n",
    "    \n",
    "    # Year attributes\n",
    "    ds_slr_endo_merged.year.attrs['long_name'] = 'years'\n",
    "    ds_slr_endo_merged.year.attrs['year_type'] = 'calendar year'\n",
    "    ds_slr_endo_merged.year.attrs['range'] = '2000 - 2101'\n",
    "    ds_slr_endo_merged.year.attrs['comment'] = 'years referring to the start of each year'\n",
    "    \n",
    "    # Basin attributes\n",
    "    ds_slr_endo_merged.basin_common.attrs['long_name'] = 'WBM basins derived from MERIT data'\n",
    "    ds_slr_endo_merged.basin_uncommon.attrs['long_name'] = 'WBM basins derived from MERIT data but not included in simulations. 99999 refers to RGI Region 19.'\n",
    "\n",
    "    # Dataset processing attributes\n",
    "    ds_slr_endo_merged.attrs['date_created'] = '10/18/2024'\n",
    "    ds_slr_endo_merged.attrs['exor_frac_option'] = '2'\n",
    "    \n",
    "    ds_slr_endo_merged.to_netcdf(ds_pygem_fp + ds_slr_endo_merged_fn)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    ds_slr_endo_merged = xr.open_dataset(ds_pygem_fp + ds_slr_endo_merged_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed3093-f8f3-43eb-902c-d42438b3a5cd",
   "metadata": {},
   "source": [
    "### Check against the full dataset\n",
    "Confirmed 08/17/2024 that this was done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e52349-1df4-48b3-a434-92ec23870fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scenario in scenarios:\n",
    "#     ds_pygem_fn = 'pygem_' + scenario + '_2000_2100-basin_annual-wibt-common.nc'\n",
    "#     ds_pygem = xr.open_dataset(ds_pygem_fp + ds_pygem_fn)\n",
    "#     ds_pygem_fn_uncommon = 'pygem_' + scenario + '_2000_2100-basin_annual-wibt-uncommon-w19.nc'\n",
    "#     ds_pygem_uncommon = xr.open_dataset(ds_pygem_fp + ds_pygem_fn_uncommon)\n",
    "#     ds_wbm_fp = 'wbm_processed/basin_components_yearly-wibt-common/'\n",
    "#     ds_wbm_fn = 'wbm_' + scenario + '_2016_2099_v4-basin_components_yearly-wibt-common.nc'\n",
    "#     ds_wbm = xr.open_dataset(ds_wbm_fp + ds_wbm_fn)\n",
    "\n",
    "#     # Subset to common years\n",
    "#     years_wbm = ds_wbm.year.values\n",
    "#     years_mass = ds_pygem.year.values\n",
    "#     year_idx_start = np.where(years_mass == years_wbm[0])[0][0]\n",
    "#     year_idx_end = np.where(years_mass == years_wbm[-1]+1)[0][0]\n",
    "    \n",
    "#     mass_common = ds_pygem.mass_annual.values[:,:,year_idx_start:year_idx_end+1].sum(1)\n",
    "#     mass_bsl_common = ds_pygem.mass_bsl_annual.values[:,:,year_idx_start:year_idx_end+1].sum(1)\n",
    "#     mass_uncommon = ds_pygem_uncommon.mass_annual.values[:,:,year_idx_start:year_idx_end+1].sum(1)\n",
    "#     mass_bsl_uncommon = ds_pygem_uncommon.mass_bsl_annual.values[:,:,year_idx_start:year_idx_end+1].sum(1)\n",
    "    \n",
    "#     mass = mass_common + mass_uncommon\n",
    "#     mass_bsl = mass_bsl_common + mass_bsl_uncommon\n",
    "\n",
    "#     print(scenario)\n",
    "#     print('  Initial mass (Gt):', mass.mean(0)[0] / 1e12)\n",
    "#     print('  % below sea level:', np.round(mass_bsl.mean(0)[0] / mass.mean(0)[0] * 100,0))\n",
    "    \n",
    "#     mass_mmSLE = mass_to_mmSLE(mass, mass_bsl)\n",
    "#     slr_mmSLE = mass_mmSLE[:,0:-1] - mass_mmSLE[:,1:]\n",
    "#     slr_mmSLE_cumsum = np.cumsum(slr_mmSLE, axis=1)\n",
    "    \n",
    "#     slr_total_2015_2100 = slr_mmSLE_cumsum[:,-1]\n",
    "#     print('  glacier contribution to SLR 2015-2100:', \n",
    "#           np.round(slr_total_2015_2100.mean(),1), 'mm SLE')\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a3491-64da-4621-9edf-8b729d6c107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass_mmSLE_annual_all = mass_to_mmSLE(mass_annual_all, mass_bsl_annual_all)\n",
    "# slr_mmSLE_annual = mass_mmSLE_annual_all[:,0:-1] - mass_mmSLE_annual_all[:,1:]\n",
    "\n",
    "# mass_mmSLE_annual_all_nobsl = mass_to_mmSLE(mass_annual_all, np.zeros(mass_annual_all.shape))\n",
    "# slr_mmSLE_annual_nobsl = mass_mmSLE_annual_all_nobsl[:,0:-1] - mass_mmSLE_annual_all_nobsl[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257740e-474d-4feb-867b-b78b665a779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass_mmSLE_annual_all = mass_to_mmSLE(mass_annual_all, mass_bsl_annual_all)\n",
    "# slr_mmSLE_annual = mass_mmSLE_annual_all[:,0:-1] - mass_mmSLE_annual_all[:,1:]\n",
    "# print(slr_mmSLE_annual.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa860782-fc51-482b-9dcf-5dde4e343225",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "Derive summary statistics of sea-level rise contributions from various basins.\n",
    "\n",
    "Note that summary statistics should be based on the NSIDC datasets as these include ALL glaciers.  Some glaciers may be missing from the basin-by-basin analyses.\n",
    "\n",
    "<b>The reduction in sea-level rise due to the hydrological correction is thus the data that should be used from here.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b74e2-0906-4f6c-bb88-8d9ce8c4d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_slr(ds_slr, scenario, year_start=2015):\n",
    "\n",
    "    nscenario = list(ds_slr.scenarios.values).index(scenario)\n",
    "\n",
    "    # ----- UNCOMMON BASINS -----\n",
    "    # Contribute 100% to sea-level rise\n",
    "    slr_mmSLE_uncommon_basins = ds_slr.slr_mmSLE_uncommon.values[nscenario,:,:,:]\n",
    "    slr_mmSLE_uncommon_2015_2100 = slr_mmSLE_uncommon_basins[:,:,year_start_idx:].sum(1).sum(1)\n",
    "\n",
    "    # ----- ENDORHEIC AND EXORHEIC BASINS -----\n",
    "    # These basin are \"common\" (shared) between PyGEM and WBM\n",
    "    slr_mmSLE_common = ds_slr.slr_mmSLE_common.values[nscenario,:,:,:]\n",
    "    slr_mmSLE_common_raw = ds_slr.slr_mmSLE_common_raw.values[nscenario,:,:,:]\n",
    "    endo_flag = ds_slr.endo_flag.values\n",
    "    endo_idx = np.where(endo_flag == 1)[0]\n",
    "    exor_idx = np.where(endo_flag == 0)[0]\n",
    "\n",
    "    # -- ENDORHEIC Contributions --\n",
    "    slr_mmSLE_endo_raw_basins = slr_mmSLE_common_raw[:, endo_idx, :]\n",
    "    slr_mmSLE_endo_raw_2015_2100 = slr_mmSLE_endo_raw_basins[:,:,year_start_idx:].sum(1).sum(1)\n",
    "\n",
    "    # -- EXORHEIC Contributions --\n",
    "    slr_mmSLE_exor_basins = slr_mmSLE_common[:, exor_idx, :]\n",
    "    slr_mmSLE_exor_2015_2100 = slr_mmSLE_exor_basins[:,:,year_start_idx:].sum(1).sum(1)\n",
    "\n",
    "    slr_mmSLE_exor_raw_basins = slr_mmSLE_common_raw[:, exor_idx, :]\n",
    "    slr_mmSLE_exor_raw_2015_2100 = slr_mmSLE_exor_raw_basins[:,:,year_start_idx:].sum(1).sum(1)\n",
    "    \n",
    "    # ----- TOTAL CONTRIBUTIONS-----\n",
    "    # Adjusted total\n",
    "    slr_mmSLE_total_2015_2100 = slr_mmSLE_uncommon_2015_2100 + slr_mmSLE_exor_2015_2100\n",
    "\n",
    "    # Unadjusted (Raw) total\n",
    "    slr_mmSLE_raw_total_2015_2100 = slr_mmSLE_uncommon_2015_2100 + slr_mmSLE_exor_raw_2015_2100 + slr_mmSLE_endo_raw_2015_2100\n",
    "\n",
    "    return (slr_mmSLE_total_2015_2100, slr_mmSLE_uncommon_2015_2100, slr_mmSLE_exor_2015_2100, \n",
    "            slr_mmSLE_raw_total_2015_2100, slr_mmSLE_exor_raw_2015_2100, slr_mmSLE_endo_raw_2015_2100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e129c-3410-481e-9abf-1011d03792f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = ['Total Raw (mm SLE)', 'Total (mm SLE)', \n",
    "                'Uncommon Basins (mm SLE)', 'Exorheic Basins (mm SLE)',\n",
    "                'Dif Total (mm SLE)', 'Dif Exorheic (mm SLE)', 'Dif Endorheic (mm SLE)',\n",
    "                'Dif Total (%)']\n",
    "scenario_cns = ['ssp126-mean', 'ssp126-median', 'ssp126-std', 'ssp126-min', 'ssp126-max',\n",
    "                'ssp245-mean', 'ssp245-median', 'ssp245-std', 'ssp245-min', 'ssp245-max',\n",
    "                'ssp370-mean', 'ssp370-median', 'ssp370-std', 'ssp370-min', 'ssp370-max',\n",
    "                'ssp585-mean', 'ssp585-median', 'ssp585-std', 'ssp585-min', 'ssp585-max']\n",
    "summary_df = pd.DataFrame(np.zeros((len(summary_rows), len(scenario_cns))), index=summary_rows, columns=scenario_cns)\n",
    "\n",
    "year_start = 2016\n",
    "year_start_idx = np.where(years_mass == year_start)[0][0]\n",
    "\n",
    "for nscenario, scenario in enumerate(scenarios):\n",
    "    print('\\n\\n',scenario)\n",
    "    \n",
    "    \n",
    "    # ----- UNCOMMON BASINS -----\n",
    "    # Contribute 100% to sea-level rise\n",
    "    slr_mmSLE_uncommon_basins = ds_slr.slr_mmSLE_uncommon.values[nscenario,:,:,:]\n",
    "    slr_mmSLE_uncommon_2015_2100 = slr_mmSLE_uncommon_basins[:,:,year_start_idx:].sum(1).sum(1)\n",
    "\n",
    "    print('  ', np.round(np.median(slr_mmSLE_uncommon_2015_2100),1), 'mm SLE',\n",
    "          'Uncommon glaciers (2015-2100)')\n",
    "    \n",
    "    # ----- ENDORHEIC AND EXORHEIC BASINS -----\n",
    "    # These basin are \"common\" (shared) between PyGEM and WBM\n",
    "    slr_mmSLE_common = ds_slr.slr_mmSLE_common.values[nscenario,:,:,:]\n",
    "    slr_mmSLE_common_raw = ds_slr.slr_mmSLE_common_raw.values[nscenario,:,:,:]\n",
    "    endo_flag = ds_slr.endo_flag.values\n",
    "    endo_idx = np.where(endo_flag == 1)[0]\n",
    "    exor_idx = np.where(endo_flag == 0)[0]\n",
    "\n",
    "    # -- ENDORHEIC Contributions --\n",
    "    slr_mmSLE_endo_raw_basins = slr_mmSLE_common_raw[:, endo_idx, :]\n",
    "    slr_mmSLE_endo_raw_2015_2100 = slr_mmSLE_endo_raw_basins[:,:,year_start_idx:].sum(1).sum(1)\n",
    "\n",
    "    print('  ', np.round(np.median(slr_mmSLE_endo_raw_2015_2100),1), 'mm SLE',\n",
    "          'Endorheic glaciers (2015-2100)')\n",
    "\n",
    "\n",
    "    # -- EXORHEIC Contributions --\n",
    "    slr_mmSLE_exor_basins = slr_mmSLE_common[:, exor_idx, :]\n",
    "    slr_mmSLE_exor_2015_2100 = slr_mmSLE_exor_basins[:,:,year_start_idx:].sum(1).sum(1)\n",
    "\n",
    "    slr_mmSLE_exor_raw_basins = slr_mmSLE_common_raw[:, exor_idx, :]\n",
    "    slr_mmSLE_exor_raw_2015_2100 = slr_mmSLE_exor_raw_basins[:,:,year_start_idx:].sum(1).sum(1)\n",
    "\n",
    "    print('  ', np.round(np.median(slr_mmSLE_exor_2015_2100),1), 'mm SLE',\n",
    "          'Exorheic glaciers (2015-2100)')\n",
    "\n",
    "    print('  ', np.round(np.median(slr_mmSLE_exor_raw_2015_2100),1), 'mm SLE',\n",
    "          'Exorheic glaciers (2015-2100) (100% contribution)')\n",
    "\n",
    "    # ----- CALCULATE DIFFERENCES -----\n",
    "    # aggregate exorheic basins and endorheic basins\n",
    "    exor_difference = slr_mmSLE_exor_raw_2015_2100 - slr_mmSLE_exor_2015_2100\n",
    "    total_difference = slr_mmSLE_endo_raw_2015_2100 + exor_difference\n",
    "\n",
    "    # Adjusted total\n",
    "    slr_mmSLE_total_2015_2100 = slr_mmSLE_uncommon_2015_2100 + slr_mmSLE_exor_2015_2100\n",
    "    print('\\n  ', np.round(np.median(slr_mmSLE_total_2015_2100),1), 'mm SLE',\n",
    "          'Total (2015-2100)')\n",
    "\n",
    "    # Unadjusted (Raw) total\n",
    "    slr_mmSLE_raw_total_2015_2100 = slr_mmSLE_uncommon_2015_2100 + slr_mmSLE_exor_raw_2015_2100 + slr_mmSLE_endo_raw_2015_2100\n",
    "    print('  ', np.round(np.median(slr_mmSLE_raw_total_2015_2100),1), 'mm SLE',\n",
    "          'Total (RAW) (2015-2100)')\n",
    "    \n",
    "    print('\\n  Reduction Total:    ', str(np.round(np.median(total_difference),1)), 'mm SLE',\n",
    "          ', std ' + str(np.round(total_difference.std(),1)) + ' mm SLE')\n",
    "    \n",
    "    print('  Reduction Endorheic:', str(np.round(np.median(slr_mmSLE_endo_raw_2015_2100),1)), 'mm SLE',\n",
    "          ', std ' + str(np.round(slr_mmSLE_endo_raw_2015_2100.std(),1)) + ' mm SLE')\n",
    "    \n",
    "    print('  Reduction Exorheic: ', str(np.round(np.median(exor_difference),1)), ' mm SLE',\n",
    "          ', std ' + str(np.round(exor_difference.std(),1)) + ' mm SLE')\n",
    "    \n",
    "    # Percent Reduction\n",
    "    print('\\n  Reduction Total:', str(np.round(np.median(100*(total_difference / slr_mmSLE_raw_total_2015_2100)),1)) + '%',\n",
    "          ', std ', str(np.round(100*(total_difference / slr_mmSLE_raw_total_2015_2100).std(),1)) + '%')\n",
    "    print('  values:', np.round(100*(total_difference / slr_mmSLE_raw_total_2015_2100),1))\n",
    "\n",
    "    # Fill in summary csv\n",
    "    summary_df.loc['Total Raw (mm SLE)', scenario + '-mean'] = slr_mmSLE_raw_total_2015_2100.mean()\n",
    "    summary_df.loc['Total Raw (mm SLE)', scenario + '-median'] = np.median(slr_mmSLE_raw_total_2015_2100)\n",
    "    summary_df.loc['Total Raw (mm SLE)', scenario + '-std'] = slr_mmSLE_raw_total_2015_2100.std()\n",
    "    summary_df.loc['Total Raw (mm SLE)', scenario + '-min'] = slr_mmSLE_raw_total_2015_2100.min()\n",
    "    summary_df.loc['Total Raw (mm SLE)', scenario + '-max'] = slr_mmSLE_raw_total_2015_2100.max()\n",
    "    summary_df.loc['Total (mm SLE)', scenario + '-mean'] = slr_mmSLE_total_2015_2100.mean()\n",
    "    summary_df.loc['Total (mm SLE)', scenario + '-median'] = np.median(slr_mmSLE_total_2015_2100)\n",
    "    summary_df.loc['Total (mm SLE)', scenario + '-std'] = slr_mmSLE_total_2015_2100.std()\n",
    "    summary_df.loc['Total (mm SLE)', scenario + '-min'] = slr_mmSLE_total_2015_2100.min()\n",
    "    summary_df.loc['Total (mm SLE)', scenario + '-max'] = slr_mmSLE_total_2015_2100.max()\n",
    "    summary_df.loc['Uncommon Basins (mm SLE)', scenario + '-mean'] = slr_mmSLE_uncommon_2015_2100.mean()\n",
    "    summary_df.loc['Uncommon Basins (mm SLE)', scenario + '-median'] = np.median(slr_mmSLE_uncommon_2015_2100)\n",
    "    summary_df.loc['Uncommon Basins (mm SLE)', scenario + '-std'] = slr_mmSLE_uncommon_2015_2100.std()\n",
    "    summary_df.loc['Uncommon Basins (mm SLE)', scenario + '-min'] = slr_mmSLE_uncommon_2015_2100.min()\n",
    "    summary_df.loc['Uncommon Basins (mm SLE)', scenario + '-max'] = slr_mmSLE_uncommon_2015_2100.max()\n",
    "    summary_df.loc['Exorheic Basins (mm SLE)', scenario + '-mean'] = slr_mmSLE_exor_2015_2100.mean()\n",
    "    summary_df.loc['Exorheic Basins (mm SLE)', scenario + '-median'] = np.median(slr_mmSLE_exor_2015_2100)\n",
    "    summary_df.loc['Exorheic Basins (mm SLE)', scenario + '-std'] = slr_mmSLE_exor_2015_2100.std()\n",
    "    summary_df.loc['Exorheic Basins (mm SLE)', scenario + '-min'] = slr_mmSLE_exor_2015_2100.min()\n",
    "    summary_df.loc['Exorheic Basins (mm SLE)', scenario + '-max'] = slr_mmSLE_exor_2015_2100.max()\n",
    "    summary_df.loc['Dif Total (mm SLE)', scenario + '-mean'] = total_difference.mean()\n",
    "    summary_df.loc['Dif Total (mm SLE)', scenario + '-median'] = np.median(total_difference)\n",
    "    summary_df.loc['Dif Total (mm SLE)', scenario + '-std'] = total_difference.std()\n",
    "    summary_df.loc['Dif Total (mm SLE)', scenario + '-min'] = total_difference.min()\n",
    "    summary_df.loc['Dif Total (mm SLE)', scenario + '-max'] = total_difference.max()\n",
    "    summary_df.loc['Dif Exorheic (mm SLE)', scenario + '-mean'] = exor_difference.mean()\n",
    "    summary_df.loc['Dif Exorheic (mm SLE)', scenario + '-median'] = np.median(exor_difference)\n",
    "    summary_df.loc['Dif Exorheic (mm SLE)', scenario + '-std'] = exor_difference.std()\n",
    "    summary_df.loc['Dif Exorheic (mm SLE)', scenario + '-min'] = exor_difference.min()\n",
    "    summary_df.loc['Dif Exorheic (mm SLE)', scenario + '-max'] = exor_difference.max()\n",
    "    summary_df.loc['Dif Endorheic (mm SLE)', scenario + '-mean'] = slr_mmSLE_endo_raw_2015_2100.mean()\n",
    "    summary_df.loc['Dif Endorheic (mm SLE)', scenario + '-median'] = np.median(slr_mmSLE_endo_raw_2015_2100)\n",
    "    summary_df.loc['Dif Endorheic (mm SLE)', scenario + '-std'] = slr_mmSLE_endo_raw_2015_2100.std()\n",
    "    summary_df.loc['Dif Endorheic (mm SLE)', scenario + '-min'] = slr_mmSLE_endo_raw_2015_2100.min()\n",
    "    summary_df.loc['Dif Endorheic (mm SLE)', scenario + '-max'] = slr_mmSLE_endo_raw_2015_2100.max()\n",
    "    summary_df.loc['Dif Total (%)', scenario + '-mean'] = 100*(total_difference / slr_mmSLE_total_2015_2100).mean()\n",
    "    summary_df.loc['Dif Total (%)', scenario + '-median'] = np.median(100*(total_difference / slr_mmSLE_total_2015_2100))\n",
    "    summary_df.loc['Dif Total (%)', scenario + '-std'] = 100*(total_difference / slr_mmSLE_total_2015_2100).std()\n",
    "    summary_df.loc['Dif Total (%)', scenario + '-min'] = 100*(total_difference / slr_mmSLE_total_2015_2100).min()\n",
    "    summary_df.loc['Dif Total (%)', scenario + '-max'] = 100*(total_difference / slr_mmSLE_total_2015_2100).max()\n",
    "\n",
    "summary_df.to_csv(results_fp + 'summary_ssps.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad06e7-0f90-4d4b-a813-eeae2feac3d9",
   "metadata": {},
   "source": [
    "## WBM Basin Raster with Interbasin Transfers Merged\n",
    "Load interbasin transfer dataset, update raster_basin, export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ce448-89f2-4aa6-a90a-23f1eb700855",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_tif_fn_wibt = 'MERIT_plus_15min_v1_IDs_wcrs-wibt.tif'\n",
    "if not os.path.exists(basin_tif_fn_wibt):\n",
    "\n",
    "    # ----- Interbasin Transfer Dataset ------\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Interbasin transfer data (prepared by Stanley Gliddon 8/14/2024)\n",
    "    df_ibt = pd.read_csv('MERIT_plus_15min_v1_IDs_super.csv')\n",
    "    \n",
    "    # Loop through basins and determine the basins that need to be merged\n",
    "    ibt_dict = {}\n",
    "    for nrow, ID in enumerate(df_ibt.ID):\n",
    "        glaciers_ibt = df_ibt.loc[nrow,'Glaciers_IBT']\n",
    "        if not glaciers_ibt in ['none']:\n",
    "            ibt_dict[ID] = [int(x) for x in glaciers_ibt.split(',')]\n",
    "    \n",
    "    # Given dictionary\n",
    "    data = ibt_dict\n",
    "    \n",
    "    # Initialize the graph as an adjacency list\n",
    "    graph = defaultdict(set)\n",
    "    \n",
    "    # Build the graph\n",
    "    for key, values in ibt_dict.items():\n",
    "        for value in values:\n",
    "            graph[key].add(value)\n",
    "            graph[value].add(key)\n",
    "    \n",
    "    # Function to find all nodes in the same connected component\n",
    "    def find_connected_component(start, visited):\n",
    "        component = []\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                component.append(node)\n",
    "                stack.extend(graph[node] - visited)\n",
    "        return component\n",
    "    \n",
    "    # Find all connected components\n",
    "    visited = set()\n",
    "    connected_ibts = []\n",
    "    \n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            component = find_connected_component(node, visited)\n",
    "            connected_ibts.append(component)\n",
    "    \n",
    "    # Sort each component list and then sort the list of components\n",
    "    ibt_ids_sorted = [sorted(component) for component in connected_ibts]\n",
    "    ibt_ids_sorted.sort(key=lambda x: x[0])  # Sort based on the first element of each component\n",
    "    \n",
    "    # Print the sorted connected components\n",
    "    for component in ibt_ids_sorted:\n",
    "        print(component)\n",
    "\n",
    "    \n",
    "    # ----- Merge file together -----\n",
    "    src = rasterio.open(basin_tif_fn)\n",
    "    basin_raster = src.read(1)\n",
    "    \n",
    "    for ibt_ids in ibt_ids_sorted:\n",
    "        for id in ibt_ids:\n",
    "            basin_raster[basin_raster == id] = ibt_ids[0]\n",
    "    \n",
    "    with rasterio.open(basin_tif_fn_wibt,\n",
    "                       'w',\n",
    "                       driver='GTiff',\n",
    "                       height=basin_raster.shape[0],\n",
    "                       width=basin_raster.shape[1],\n",
    "                       count=1,\n",
    "                       dtype=basin_raster.dtype,\n",
    "                       crs=src.crs,\n",
    "                       transform=src.transform\n",
    "                      ) as dst:\n",
    "        dst.write(basin_raster[np.newaxis,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c33a9e-9534-4d34-a3d6-ee57e6d21797",
   "metadata": {},
   "source": [
    "#### Load Raster and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf98b8-159e-4d19-9685-bb8f13b2a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = rasterio.open(basin_tif_fn_wibt)\n",
    "basin_raster = src.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd37df-3f99-478b-980e-894b5ed8d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2, figsize=(10,4))\n",
    "m1 = ax[0].imshow(basin_raster)\n",
    "ax[0].set_title('Array Coordinates - Basin Ids')\n",
    "full_extent = [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\n",
    "m2 = ax[1].imshow(basin_raster, extent=full_extent)\n",
    "ax[1].set_title('Projected Coordinates - Basin Ids')\n",
    "# plt.colorbar(m2, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4467fa3-ba92-49b4-b47c-30e3d66a31a7",
   "metadata": {},
   "source": [
    "## Figure: Fraction remaining in each basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47d0d7-57da-4a51-9159-7616fd87faff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ignore_greenland = True\n",
    "\n",
    "for nscenario, scenario in enumerate(scenarios):\n",
    "    # Exorheic glaciated basins\n",
    "    basin_raster_frac_exor = np.zeros(basin_raster.shape)\n",
    "    basin_raster_frac_exor[:,:] = np.nan\n",
    "    # Endorheic glaciated basins\n",
    "    basin_raster_frac_endo = np.zeros(basin_raster.shape)\n",
    "    basin_raster_frac_endo[:,:] = np.nan\n",
    "    # Non-glaciated basins\n",
    "    basin_raster_noglac = np.zeros(basin_raster.shape)\n",
    "    basin_raster_noglac[:,:] = np.nan\n",
    "    basin_raster_noglac[basin_raster > -9999] = 0\n",
    "    if ignore_greenland:\n",
    "        for id in basin_ids_greenland:\n",
    "            basin_raster_noglac[basin_raster == id] = 0\n",
    "    # Oceans\n",
    "    basin_raster_ocean = np.zeros(basin_raster.shape)\n",
    "    basin_raster_ocean[:,:] = 0\n",
    "    # basin_raster_ocean[basin_raster == -9999] = 0\n",
    "    \n",
    "    # ----- Uncommon Basins (i.e., PyGEM but no WBM) -----\n",
    "    basin_ids_uncommon = list(ds_slr.basin_ids_uncommon.values)\n",
    "    for id in basin_ids_uncommon:\n",
    "        if ignore_greenland:\n",
    "            if not id in basin_ids_greenland:\n",
    "                basin_raster_frac_exor[basin_raster == id] = 1\n",
    "        else:\n",
    "            basin_raster_frac_exor[basin_raster == id] = 1\n",
    "    \n",
    "    # ----- Common Basins (Endorheic and Exorheic) -----\n",
    "    basin_ids_common = ds_slr.basin_ids_common.values\n",
    "    endo_flag = ds_slr.endo_flag.values\n",
    "    slr_mmSLE_common = ds_slr.slr_mmSLE_common.values[nscenario,:,:,:]\n",
    "    slr_mmSLE_common_raw = ds_slr.slr_mmSLE_common_raw.values[nscenario,:,:,:]\n",
    "    \n",
    "    # -- Endorheic --\n",
    "    endo_idx = np.where(endo_flag == 1)[0]\n",
    "    slr_mmSLE_endo_raw = slr_mmSLE_common_raw[:, endo_idx, :]\n",
    "    slr_mmSLE_endo_raw_cumsum = np.cumsum(slr_mmSLE_endo_raw, axis=2)\n",
    "    slr_mmSLE_endo_raw_2015_2100 = slr_mmSLE_endo_raw_cumsum[:,:,-1]\n",
    "    \n",
    "    basin_ids_common_endo = basin_ids_common[endo_idx]\n",
    "    for id in basin_ids_common_endo:\n",
    "        basin_raster_frac_endo[basin_raster == id] = 0\n",
    "    \n",
    "    \n",
    "    # -- Exorheic --\n",
    "    exor_idx = np.where(endo_flag == 0)[0]\n",
    "    basin_ids_common_exor = basin_ids_common[exor_idx]\n",
    "    # slr adjusted for downstream hydrology\n",
    "    slr_mmSLE_exor = slr_mmSLE_common[:, exor_idx, :]\n",
    "    slr_mmSLE_exor_cumsum = np.cumsum(slr_mmSLE_exor, axis=2)\n",
    "    slr_mmSLE_exor_2015_2100 = slr_mmSLE_exor_cumsum[:,:,-1]\n",
    "    # slr unadjusted for downstream hydrology (assuming 100% contribution)\n",
    "    slr_mmSLE_exor_raw = slr_mmSLE_common_raw[:, exor_idx, :]\n",
    "    slr_mmSLE_exor_raw_cumsum = np.cumsum(slr_mmSLE_exor_raw, axis=2)\n",
    "    slr_mmSLE_exor_raw_2015_2100 = slr_mmSLE_exor_raw_cumsum[:,:,-1]\n",
    "    # Fraction remaining\n",
    "    slr_2015_2100_frac_remaining_exor = slr_mmSLE_exor_2015_2100 / slr_mmSLE_exor_raw_2015_2100\n",
    "    slr_2015_2100_frac_remaining_exor_mean = slr_2015_2100_frac_remaining_exor.mean(0)\n",
    "\n",
    "    basin_ids_common_exor = basin_ids_common[exor_idx]\n",
    "    for nid, id in enumerate(basin_ids_common_exor):\n",
    "        basin_raster_frac_exor[basin_raster == id] = slr_2015_2100_frac_remaining_exor_mean[nid]\n",
    "            \n",
    "    # ----- PLOT OF FRACTION REMAINING -----\n",
    "    plot_frac_min = 0.5\n",
    "    plot_frac_max = 1\n",
    "    cmap_frac = 'RdYlBu'\n",
    "    # cmap_frac = 'Spectral'\n",
    "    full_extent = [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, squeeze=False, sharex=False, sharey=False,\n",
    "                          gridspec_kw={'wspace':0.5, 'hspace':0.3})\n",
    "    # exorheic basins (colored by fraction that reaches the ocean)\n",
    "    pcm = ax[0,0].imshow(basin_raster_frac_exor, vmin=plot_frac_min, vmax=plot_frac_max, cmap=cmap_frac, zorder=4, extent=full_extent)\n",
    "    # endorheic basins (dark grey)\n",
    "    pcm2 = ax[0,0].imshow(basin_raster_frac_endo, vmin=0, vmax=1, cmap='gray', zorder=3, extent=full_extent)\n",
    "    # non-glaciated basin (light grey)\n",
    "    pcm3 = ax[0,0].imshow(basin_raster_noglac, vmin=-1, vmax=1, cmap='RdBu', zorder=2, extent=full_extent)\n",
    "    # ocean (blue)\n",
    "    # pcm4 = ax[0,0].imshow(basin_raster_ocean, vmin=-0.15, vmax=0.2, cmap='coolwarm', zorder=1, extent=full_extent)\n",
    "    pcm4 = ax[0,0].imshow(basin_raster_ocean, vmin=-0.2, vmax=1, cmap='binary', zorder=1, extent=full_extent)\n",
    "    \n",
    "    # Divide existing axes and create\n",
    "    # new axes at right side of image\n",
    "    divider = make_axes_locatable(ax[0,0])\n",
    "    cax = divider.append_axes(\"right\", size=\"2%\", pad=0.15)\n",
    "    # cb = fig.colorbar(pcm, fraction=0.047*height_in/width_in, pad=0.04)\n",
    "    cb = fig.colorbar(pcm, cax=cax)\n",
    "\n",
    "    # Title\n",
    "    ax[0,0].set_title('Fraction Reaching Ocean (' + ssp_name_dict[scenario] + ')')\n",
    "    \n",
    "    fig.set_size_inches(8,3)\n",
    "    \n",
    "    fig_fp = results_fp + '_figures/'\n",
    "    if not os.path.exists(fig_fp):\n",
    "        os.makedirs(fig_fp)\n",
    "    fig.savefig(fig_fp + 'globe_basins_frac2ocean_' + scenario + '.png', dpi=500)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5cd130-e680-4802-986a-559c443000dc",
   "metadata": {},
   "source": [
    "## Figure: Fraction remaining in each RGI region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08612b31-a5d6-44c8-b0e4-b21ac2ad89c3",
   "metadata": {},
   "source": [
    "#### Fraction remaining dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a325a22-e06c-4470-b89e-0d4c589708e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_slr = ds_slr.year.values\n",
    "\n",
    "basin_frac_remaining_dict = {}\n",
    "for nscenario, scenario in enumerate(scenarios):\n",
    "    basin_frac_remaining_dict[scenario] = {}\n",
    "    \n",
    "for nscenario, scenario in enumerate(scenarios):\n",
    "    # -- Uncommon basins --\n",
    "    for id in list(ds_slr.basin_ids_uncommon.values):\n",
    "        basin_frac_remaining_dict[scenario][id] = np.zeros((len(gcm_names),years_slr.shape[0]))+1\n",
    "\n",
    "    # -- Common Basins (Endorheic and Exorheic) --\n",
    "    basin_ids_common = ds_slr.basin_ids_common.values\n",
    "    endo_flag = ds_slr.endo_flag.values\n",
    "    slr_mmSLE_common = ds_slr.slr_mmSLE_common.values[nscenario,:,:,:]\n",
    "    slr_mmSLE_common_raw = ds_slr.slr_mmSLE_common_raw.values[nscenario,:,:,:]\n",
    "    \n",
    "    # -- Endorheic --\n",
    "    basin_ids_common_endo = basin_ids_common[endo_idx]\n",
    "    for id in basin_ids_common_endo:\n",
    "        basin_frac_remaining_dict[scenario][id] = np.zeros((len(gcm_names),years_slr.shape[0]))\n",
    "    \n",
    "    # -- Exorheic --\n",
    "    exor_idx = np.where(endo_flag == 0)[0]\n",
    "    basin_ids_common_exor = basin_ids_common[exor_idx]\n",
    "    \n",
    "    # slr adjusted for downstream hydrology\n",
    "    slr_mmSLE_exor = slr_mmSLE_common[:, exor_idx, :]\n",
    "    # slr_mmSLE_exor_cumsum = np.cumsum(slr_mmSLE_exor, axis=2)\n",
    "    # slr_mmSLE_exor_2015_2100 = slr_mmSLE_exor_cumsum[:,:,-1]\n",
    "    \n",
    "    # slr unadjusted for downstream hydrology (assuming 100% contribution)\n",
    "    slr_mmSLE_exor_raw = slr_mmSLE_common_raw[:, exor_idx, :]\n",
    "    # slr_mmSLE_exor_raw_cumsum = np.cumsum(slr_mmSLE_exor_raw, axis=2)\n",
    "    # slr_mmSLE_exor_raw_2015_2100 = slr_mmSLE_exor_raw_cumsum[:,:,-1]\n",
    "    \n",
    "    # Fraction remaining\n",
    "    slr_frac_remaining_exor = np.zeros(slr_mmSLE_exor.shape)+1\n",
    "    slr_frac_remaining_exor[slr_mmSLE_exor_raw > 0] = slr_mmSLE_exor[slr_mmSLE_exor_raw > 0] / slr_mmSLE_exor_raw[slr_mmSLE_exor_raw > 0]\n",
    "    # slr_2015_2100_frac_remaining_exor = slr_mmSLE_exor_2015_2100 / slr_mmSLE_exor_raw_2015_2100\n",
    "\n",
    "    # Loop through basins\n",
    "    for nid, id in enumerate(basin_ids_common_exor):\n",
    "        basin_frac_remaining_dict[scenario][id] = slr_frac_remaining_exor[:,nid,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee323ce-88b9-4626-95b4-c0dd51191eeb",
   "metadata": {},
   "source": [
    "#### Modify Sea Level Contribution from Each Glacier by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc1a75-b840-41b1-8b70-6ca2b7c7ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraction remaining by region:')\n",
    "print(' - uses annual value for each GCM of water that reaches the ocean in each basin based on the exor_frac_option')\n",
    "print('\\nSea Level Contribution from 2015-2100 (mmSLE)')\n",
    "      \n",
    "# Load and aggregate data\n",
    "regions = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "\n",
    "reg_annual_frac_dict = {}\n",
    "reg_annual_slr_mmSLE_dict = {}\n",
    "reg_annual_slr_mmSLE_raw_dict = {}\n",
    "for scenario in scenarios:\n",
    "    reg_annual_frac_dict[scenario] = {}\n",
    "    reg_annual_slr_mmSLE_dict[scenario] = {}\n",
    "    reg_annual_slr_mmSLE_raw_dict[scenario] = {}\n",
    "    \n",
    "    print('\\n',scenario)\n",
    "    global_slr_mmSLE = None\n",
    "    global_slr_mmSLE_raw = None\n",
    "    for reg in regions:\n",
    "\n",
    "        # ----- LOAD DATA -----\n",
    "        # Load mass annual and mass below sea level data\n",
    "        fp_reg_mass = pygem_output_fp + 'mass_annual/' + str(reg).zfill(2) + '/'\n",
    "        fn_reg_mass = 'R' + str(reg).zfill(2) + '_glac_mass_annual_c2_ba1_50sets_2000_2100-' + scenario + '.nc'\n",
    "        ds_mass = xr.open_dataset(fp_reg_mass + fn_reg_mass)\n",
    "\n",
    "        fp_reg_mass_bsl = pygem_output_fp + 'mass_bsl_annual/' + str(reg).zfill(2) + '/'\n",
    "        fn_reg_mass_bsl = 'R' + str(reg).zfill(2) + '_glac_mass_bsl_annual_c2_ba1_50sets_2000_2100-' + scenario + '.nc'\n",
    "        ds_mass_bsl = xr.open_dataset(fp_reg_mass_bsl + fn_reg_mass_bsl)\n",
    "        \n",
    "        mass_annual = ds_mass['glac_mass_annual'].values\n",
    "        mass_bsl_annual = ds_mass_bsl['glac_mass_bsl_annual'].values\n",
    "\n",
    "        \n",
    "        # Identify tidewater glaciers for bsl correction\n",
    "        rgiids_raw = list(ds_mass.RGIId.values)\n",
    "        glacno_list = [x.split('-')[1] for x in rgiids_raw]\n",
    "        # Add trailing zero to ensure RGIIds are appropriate\n",
    "        if reg < 10:\n",
    "            rgiids = ['RGI60-0' + x for x in glacno_list]\n",
    "        else:\n",
    "            rgiids = ['RGI60-' + x for x in glacno_list]\n",
    "            \n",
    "        if reg in list(nontw_idx_dict.keys()):\n",
    "            nontw_idx = nontw_idx_dict[reg]\n",
    "        else:\n",
    "            main_glac_rgi_reg = selectglaciersrgitable(glac_no=glacno_list)\n",
    "            if main_glac_rgi_reg.TermType.sum() == 0:\n",
    "                tw_idx = []\n",
    "            else:\n",
    "                tw_idx = np.where((main_glac_rgi_reg.TermType.values == 1) | (main_glac_rgi_reg.TermType.values == 5))[0]\n",
    "                if len(tw_idx) == 0:\n",
    "                    tw_idx = []\n",
    "            nontw_idx = [x for x in main_glac_rgi_reg.index.values if x not in tw_idx]\n",
    "            nontw_idx_dict[reg] = nontw_idx\n",
    "\n",
    "        mass_bsl_annual[:,nontw_idx,:] = 0\n",
    "\n",
    "        # ANNUAL SLE ESTIMATE\n",
    "        vol_annual = mass_annual / 900\n",
    "        vol_bsl_annual = mass_bsl_annual / 900\n",
    "    \n",
    "        glac_slr_mmSLE_raw = slr_mmSLEyr(vol_annual, vol_bsl_annual)\n",
    "\n",
    "        # BASIN-BY-BASIN ADJUSTMENT\n",
    "        rgi_df_wbasins = pd.read_csv(rgiids_wbasins_fn)\n",
    "        group_dict = dict(zip(rgi_df_wbasins.RGIId, rgi_df_wbasins.MeritID))\n",
    "        \n",
    "        # loop through RGIIds\n",
    "        glac_slr_mmSLE = np.zeros(glac_slr_mmSLE_raw.shape)\n",
    "        for n_rgiid, rgiid in enumerate(rgiids):\n",
    "            try:\n",
    "                basin_id = group_dict[rgiid]\n",
    "                glac_frac_remaining = basin_frac_remaining_dict[scenario][basin_id]\n",
    "            except:\n",
    "                glac_frac_remaining = np.zeros((len(gcm_names),years.shape[0]-1))+1\n",
    "\n",
    "            single_glac_slr_mmSLE = glac_frac_remaining * glac_slr_mmSLE_raw[:,n_rgiid,:]\n",
    "            glac_slr_mmSLE[:,n_rgiid,:] = single_glac_slr_mmSLE\n",
    "\n",
    "        # STORE REGIONAL DATA\n",
    "        reg_annual_slr_mmSLE_dict[scenario][reg] = glac_slr_mmSLE.sum(1)\n",
    "        reg_annual_slr_mmSLE_raw_dict[scenario][reg] = glac_slr_mmSLE_raw.sum(1)\n",
    "        reg_annual_frac_dict[scenario][reg] = glac_slr_mmSLE.sum(1) / glac_slr_mmSLE_raw.sum(1)\n",
    "        \n",
    "        # Aggregate globally\n",
    "        if global_slr_mmSLE is None:\n",
    "            global_slr_mmSLE = glac_slr_mmSLE.sum(1)\n",
    "            global_slr_mmSLE_raw = glac_slr_mmSLE_raw.sum(1)\n",
    "        else:\n",
    "            global_slr_mmSLE = global_slr_mmSLE + glac_slr_mmSLE.sum(1)\n",
    "            global_slr_mmSLE_raw = global_slr_mmSLE_raw + glac_slr_mmSLE_raw.sum(1)\n",
    "\n",
    "    # STORE GLOBAL DATA\n",
    "    reg_annual_slr_mmSLE_dict[scenario]['all'] = global_slr_mmSLE\n",
    "    reg_annual_slr_mmSLE_raw_dict[scenario]['all'] = global_slr_mmSLE_raw \n",
    "    reg_annual_frac_dict[scenario]['all'] = global_slr_mmSLE / global_slr_mmSLE_raw\n",
    "\n",
    "    print('   Raw (2015-2100 mmSLE):', int(np.round(np.median(global_slr_mmSLE_raw[:,16:].sum(1)))), \n",
    "          '+/-', int(np.round(1.96*np.std(global_slr_mmSLE_raw[:,16:].sum(1))))) \n",
    "    print('   Adj (2015-2100 mmSLE):', int(np.round(np.median(global_slr_mmSLE[:,16:].sum(1)))), \n",
    "          '+/-', int(np.round(1.96*np.std(global_slr_mmSLE[:,16:].sum(1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc49f06-a446-4a71-acb9-7f305104ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ----- FIGURE: CUMULATIVE SEA-LEVEL RISE REGIONAL -----\n",
    "print(' - uses annual value for each GCM of water that reaches the ocean in each basin based on the exor_frac_option')\n",
    "\n",
    "years = ds_mass.year.values\n",
    "\n",
    "scenario_colordict = {'ssp119':'#081d58', 'ssp126':'#1d91c0', 'ssp245':'#7fcdbb', 'ssp370':'#F47A20', 'ssp585':'#ED2024'}\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(nrows=6,ncols=4,wspace=0.3,hspace=0.4)\n",
    "ax1 = fig.add_subplot(gs[0:2,0:2])\n",
    "ax2 = fig.add_subplot(gs[0,3])\n",
    "ax3 = fig.add_subplot(gs[1,2])\n",
    "ax4 = fig.add_subplot(gs[1,3])\n",
    "ax5 = fig.add_subplot(gs[2,0])\n",
    "ax6 = fig.add_subplot(gs[2,1])\n",
    "ax7 = fig.add_subplot(gs[2,2])\n",
    "ax8 = fig.add_subplot(gs[2,3])\n",
    "ax9 = fig.add_subplot(gs[3,0])\n",
    "ax10 = fig.add_subplot(gs[3,1])\n",
    "ax11 = fig.add_subplot(gs[3,2])\n",
    "ax12 = fig.add_subplot(gs[3,3])\n",
    "ax13 = fig.add_subplot(gs[4,0])\n",
    "ax14 = fig.add_subplot(gs[4,1])\n",
    "ax15 = fig.add_subplot(gs[4,2])\n",
    "ax16 = fig.add_subplot(gs[4,3])\n",
    "ax17 = fig.add_subplot(gs[5,0])\n",
    "ax18 = fig.add_subplot(gs[5,1])\n",
    "ax19 = fig.add_subplot(gs[5,2])\n",
    "ax20 = fig.add_subplot(gs[5,3])\n",
    "\n",
    "regions_ordered = ['all',1,5,19,3,4,9,7,17,6,13,14,2,15,8,10,11,16,18,12]\n",
    "for nax, ax in enumerate([ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10,ax11,ax12,ax13,ax14,ax15,ax16,ax17,ax18,ax19,ax20]):\n",
    "    \n",
    "    reg = regions_ordered[nax]          \n",
    "\n",
    "    slr_max = 0\n",
    "    for nscenario, scenario in enumerate(scenarios):\n",
    "        # Time series of fraction reaching ocean\n",
    "        reg_annual_frac = reg_annual_frac_dict[scenario][reg]\n",
    "        reg_annual_frac_med = uniform_filter(np.median(reg_annual_frac[:,year_start_idx+1:], axis=0), size=(11)) \n",
    "        ax.plot(years[year_start_idx+1:-1], 100*reg_annual_frac_med, color=scenario_colordict[scenario])\n",
    "\n",
    "        reg_annual_slr_mmSLE_raw_dict\n",
    "\n",
    "        # Cumulative sea level contribution in text\n",
    "        reg_slr_mmSLE = reg_annual_slr_mmSLE_dict[scenario][reg]\n",
    "        reg_slr_mmSLE_2015_2100_med = np.median(reg_slr_mmSLE[:,year_start_idx+1:].sum(1),axis=0)\n",
    "        if reg_slr_mmSLE_2015_2100_med < 10:\n",
    "            slr_text = str(np.round(reg_slr_mmSLE_2015_2100_med,1))\n",
    "        else:\n",
    "            slr_text = str(int(np.round(reg_slr_mmSLE_2015_2100_med,0)))\n",
    "        \n",
    "        # if nax==0:\n",
    "        #     ax.text(0.03, 0.02+nscenario*0.05, slr_text, color=scenario_colordict[scenario], size=10, \n",
    "        #             horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes)\n",
    "        # else:\n",
    "        #     ax.text(0.03, 0.02+nscenario*0.1, slr_text, color=scenario_colordict[scenario], size=9, \n",
    "        #             horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes)\n",
    "\n",
    "        # Add raw number next to it too\n",
    "        reg_slr_mmSLE_raw = reg_annual_slr_mmSLE_raw_dict[scenario][reg]\n",
    "        reg_slr_mmSLE_raw_2015_2100_med = np.median(reg_slr_mmSLE_raw[:,year_start_idx+1:].sum(1),axis=0)\n",
    "        if reg_slr_mmSLE_2015_2100_med < 10:\n",
    "            slr_text = slr_text + ' / ' + str(np.round(reg_slr_mmSLE_raw_2015_2100_med,1))\n",
    "        else:\n",
    "            slr_text = slr_text + ' / ' + str(int(np.round(reg_slr_mmSLE_raw_2015_2100_med,0)))\n",
    "\n",
    "        if not reg in [13]:\n",
    "            yadj = 0\n",
    "        else:\n",
    "            yadj=0.52\n",
    "        if nax==0:\n",
    "            ax.text(0.03, yadj+0.02+nscenario*0.05, slr_text, color=scenario_colordict[scenario], size=10, \n",
    "                    horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes)\n",
    "        else:\n",
    "            ax.text(0.03, yadj+0.02+nscenario*0.1, slr_text, color=scenario_colordict[scenario], size=9, \n",
    "                    horizontalalignment='left', verticalalignment='bottom', transform=ax.transAxes)\n",
    "\n",
    "    ax.set_xlim(2015,2100)\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(50))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "        \n",
    "    ax.set_ylim(0,105)\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(10))\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', direction='inout', right=True)\n",
    "    ax.tick_params(axis='both', which='minor', direction='in', right=True)\n",
    "    \n",
    "    if nax == 0:\n",
    "        label_height=1.07\n",
    "    else:\n",
    "        label_height=1.16\n",
    "    ax.text(1, label_height, rgi_reg_dict[reg], size=10, horizontalalignment='right', \n",
    "            verticalalignment='top', transform=ax.transAxes)\n",
    "\n",
    "    if nax == 1:\n",
    "        labels = ['SSP1-2.6', 'SSP2-4.5', 'SSP3-7.0', 'SSP5-8.5']\n",
    "        ax.legend(loc=(-1.5,0.2), labels=labels, fontsize=10, ncol=1, columnspacing=0.5, labelspacing=0.25, \n",
    "                  handlelength=1, handletextpad=0.25, borderpad=0, frameon=False\n",
    "                  )\n",
    "fig.text(0.07,0.5,'Percentage of Mass Loss Reaching Ocean (%)', size=12, horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "# Save figure\n",
    "fig_fn = 'FigS2_RGI_Regions-Percent_Reach_Ocean.png'\n",
    "fig_fp = results_fp + '_figures/'\n",
    "if not os.path.exists(fig_fp):\n",
    "    os.makedirs(fig_fp)\n",
    "fig.set_size_inches(8.5,11)\n",
    "fig.savefig(fig_fp + fig_fn, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b83c0-b5eb-4814-98af-46baeb40e902",
   "metadata": {},
   "source": [
    "## Global figure: time series of glacier runoff and discharge\n",
    "Exclude Greenland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060594b2-f71e-40cc-997e-156729c10c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basin database (including interbasin transfers) prepared by Stanley Gliddon 8/14/2024\n",
    "df_basins = pd.read_csv('MERIT_plus_15min_v1_IDs_super.csv')\n",
    "df_basin_ids = list(df_basins.ID.values)\n",
    "basin_ids_greenland = list(df_basins.loc[df_basins['SubContinentName'] == 'Greenland', 'ID'].values)\n",
    "\n",
    "global_cns = ['qpg_m3s_2016_2035_mean', 'qpg_m3s_2016_2035_median', 'qpg_m3s_2016_2035_std', \n",
    "              'qpg_m3s_2080_2099_mean', 'qpg_m3s_2080_2099_median', 'qpg_m3s_2080_2099_std', \n",
    "              'glmelt_m3s_2016_2035_mean', 'glmelt_m3s_2016_2035_median', 'glmelt_m3s_2016_2035_std', \n",
    "              'glmelt_m3s_2080_2099_mean', 'glmelt_m3s_2080_2099_median', 'glmelt_m3s_2080_2099_std', 'glmelt_m3s_2080_2099_95',\n",
    "              'glmelt_endo_m3s_2016_2035_mean', 'glmelt_endo_m3s_2016_2035_median', 'glmelt_endo_m3s_2016_2035_std', \n",
    "              'glmelt_endo_m3s_2080_2099_mean', 'glmelt_endo_m3s_2080_2099_median', 'glmelt_endo_m3s_2080_2099_std', \n",
    "              'frac_2016_2035_mean', 'frac_2016_2035_median', 'frac_2016_2035_std', \n",
    "              'frac_2080_2099_mean', 'frac_2080_2099_median', 'frac_2080_2099_std', 'frac_2080_2099_95',\n",
    "              'SLR_mmsle_raw_2015_2100_mean', 'SLR_mmsle_raw_2015_2100_median', 'SLR_mmsle_raw_2015_2100_std', \n",
    "              'SLR_mmsle_raw_2015_2100_95', 'SLR_mmsle_raw_2015_2100_min', 'SLR_mmsle_raw_2015_2100_max',\n",
    "              'SLR_mmsle_2015_2100_mean', 'SLR_mmsle_2015_2100_median', 'SLR_mmsle_2015_2100_std', \n",
    "              'SLR_mmsle_2015_2100_95', 'SLR_mmsle_2015_2100_min', 'SLR_mmsle_2015_2100_max',\n",
    "              'SLR_mmsle_2015_2100_correction_mean', 'SLR_mmsle_2015_2100_correction_median', 'SLR_mmsle_2015_2100_correction_std', 'SLR_mmsle_2015_2100_correction_95',\n",
    "              'SLR_mmsle_2015_2100_correction_perc_mean', 'SLR_mmsle_2015_2100_correction_perc_median', 'SLR_mmsle_2015_2100_correction_perc_std', \n",
    "              'SLR_mmsle_2015_2100_correction_perc_95']\n",
    "\n",
    "global_df = pd.DataFrame(np.zeros((len(scenarios), len(global_cns))), columns=global_cns, index=scenarios)\n",
    "\n",
    "print(global_cns)\n",
    "\n",
    "for nscenario, scenario in enumerate(scenarios):\n",
    "    print(scenario)\n",
    "\n",
    "    basin_ids_all = []\n",
    "    uncommon_basin_ids_all = []\n",
    "    common_basin_ids_all = []\n",
    "\n",
    "    # ----- Load Data -----\n",
    "    ds_wbm_fp = 'wbm_processed/basin_components_yearly-wibt-common/'\n",
    "    ds_wbm_fn = 'wbm_' + scenario + '_2016_2099_v4-basin_components_yearly-wibt-common.nc'\n",
    "    ds_wbm = xr.open_dataset(ds_wbm_fp + ds_wbm_fn)\n",
    "\n",
    "    # ds_pygem_fp = 'pygem_processed/basin_data/'\n",
    "    # ds_pygem_fn = 'pygem_' + scenario + '_2000_2100-basin_annual-wibt-common.nc'\n",
    "    # ds_pygem = xr.open_dataset(ds_pygem_fp + ds_pygem_fn)\n",
    "\n",
    "    # ds_pygem_fn_uncommon = 'pygem_' + scenario + '_2000_2100-basin_annual-wibt-uncommon-w19.nc'\n",
    "    # ds_pygem_uncommon = xr.open_dataset(ds_pygem_fp + ds_pygem_fn_uncommon)\n",
    "\n",
    "    # Select non-greenland data and non-greenland + exorheic data\n",
    "    wbm_ids = list(ds_wbm.basin_ids.values)\n",
    "    wbm_ids_init = [int(x.split('-')[0]) for x in wbm_ids]\n",
    "    basin_ids_all = list(df_basins['ID'].values)\n",
    "    \n",
    "    wbm_ids_init_idx = [basin_ids_all.index(x) for x in wbm_ids_init]\n",
    "    \n",
    "    df_basins_wbm = df_basins.loc[wbm_ids_init_idx,:]\n",
    "    df_basins_wbm.reset_index(inplace=True, drop=True)\n",
    "    basin_ids_greenland = df_basins_wbm.loc[df_basins_wbm['SubContinentName'] == 'Greenland', 'ID'].values\n",
    "    \n",
    "    basin_ids_nogreenland = []\n",
    "    basin_ids_nogreenland_exor = []\n",
    "    basin_ids_nogreenland_endo = []\n",
    "    for nbasin, basin_id in enumerate(wbm_ids_init):\n",
    "        if not basin_id in basin_ids_greenland:\n",
    "            basin_ids_nogreenland.append(basin_id)\n",
    "\n",
    "            if df_basins_wbm.loc[nbasin, 'Endorheic'] == 0:\n",
    "                basin_ids_nogreenland_exor.append(basin_id)\n",
    "            else:\n",
    "                basin_ids_nogreenland_endo.append(basin_id)\n",
    "    \n",
    "    basin_ids_nogreenland_idx = [wbm_ids_init.index(x) for x in basin_ids_nogreenland]\n",
    "    basin_ids_nogreenland_exor_idx = [wbm_ids_init.index(x) for x in basin_ids_nogreenland_exor]\n",
    "    basin_ids_nogreenland_endo_idx = [wbm_ids_init.index(x) for x in basin_ids_nogreenland_endo]\n",
    "\n",
    "    # Values of interest\n",
    "    glmelt = ds_wbm.glmelt.values\n",
    "    q_pg = ds_wbm.discharge_pg.values\n",
    "\n",
    "    # REMOVE GREENLAND\n",
    "    # Glacier melt can be summed\n",
    "    glmelt_nogreenland = glmelt[:,basin_ids_nogreenland_idx,:].sum(1)\n",
    "    glmelt_nogreenland_endo = glmelt[:,basin_ids_nogreenland_endo_idx,:].sum(1)\n",
    "    \n",
    "    # ENDORHEIC BASIN ADJUSTED\n",
    "    # - reaching ocean must account for endorheic basins\n",
    "    q_pg_nogreenland_exor = q_pg[:,basin_ids_nogreenland_exor_idx,:].sum(1)\n",
    "    \n",
    "    # FRACTION REACHING OCEAN\n",
    "    frac_ocean_nogreenland = 100 * q_pg_nogreenland_exor / glmelt_nogreenland\n",
    "\n",
    "    # GLOBAL SEA-LEVEL RISE\n",
    "    global_slr_mmSLE = reg_annual_slr_mmSLE_dict[scenario]['all']\n",
    "    global_slr_mmSLE_raw = reg_annual_slr_mmSLE_raw_dict[scenario]['all']\n",
    "    \n",
    "    years_mass = ds_mass.year.values\n",
    "    years_mass_start_idx = np.where(years_mass == 2016)[0][0]\n",
    "    years_slr = years_mass[years_mass_start_idx:-1]\n",
    "    \n",
    "    global_slr_mmSLE_2015_2100 = np.sum(global_slr_mmSLE[:,years_mass_start_idx:], axis=1)\n",
    "    global_slr_mmSLE_raw_2015_2100 = np.sum(global_slr_mmSLE_raw[:,years_mass_start_idx:], axis=1)\n",
    "\n",
    "    # ----- CSV data -----\n",
    "    years = ds_wbm.year.values\n",
    "    year_start = 2016\n",
    "    year_end = 2035\n",
    "    year_idx_start = np.where(years == year_start)[0][0]\n",
    "    year_idx_end = np.where(years == year_end)[0][0]\n",
    "\n",
    "    glmelt_subset = glmelt_nogreenland[:,year_idx_start:year_idx_end+1]\n",
    "    glmelt_endo_subset = glmelt_nogreenland_endo[:,year_idx_start:year_idx_end+1]\n",
    "    q_pg_subset = q_pg_nogreenland_exor[:,year_idx_start:year_idx_end+1]\n",
    "    frac_ocean_subset = frac_ocean_nogreenland[:,year_idx_start:year_idx_end+1]\n",
    "\n",
    "    global_df.loc[scenario, 'qpg_m3s_2016_2035_mean'] = np.mean(np.mean(q_pg_subset,axis=0))\n",
    "    global_df.loc[scenario, 'qpg_m3s_2016_2035_median'] = np.median(np.mean(q_pg_subset,axis=0))\n",
    "    global_df.loc[scenario, 'qpg_m3s_2016_2035_std'] = np.std(np.mean(q_pg_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_m3s_2016_2035_mean'] = np.mean(np.mean(glmelt_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_m3s_2016_2035_median'] = np.median(np.mean(glmelt_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_m3s_2016_2035_std'] = np.std(np.mean(glmelt_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_endo_m3s_2016_2035_mean'] = np.mean(np.mean(glmelt_endo_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_endo_m3s_2016_2035_median'] = np.median(np.mean(glmelt_endo_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_endo_m3s_2016_2035_std'] = np.std(np.mean(glmelt_endo_subset,axis=0))\n",
    "    global_df.loc[scenario, 'frac_2016_2035_mean'] = np.mean(np.mean(frac_ocean_subset,axis=0))\n",
    "    global_df.loc[scenario, 'frac_2016_2035_median'] = np.median(np.mean(frac_ocean_subset,axis=0))\n",
    "    global_df.loc[scenario, 'frac_2016_2035_std'] = np.std(np.mean(frac_ocean_subset,axis=0))\n",
    "\n",
    "    \n",
    "    year_start = 2080\n",
    "    year_end = 2099\n",
    "    year_idx_start = np.where(years == year_start)[0][0]\n",
    "    year_idx_end = np.where(years == year_end)[0][0]\n",
    "\n",
    "    glmelt_subset = glmelt_nogreenland[:,year_idx_start:year_idx_end+1]\n",
    "    glmelt_endo_subset = glmelt_nogreenland_endo[:,year_idx_start:year_idx_end+1]\n",
    "    q_pg_subset = q_pg_nogreenland_exor[:,year_idx_start:year_idx_end+1]\n",
    "    frac_ocean_subset = frac_ocean_nogreenland[:,year_idx_start:year_idx_end+1]\n",
    "\n",
    "    global_df.loc[scenario, 'qpg_m3s_2080_2099_mean'] = np.mean(np.mean(q_pg_subset,axis=0))\n",
    "    global_df.loc[scenario, 'qpg_m3s_2080_2099_median'] = np.median(np.mean(q_pg_subset,axis=0))\n",
    "    global_df.loc[scenario, 'qpg_m3s_2080_2099_std'] = np.std(np.mean(q_pg_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_m3s_2080_2099_mean'] = np.mean(np.mean(glmelt_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_m3s_2080_2099_median'] = np.median(np.mean(glmelt_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_m3s_2080_2099_std'] = np.std(np.mean(glmelt_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_m3s_2080_2099_95'] = 1.96*np.std(np.mean(glmelt_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_endo_m3s_2080_2099_mean'] = np.mean(np.mean(glmelt_endo_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_endo_m3s_2080_2099_median'] = np.median(np.mean(glmelt_endo_subset,axis=0))\n",
    "    global_df.loc[scenario, 'glmelt_endo_m3s_2080_2099_std'] = np.std(np.mean(glmelt_endo_subset,axis=0))\n",
    "    global_df.loc[scenario, 'frac_2080_2099_mean'] = np.mean(np.mean(frac_ocean_subset,axis=0))\n",
    "    global_df.loc[scenario, 'frac_2080_2099_median'] = np.median(np.mean(frac_ocean_subset,axis=0))\n",
    "    global_df.loc[scenario, 'frac_2080_2099_std'] = np.std(np.mean(frac_ocean_subset,axis=0))\n",
    "    global_df.loc[scenario, 'frac_2080_2099_95'] = 1.96*np.std(np.mean(frac_ocean_subset,axis=0))\n",
    "\n",
    "    global_df.loc[scenario,'SLR_mmsle_raw_2015_2100_mean'] = np.mean(global_slr_mmSLE_raw_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_raw_2015_2100_median'] = np.median(global_slr_mmSLE_raw_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_raw_2015_2100_std'] = np.std(global_slr_mmSLE_raw_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_raw_2015_2100_95'] = 1.96*np.std(global_slr_mmSLE_raw_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_raw_2015_2100_min'] = np.min(global_slr_mmSLE_raw_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_raw_2015_2100_max'] = np.max(global_slr_mmSLE_raw_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_mean'] = np.mean(global_slr_mmSLE_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_median'] = np.median(global_slr_mmSLE_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_std'] = np.std(global_slr_mmSLE_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_95'] = 1.96*np.std(global_slr_mmSLE_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_min'] = np.min(global_slr_mmSLE_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_max'] = np.max(global_slr_mmSLE_2015_2100)\n",
    "\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_correction_mean'] = np.mean(global_slr_mmSLE_raw_2015_2100 - global_slr_mmSLE_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_correction_median'] = np.median(global_slr_mmSLE_raw_2015_2100 - global_slr_mmSLE_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_correction_std'] = np.std(global_slr_mmSLE_raw_2015_2100 - global_slr_mmSLE_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_correction_95'] = 1.96*np.std(global_slr_mmSLE_raw_2015_2100 - global_slr_mmSLE_2015_2100)\n",
    "\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_correction_perc_mean'] = np.mean(\n",
    "        100 * (global_slr_mmSLE_raw_2015_2100 - global_slr_mmSLE_2015_2100) / global_slr_mmSLE_raw_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_correction_perc_median'] = np.median(\n",
    "        100 * (global_slr_mmSLE_raw_2015_2100 - global_slr_mmSLE_2015_2100) / global_slr_mmSLE_raw_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_correction_perc_std'] = np.std(\n",
    "        100 * (global_slr_mmSLE_raw_2015_2100 - global_slr_mmSLE_2015_2100) / global_slr_mmSLE_raw_2015_2100)\n",
    "    global_df.loc[scenario,'SLR_mmsle_2015_2100_correction_perc_95'] = 1.96*np.std(\n",
    "        100 * (global_slr_mmSLE_raw_2015_2100 - global_slr_mmSLE_2015_2100) / global_slr_mmSLE_raw_2015_2100)\n",
    "    \n",
    "    \n",
    "# global_cns = []\n",
    "# for cn in global_cns_pre:\n",
    "#     for scenario in scenarios:\n",
    "#         global_cns.append(cn + '-' + scenario)\n",
    "\n",
    "global_df.to_csv(results_fp + 'global_discharge_stats_no5-19.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a51820-fc85-45b7-9f27-1e4bd8736eb4",
   "metadata": {},
   "source": [
    "## Global Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4c1f8-2ed2-4422-8b5e-fc7885e52255",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_each_gcm = True\n",
    "add_only_adjusted = True\n",
    "add_fill_between = True\n",
    "if add_fill_between:\n",
    "    add_each_gcm = False\n",
    "scenarios_uncertainty = ['ssp126', 'ssp585']\n",
    "smooth_results = True\n",
    "lw_main = 1.1\n",
    "alpha_single = 0.5\n",
    "\n",
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(nrows=2, ncols=1, hspace=0.07, height_ratios=[3,2])\n",
    "gs0 = gs[0].subgridspec(2,2, height_ratios=[2,1], width_ratios=[6,1], wspace=0.02, hspace=0.1)\n",
    "gs1 = gs[1].subgridspec(1,2, width_ratios=[6,1], wspace=0.02)\n",
    "\n",
    "ax0 = fig.add_subplot(gs0[1,0])\n",
    "ax0box = fig.add_subplot(gs0[1,1])\n",
    "ax1 = fig.add_subplot(gs0[0,0])\n",
    "ax1box = fig.add_subplot(gs0[0,1])\n",
    "ax2 = fig.add_subplot(gs1[0,0])\n",
    "ax2box = fig.add_subplot(gs1[0,1])\n",
    "\n",
    "# Flip order\n",
    "# fig = plt.figure()\n",
    "# gs = fig.add_gridspec(nrows=2, ncols=1, hspace=0.07, height_ratios=[3,2])\n",
    "# gs0 = gs[0].subgridspec(2,2, height_ratios=[1,2], width_ratios=[10,1], wspace=0.02, hspace=0.1)\n",
    "# gs1 = gs[1].subgridspec(1,2, width_ratios=[10,1], wspace=0.02)\n",
    "\n",
    "# ax0 = fig.add_subplot(gs0[0,0])\n",
    "# ax0box = fig.add_subplot(gs0[0,1])\n",
    "# ax1 = fig.add_subplot(gs0[1,0])\n",
    "# ax1box = fig.add_subplot(gs0[1,1])\n",
    "# ax2 = fig.add_subplot(gs1[0,0])\n",
    "# ax2box = fig.add_subplot(gs1[0,1])\n",
    "\n",
    "# ----- Load Data -----\n",
    "data_boxplot_fraction = []\n",
    "data_boxplot_qpg = []\n",
    "data_boxplot_slr = []\n",
    "for nscenario, scenario in enumerate(scenarios[::-1]):\n",
    "    ds_wbm_fp = 'wbm_processed/basin_components_yearly-wibt-common/'\n",
    "    ds_wbm_fn = 'wbm_' + scenario + '_2016_2099_v4-basin_components_yearly-wibt-common.nc'\n",
    "    ds_wbm = xr.open_dataset(ds_wbm_fp + ds_wbm_fn)\n",
    "    \n",
    "    basin_ids_nogreenland_idx = [wbm_ids_init.index(x) for x in basin_ids_nogreenland]\n",
    "    basin_ids_nogreenland_exor_idx = [wbm_ids_init.index(x) for x in basin_ids_nogreenland_exor]\n",
    "    basin_ids_nogreenland_endo_idx = [wbm_ids_init.index(x) for x in basin_ids_nogreenland_endo]\n",
    "    \n",
    "    # Values of interest\n",
    "    glmelt = ds_wbm.glmelt.values\n",
    "    q_pg = ds_wbm.discharge_pg.values\n",
    "    \n",
    "    glmelt_nogreenland = glmelt[:,basin_ids_nogreenland_idx,:].sum(1)\n",
    "    q_pg_nogreenland_exor = q_pg[:,basin_ids_nogreenland_exor_idx,:].sum(1)\n",
    "    frac_ocean_nogreenland = 100 * q_pg_nogreenland_exor / glmelt_nogreenland\n",
    "\n",
    "    if smooth_results:\n",
    "        glmelt_nogreenland = uniform_filter(glmelt_nogreenland, size=(11))\n",
    "        q_pg_nogreenland_exor = uniform_filter(q_pg_nogreenland_exor, size=(11))\n",
    "        frac_ocean_nogreenland = uniform_filter(frac_ocean_nogreenland, size=(11))\n",
    "\n",
    "    data_boxplot_fraction.append(frac_ocean_nogreenland[:,-1])\n",
    "    data_boxplot_qpg.append(q_pg_nogreenland_exor[:,-1] / 1e3)\n",
    "    data_boxplot_qpg.append(glmelt_nogreenland[:,-1] / 1e3)\n",
    "    \n",
    "    years = ds_wbm.year.values\n",
    "    glmelt_nogreenland_median = np.median(glmelt_nogreenland, axis=0)\n",
    "    q_pg_nogreenland_exor_median = np.median(q_pg_nogreenland_exor,axis=0)\n",
    "    frac_ocean_nogreenland_median = np.median(frac_ocean_nogreenland,axis=0)\n",
    "    \n",
    "    glmelt_nogreenland_std = np.std(glmelt_nogreenland, axis=0)\n",
    "    q_pg_nogreenland_exor_std = np.std(q_pg_nogreenland_exor,axis=0)\n",
    "    frac_ocean_nogreenland_std = np.std(frac_ocean_nogreenland,axis=0)\n",
    "    glmelt_nogreenland_min = np.min(glmelt_nogreenland, axis=0)\n",
    "    q_pg_nogreenland_exor_min = np.min(q_pg_nogreenland_exor,axis=0)\n",
    "    frac_ocean_nogreenland_min = np.min(frac_ocean_nogreenland,axis=0)\n",
    "    glmelt_nogreenland_max = np.max(glmelt_nogreenland, axis=0)\n",
    "    q_pg_nogreenland_exor_max = np.max(q_pg_nogreenland_exor,axis=0)\n",
    "    frac_ocean_nogreenland_max = np.max(frac_ocean_nogreenland,axis=0)\n",
    "\n",
    "    \n",
    "    # ----- FRACTION REACHING OCEAN -----\n",
    "    ax0.plot(years, frac_ocean_nogreenland_median, color=ssp_colordict[scenario], lw=lw_main, zorder=2)\n",
    "    if add_fill_between and scenario in scenarios_uncertainty:\n",
    "        ax0.fill_between(years, frac_ocean_nogreenland_min, \n",
    "                         frac_ocean_nogreenland_max, \n",
    "                         color=ssp_colordict[scenario], alpha=0.2, lw=0)\n",
    "        # ax0.fill_between(years, frac_ocean_nogreenland_median - 1.96*frac_ocean_nogreenland_std, \n",
    "        #                  frac_ocean_nogreenland_median + 1.96*frac_ocean_nogreenland_std, \n",
    "        #                  color=ssp_colordict[scenario], alpha=0.2, lw=0)\n",
    "    elif add_each_gcm and scenario in scenarios_uncertainty:\n",
    "        for ngcm, gcm in enumerate(gcm_names):\n",
    "            frac_ocean_nogreenland_single = frac_ocean_nogreenland[ngcm,:]\n",
    "            ax0.plot(years, frac_ocean_nogreenland_single, color=ssp_colordict[scenario], lw=0.5, alpha=alpha_single, zorder=1)\n",
    "    ax0.set_ylabel('Runoff\\nto Ocean\\n(%)', size=12)\n",
    "    ax0.yaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax0.yaxis.set_minor_locator(MultipleLocator(1))\n",
    "    if smooth_results:\n",
    "        frac_ylim_min, frac_ylim_max = 92.5, 97\n",
    "        ax0.set_ylim(frac_ylim_min, frac_ylim_max)\n",
    "    else:\n",
    "        frac_ylim_min, frac_ylim_max = 90.01, 98\n",
    "        ax0.set_ylim(frac_ylim_min, frac_ylim_max)\n",
    "    ax0.set_xlim(2015,2100)\n",
    "    ax0.xaxis.set_major_locator(MultipleLocator(20))\n",
    "    ax0.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax0.axes.xaxis.set_ticklabels([])\n",
    "    ax0.tick_params(axis='both', which='major', direction='inout', right=True)\n",
    "    ax0.tick_params(axis='both', which='minor', direction='in', right=True)\n",
    "\n",
    "    if nscenario == 0:\n",
    "        ax0.text(0.01, 0.99, 'b', fontsize=10, weight='bold', ha='left', va='top', transform=ax0.transAxes)\n",
    "            \n",
    "    # ----- DISCHARGE -----\n",
    "    ax1.plot(years, glmelt_nogreenland_median / 1e3, color=ssp_colordict[scenario], \n",
    "             linestyle='--', linewidth=lw_main, zorder=2, label=None)\n",
    "    ax1.plot(years, q_pg_nogreenland_exor_median / 1e3, color=ssp_colordict[scenario], \n",
    "             linestyle='-', linewidth=lw_main, zorder=3, label=None)\n",
    "    if add_fill_between and scenario in scenarios_uncertainty:\n",
    "        # ax1.fill_between(years, (q_pg_nogreenland_exor_median - 1.96*q_pg_nogreenland_exor_std) / 1e3, \n",
    "        #                  (q_pg_nogreenland_exor_median + 1.96*q_pg_nogreenland_exor_std) / 1e3, \n",
    "        #                  color=ssp_colordict[scenario], alpha=0.2, lw=0)\n",
    "        ax1.fill_between(years, q_pg_nogreenland_exor_min / 1e3, \n",
    "                         q_pg_nogreenland_exor_max / 1e3, \n",
    "                         color=ssp_colordict[scenario], alpha=0.2, lw=0)\n",
    "    elif add_each_gcm and scenario in scenarios_uncertainty:\n",
    "        for ngcm, gcm in enumerate(gcm_names):\n",
    "            q_pg_nogreenland_exor_single = q_pg_nogreenland_exor[ngcm,:]\n",
    "            if not add_only_adjusted:\n",
    "                glmelt_nogreenland_single = glmelt_nogreenland[ngcm,:]\n",
    "            ax1.plot(years, q_pg_nogreenland_exor_single / 1e3, color=ssp_colordict[scenario], ls='-', lw=0.5, alpha=alpha_single, zorder=1)\n",
    "            ax1.plot(years, glmelt_nogreenland_single / 1e3, color=ssp_colordict[scenario], ls='--', lw=0.5, alpha=alpha_single, zorder=1)\n",
    "    ax1.set_ylabel('Glacier\\nRunoff\\n(10$^3$ m$^3$/s)', size=12)\n",
    "    ax1.yaxis.set_major_locator(MultipleLocator(10))\n",
    "    ax1.yaxis.set_minor_locator(MultipleLocator(5))\n",
    "    if smooth_results:\n",
    "        q_ylim_min, q_ylim_max = 14, 35\n",
    "        ax1.set_ylim(q_ylim_min, q_ylim_max)\n",
    "    else:\n",
    "        q_ylim_min, q_ylim_max = 11, 44\n",
    "        ax1.set_ylim(q_ylim_min, q_ylim_max)\n",
    "    ax1.set_xlim(2015,2100)\n",
    "    ax1.xaxis.set_major_locator(MultipleLocator(20))\n",
    "    ax1.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax1.axes.xaxis.set_ticklabels([])\n",
    "\n",
    "    if nscenario == 0:\n",
    "        if add_each_gcm or add_fill_between:\n",
    "            loc_legend = (0.03,0.03)\n",
    "        else:\n",
    "            loc_legend = (0.03,0.03)\n",
    "        ax1b = ax1.twinx()\n",
    "        ax1b.plot([0,0],[0,0], color='grey', linestyle='--', linewidth=lw_main, zorder=1, label='at Terminus')\n",
    "        ax1b.plot([0,0],[0,0], color='grey', linestyle='-', linewidth=lw_main, zorder=1, label='at Ocean')\n",
    "        ax1b.axes.get_yaxis().set_visible(False)\n",
    "        ax1b.legend(loc=loc_legend, fontsize=10, ncol=1, columnspacing=0.5, labelspacing=0.25, \n",
    "                   handlelength=1, handletextpad=0.25, borderpad=0, frameon=False)\n",
    "        ax1b.set_axis_off()\n",
    "        \n",
    "        ax1.tick_params(axis='both', which='major', direction='inout', right=True)\n",
    "        ax1.tick_params(axis='both', which='minor', direction='in', right=True)\n",
    "    \n",
    "        ax1.text(0.01, 0.99, 'a', fontsize=10, weight='bold', ha='left', va='top', transform=ax1.transAxes)\n",
    "\n",
    "# ----- Sea Level Contribution -----\n",
    "for scenario in scenarios[::-1]:\n",
    "\n",
    "    # ----- PROCESSING -----\n",
    "    nscenario = list(ds_slr.scenarios.values).index(scenario)\n",
    "    \n",
    "    # Uncommon (100% contributors)\n",
    "    slr_mmSLE_uncommon_basins = ds_slr.slr_mmSLE_uncommon.values[nscenario,:,:,:]\n",
    "    slr_mmSLE_uncommon_2015_2100_annual = slr_mmSLE_uncommon_basins.sum(1)[:,year_start_idx:]\n",
    "\n",
    "    # Endorheic and Exorheic Basins\n",
    "    # These basin are \"common\" (shared) between PyGEM and WBM\n",
    "    slr_mmSLE_common = ds_slr.slr_mmSLE_common.values[nscenario,:,:,:]\n",
    "    slr_mmSLE_common_raw = ds_slr.slr_mmSLE_common_raw.values[nscenario,:,:,:]\n",
    "    endo_flag = ds_slr.endo_flag.values\n",
    "    endo_idx = np.where(endo_flag == 1)[0]\n",
    "    exor_idx = np.where(endo_flag == 0)[0]\n",
    "\n",
    "    # -- ENDORHEIC Contributions --\n",
    "    slr_mmSLE_endo_raw_basins = slr_mmSLE_common_raw[:, endo_idx, :]\n",
    "    slr_mmSLE_endo_raw_2015_2100_annual = slr_mmSLE_endo_raw_basins.sum(1)[:,year_start_idx:]\n",
    "\n",
    "    # -- EXORHEIC Contributions --\n",
    "    slr_mmSLE_exor_basins = slr_mmSLE_common[:, exor_idx, :]\n",
    "    slr_mmSLE_exor_2015_2100_annual = slr_mmSLE_exor_basins.sum(1)[:,year_start_idx:]\n",
    "\n",
    "    slr_mmSLE_exor_raw_basins = slr_mmSLE_common_raw[:, exor_idx, :]\n",
    "    slr_mmSLE_exor_raw_2015_2100_annual = slr_mmSLE_exor_raw_basins.sum(1)[:,year_start_idx:]\n",
    "    \n",
    "    # ----- TOTAL CONTRIBUTIONS-----\n",
    "    # Adjusted total\n",
    "    slr_mmSLE_total_2015_2100_annual = slr_mmSLE_uncommon_2015_2100_annual + slr_mmSLE_exor_2015_2100_annual\n",
    "\n",
    "    # Unadjusted (Raw) total\n",
    "    slr_mmSLE_raw_total_2015_2100_annual = (slr_mmSLE_uncommon_2015_2100_annual + slr_mmSLE_exor_raw_2015_2100_annual + \n",
    "                                            slr_mmSLE_endo_raw_2015_2100_annual)\n",
    "\n",
    "    # Cumulative Sum\n",
    "    global_slr_mmSLE_cumsum_2015_2100 = np.cumsum(slr_mmSLE_total_2015_2100_annual, axis=1)\n",
    "    global_slr_mmSLE_raw_cumsum_2015_2100 = np.cumsum(slr_mmSLE_raw_total_2015_2100_annual, axis=1)\n",
    "    \n",
    "    # ----- END PROCESSING -----\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # global_slr_mmSLE = reg_annual_slr_mmSLE_dict[scenario]['all']\n",
    "    # global_slr_mmSLE_raw = reg_annual_slr_mmSLE_raw_dict[scenario]['all']\n",
    "    \n",
    "    # years_mass = ds_mass.year.values\n",
    "    # years_mass_start_idx = np.where(years_mass == 2015)[0][0]\n",
    "    years_slr = years_mass[years_mass_start_idx:-1]\n",
    "    \n",
    "    # global_slr_mmSLE_cumsum_2015_2100 = np.cumsum(global_slr_mmSLE[:,years_mass_start_idx:], axis=1)\n",
    "    # global_slr_mmSLE_raw_cumsum_2015_2100 = np.cumsum(global_slr_mmSLE_raw[:,years_mass_start_idx:], axis=1)\n",
    "\n",
    "    data_boxplot_slr.append(global_slr_mmSLE_cumsum_2015_2100[:,-1])\n",
    "    data_boxplot_slr.append(global_slr_mmSLE_raw_cumsum_2015_2100[:,-1])\n",
    "    \n",
    "    global_slr_mmSLE_cumsum_2015_2100_median = np.median(global_slr_mmSLE_cumsum_2015_2100, axis=0)\n",
    "    global_slr_mmSLE_raw_cumsum_2015_2100_median = np.median(global_slr_mmSLE_raw_cumsum_2015_2100, axis=0)\n",
    "\n",
    "    global_slr_mmSLE_cumsum_2015_2100_std = np.std(global_slr_mmSLE_cumsum_2015_2100, axis=0)\n",
    "    global_slr_mmSLE_raw_cumsum_2015_2100_std = np.std(global_slr_mmSLE_raw_cumsum_2015_2100, axis=0)\n",
    "    global_slr_mmSLE_cumsum_2015_2100_min = np.min(global_slr_mmSLE_cumsum_2015_2100, axis=0)\n",
    "    global_slr_mmSLE_raw_cumsum_2015_2100_min = np.min(global_slr_mmSLE_raw_cumsum_2015_2100, axis=0)\n",
    "    global_slr_mmSLE_cumsum_2015_2100_max = np.max(global_slr_mmSLE_cumsum_2015_2100, axis=0)\n",
    "    global_slr_mmSLE_raw_cumsum_2015_2100_max = np.max(global_slr_mmSLE_raw_cumsum_2015_2100, axis=0)\n",
    "    \n",
    "    ax2.plot(years_mass[year_start_idx:-1], global_slr_mmSLE_cumsum_2015_2100_median, color=ssp_colordict[scenario], \n",
    "             linestyle='-', linewidth=lw_main, zorder=3, label=ssp_name_dict[scenario])\n",
    "    ax2.plot(years_mass[year_start_idx:-1], global_slr_mmSLE_raw_cumsum_2015_2100_median, color=ssp_colordict[scenario], \n",
    "             linestyle='--',linewidth=lw_main, zorder=2)\n",
    "    if add_fill_between and scenario in scenarios_uncertainty:\n",
    "        ax2.fill_between(years_mass[year_start_idx:-1], global_slr_mmSLE_cumsum_2015_2100_min, \n",
    "                         global_slr_mmSLE_cumsum_2015_2100_max, \n",
    "                         color=ssp_colordict[scenario], alpha=0.2, lw=0)\n",
    "        # ax2.fill_between(years_mass[year_start_idx:-1], global_slr_mmSLE_cumsum_2015_2100_median - 1.96*global_slr_mmSLE_cumsum_2015_2100_std, \n",
    "        #                  global_slr_mmSLE_cumsum_2015_2100_median + 1.96*global_slr_mmSLE_cumsum_2015_2100_std, \n",
    "        #                  color=ssp_colordict[scenario], alpha=0.2, lw=0)\n",
    "    elif add_each_gcm and scenario in scenarios_uncertainty:\n",
    "        for ngcm, gcm in enumerate(gcm_names):\n",
    "            global_slr_mmSLE_cumsum_2015_2100_single = global_slr_mmSLE_cumsum_2015_2100[ngcm,:]\n",
    "            global_slr_mmSLE_raw_cumsum_2015_2100_single = global_slr_mmSLE_raw_cumsum_2015_2100[ngcm,:]\n",
    "            ax2.plot(years_mass[year_start_idx:-1], global_slr_mmSLE_cumsum_2015_2100_single, color=ssp_colordict[scenario], \n",
    "                     ls='-', lw=0.5, alpha=alpha_single, zorder=1)\n",
    "            if not add_only_adjusted:\n",
    "                ax2.plot(years_mass[year_start_idx:-1], global_slr_mmSLE_raw_cumsum_2015_2100_single, color=ssp_colordict[scenario], \n",
    "                         ls='--', lw=0.5, alpha=alpha_single, zorder=1)\n",
    "\n",
    "# ax2b = ax2.twinx()\n",
    "# ax2b.plot([0,0],[0,0], color='grey', linestyle='--', linewidth=lw_main, zorder=1, label='All (assumed)')\n",
    "# ax2b.plot([0,0],[0,0], color='grey', linestyle='-', linewidth=lw_main, zorder=1, label='Direct only')\n",
    "# ax2b.set_axis_off()\n",
    "# ax2b.legend(loc=(0.38,0.69), fontsize=10, ncol=1, columnspacing=0.5, labelspacing=0.25, \n",
    "#            handlelength=1, handletextpad=0.25, borderpad=0, frameon=False)\n",
    "\n",
    "ax2.set_ylabel('Sea-Level\\nRise\\n(mm)', size=12)\n",
    "slr_ylim_min, slr_ylim_max = 0, 240\n",
    "ax2.set_ylim(slr_ylim_min, slr_ylim_max)\n",
    "ax2.yaxis.set_major_locator(MultipleLocator(50))\n",
    "ax2.yaxis.set_minor_locator(MultipleLocator(10))\n",
    "ax2.set_xlim(2015,2100)\n",
    "ax2.xaxis.set_major_locator(MultipleLocator(20))\n",
    "ax2.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "ax2.tick_params(axis='both', which='major', direction='inout', right=True)\n",
    "ax2.tick_params(axis='both', which='minor', direction='in', right=True)\n",
    "ax2.legend(loc=(0.06,0.40), fontsize=10, ncol=1, columnspacing=0.5, labelspacing=0.25, \n",
    "           handlelength=1, handletextpad=0.25, borderpad=0, frameon=False)\n",
    "\n",
    "ax2.text(0.01, 0.99, 'c', fontsize=10, weight='bold', ha='left', va='top', transform=ax2.transAxes)\n",
    "\n",
    "\n",
    "# ----- BOX-AND-WHISKER PLOTS -----\n",
    "# FRACTION REACHING OCEAN\n",
    "lw_boxplot = 1\n",
    "lw_adjust_value = 0.8\n",
    "data_boxplot_fraction = data_boxplot_fraction[::-1]\n",
    "bp = ax0box.boxplot(data_boxplot_fraction, whis=100)\n",
    "ax0box.set_ylim(frac_ylim_min, frac_ylim_max)\n",
    "for nbox, box in enumerate(bp['boxes']):\n",
    "    scenario = scenarios[nbox]\n",
    "    # change outline color\n",
    "    box.set(color=ssp_colordict[scenario], linewidth=lw_boxplot)\n",
    "for nitem, item in enumerate(bp['medians']):\n",
    "    scenario = scenarios[nitem]\n",
    "    # change outline color\n",
    "    item.set(color=ssp_colordict[scenario], linewidth=lw_boxplot)\n",
    "for nitem, item in enumerate(bp['whiskers']):\n",
    "    scenario = scenarios[int(np.floor(nitem/2))]\n",
    "    # change outline color\n",
    "    item.set(color=ssp_colordict[scenario], linewidth=lw_boxplot)\n",
    "for nitem, item in enumerate(bp['caps']):\n",
    "    scenario = scenarios[int(np.floor(nitem/2))]\n",
    "    # change outline color\n",
    "    item.set(color=ssp_colordict[scenario], linewidth=lw_boxplot)\n",
    "# turn off axes\n",
    "ax0box.get_yaxis().set_visible(False)\n",
    "ax0box.get_xaxis().set_visible(False)\n",
    "ax0box.axis('off')\n",
    "\n",
    "# DISCHARGE\n",
    "data_boxplot_qpg = data_boxplot_qpg[::-1]\n",
    "bp = ax1box.boxplot(data_boxplot_qpg, whis=100)\n",
    "ax1box.set_ylim(q_ylim_min, q_ylim_max)\n",
    "scenarios_q = list(np.repeat(scenarios,2))\n",
    "for nbox, box in enumerate(bp['boxes']):\n",
    "    scenario = scenarios_q[nbox]\n",
    "    if nbox%2 == 0:\n",
    "        ls_boxplot = '--'\n",
    "        lw_adjust = lw_adjust_value \n",
    "    else:\n",
    "        ls_boxplot = '-'\n",
    "        lw_adjust = 1\n",
    "    # change outline color\n",
    "    box.set(color=ssp_colordict[scenario], linewidth=lw_adjust*lw_boxplot, ls=ls_boxplot)\n",
    "for nitem, item in enumerate(bp['medians']):\n",
    "    scenario = scenarios_q[nitem]\n",
    "    ls_boxplot = '-'\n",
    "    # change outline color\n",
    "    item.set(color=ssp_colordict[scenario], linewidth=lw_boxplot, ls=ls_boxplot)\n",
    "for nitem, item in enumerate(bp['whiskers']):\n",
    "    scenario = scenarios_q[int(np.floor(nitem/2))]\n",
    "    if int(np.floor(nitem/2))%2 == 0:\n",
    "        ls_boxplot = '--'\n",
    "        lw_adjust = lw_adjust_value \n",
    "    else:\n",
    "        ls_boxplot = '-'\n",
    "        lw_adjust = 1\n",
    "    # change outline color\n",
    "    item.set(color=ssp_colordict[scenario], linewidth=lw_adjust*lw_boxplot, ls=ls_boxplot)\n",
    "for nitem, item in enumerate(bp['caps']):\n",
    "    scenario = scenarios_q[int(np.floor(nitem/2))]\n",
    "    ls_boxplot = '-'\n",
    "    # change outline color\n",
    "    item.set(color=ssp_colordict[scenario], linewidth=lw_boxplot, ls=ls_boxplot)\n",
    "# turn off axes\n",
    "ax1box.get_yaxis().set_visible(False)\n",
    "ax1box.get_xaxis().set_visible(False)\n",
    "ax1box.axis('off')\n",
    "\n",
    "# SEA-LEVEL RISE\n",
    "data_boxplot_slr = data_boxplot_slr[::-1]\n",
    "bp = ax2box.boxplot(data_boxplot_slr, whis=100)\n",
    "ax2box.set_ylim(slr_ylim_min, slr_ylim_max)\n",
    "scenarios_q = list(np.repeat(scenarios,2))\n",
    "for nbox, box in enumerate(bp['boxes']):\n",
    "    scenario = scenarios_q[nbox]\n",
    "    if nbox%2 == 0:\n",
    "        ls_boxplot = '--'\n",
    "        lw_adjust = lw_adjust_value \n",
    "    else:\n",
    "        ls_boxplot = '-'\n",
    "        lw_adjust = 1\n",
    "    # change outline color\n",
    "    box.set(color=ssp_colordict[scenario], linewidth=lw_adjust*lw_boxplot, ls=ls_boxplot)\n",
    "for nitem, item in enumerate(bp['medians']):\n",
    "    scenario = scenarios_q[nitem]\n",
    "    ls_boxplot = '-'\n",
    "    # change outline color\n",
    "    item.set(color=ssp_colordict[scenario], linewidth=lw_boxplot, ls=ls_boxplot)\n",
    "for nitem, item in enumerate(bp['whiskers']):\n",
    "    scenario = scenarios_q[int(np.floor(nitem/2))]\n",
    "    if int(np.floor(nitem/2))%2 == 0:\n",
    "        ls_boxplot = '--'\n",
    "        lw_adjust = lw_adjust_value \n",
    "    else:\n",
    "        ls_boxplot = '-'\n",
    "        lw_adjust = 1\n",
    "    # change outline color\n",
    "    item.set(color=ssp_colordict[scenario], linewidth=lw_adjust*lw_boxplot, ls=ls_boxplot)\n",
    "for nitem, item in enumerate(bp['caps']):\n",
    "    scenario = scenarios_q[int(np.floor(nitem/2))]\n",
    "    ls_boxplot = '-'\n",
    "    # change outline color\n",
    "    item.set(color=ssp_colordict[scenario], linewidth=lw_boxplot, ls=ls_boxplot)\n",
    "# turn off axes\n",
    "ax2box.get_yaxis().set_visible(False)\n",
    "ax2box.get_xaxis().set_visible(False)\n",
    "ax2box.axis('off')\n",
    "\n",
    "\n",
    "fig_fn = 'Fig1_Global_Discharge_SLR-all_scenarios.png'\n",
    "fig_fp = results_fp + '_figures/'\n",
    "if not os.path.exists(fig_fp):\n",
    "    os.makedirs(fig_fp)\n",
    "fig.set_size_inches(4,4)\n",
    "fig.savefig(fig_fp + fig_fn, bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b83d1b-3cb4-4958-9306-53b16e6f01df",
   "metadata": {},
   "source": [
    "## WBM Basins of runoff and runoff reaching ocean\n",
    "Note: exor_frac_option of 2 was used to produce the sea-level rise contributions, which assumes the fraction of glacier mass change that reaches the ocean varies annually for each GCM/SSP.\n",
    "\n",
    "- Show the actual runoff time series of the largest basins contributing to sea-level rise that are also affected\n",
    "\n",
    "- Table of largest basins\n",
    "  - Basin, Basin size, Glacier Runoff, Glacier Runoff Reaching Ocean, % reaching ocean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc23e8-283b-44fb-bb27-ef684daac3a6",
   "metadata": {},
   "source": [
    "#### Find the basins with greatest change in their contribution to sea-level rise\n",
    "Use average of GCMs for SSP2-4.5 for estimating the total contributions from 2015-2099.\n",
    "\n",
    "Compare slr_mmSLE and slr_mmSLE_common_raw to find difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017d665-d8d2-42ba-a38e-a117bbbb3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_common_ids = ds_slr_endo_merged.basin_ids_common.values\n",
    "basin_common_endo_flag = ds_slr_endo_merged.endo_flag.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e9dc1-014f-4e20-ba98-70d952f9d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual contribution to sea-level rise by basin\n",
    "basin_slr_mmSLE_common = ds_slr_endo_merged.slr_mmSLE_common.values[1,:,:,year_start_idx:]\n",
    "basin_slr_mmSLE_common_total = np.sum(basin_slr_mmSLE_common, axis=2)\n",
    "\n",
    "# Contribution to sea-level rise by basin prior to accounting for hydrology\n",
    "basin_slr_mmSLE_common_raw = ds_slr_endo_merged.slr_mmSLE_common_raw.values[1,:,:,year_start_idx:]\n",
    "basin_slr_mmSLE_common_raw_total = np.sum(basin_slr_mmSLE_common_raw, axis=2)\n",
    "basin_slr_mmSLE_common_raw_total_median = np.median(basin_slr_mmSLE_common_raw_total, axis=0)\n",
    "\n",
    "# Difference in contribution\n",
    "basin_slr_mmSLE_common_total_difference = basin_slr_mmSLE_common_total - basin_slr_mmSLE_common_raw_total\n",
    "basin_slr_mmSLE_common_total_difference_median = np.median(basin_slr_mmSLE_common_total_difference, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ec965-f27e-4929-87e4-165b4b7c327f",
   "metadata": {},
   "source": [
    "#### Select top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64941c2-c0cc-4474-8303-114c430c9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop = 20\n",
    "ntop_exor = 10\n",
    "\n",
    "df_basins = pd.read_csv(basin_csv_fn)\n",
    "df_basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50c10f-d36f-4bd2-b4bb-b9000577f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_index = np.argsort(basin_slr_mmSLE_common_total_difference_median)\n",
    "print('Greatest Change Basins:\\n', 'rank, idx, basin_id, endo_flag, difference (mmSLE)')\n",
    "for n in np.arange(ntop):\n",
    "    ds_idx = sort_index[n]\n",
    "    basin_id = basin_common_ids[ds_idx]\n",
    "    basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "    print(' ',n, basin_id, basin_name, basin_common_endo_flag[ds_idx], np.round(basin_slr_mmSLE_common_total_difference_median[ds_idx],2))\n",
    "\n",
    "print('\\nGreatest Change Basins (Exorheic only):\\n', 'rank, idx, basin_id, endo_flag, difference (mmSLE)', 'raw contribution (mmSLE)')\n",
    "count = 0\n",
    "basin_ids_of_interest_exor = []\n",
    "for n in np.arange(sort_index.shape[0]):\n",
    "    ds_idx = sort_index[n]\n",
    "    basin_id = basin_common_ids[ds_idx]\n",
    "    basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "    if count < ntop_exor:\n",
    "        if basin_common_endo_flag[ds_idx] == 0:\n",
    "            print(' ',count, basin_id, basin_name, basin_common_endo_flag[ds_idx], \n",
    "                  np.round(basin_slr_mmSLE_common_total_difference_median[ds_idx],2),\n",
    "                  np.round(basin_slr_mmSLE_common_raw_total_median[ds_idx],2))\n",
    "            count += 1\n",
    "            basin_ids_of_interest_exor.append(basin_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f26204-9fd7-4425-9e19-2aab9ff6422f",
   "metadata": {},
   "source": [
    "#### Take-away:\n",
    "- Endorheic basins account for some of greatest changes (e.g., 33 is in the Tarim Basin where there's considerable mass)\n",
    "- Exorheic basins with greatest change are those with large glaciers and/or human presence: Indus (23), Ganges (14), Copper (226), Columbia (32), Chang Jian (11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0ab28-42d5-401e-b495-a5197c6848c1",
   "metadata": {},
   "source": [
    "## Find Largest basins\n",
    "This allows plots to focus on glaciers above a certain size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52787eec-222a-4026-9c72-f0c4e5c42e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgiids_wbasins_fn = 'RGI60_wMERIT_IDs_endo_merged.csv'\n",
    "# rgiids_wbasins_fn = 'RGI60_wMERIT_IDs.csv'\n",
    "rgi_wbasins_df = pd.read_csv(rgiids_wbasins_fn)\n",
    "\n",
    "basins_glaciers = list(np.unique(rgi_wbasins_df.MeritID))\n",
    "basin_glacier_area_df = rgi_wbasins_df.groupby('MeritID', as_index=False)['Area'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5495b8-eda2-4d8d-8e96-3f1e14dfa31d",
   "metadata": {},
   "source": [
    "## Plot top basins with a box plot for the amount reduced per SSP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d572e92-360f-40ae-b49c-02e7508b6232",
   "metadata": {},
   "source": [
    "Calculate fraction reaching the ocean for each basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2e25b-55ea-4dad-9eb9-55f28aeeef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_greenland = True\n",
    "basin_frac_dict = {}\n",
    "basin_reduction_dict = {}\n",
    "basin_contribution_dict = {}\n",
    "for nscenario, scenario in enumerate(scenarios):\n",
    "    basin_frac_dict[scenario] = {}\n",
    "    basin_reduction_dict[scenario] = {}\n",
    "    basin_contribution_dict[scenario] = {}\n",
    "\n",
    "    year_start = 2016\n",
    "    year_start_idx = np.where(years_mass == year_start)[0][0]\n",
    "\n",
    "    # ----- Common Basins (Endorheic and Exorheic) -----\n",
    "    basin_ids_common = ds_slr_endo_merged.basin_ids_common.values\n",
    "    endo_flag = ds_slr_endo_merged.endo_flag.values\n",
    "    slr_mmSLE_common = ds_slr_endo_merged.slr_mmSLE_common.values[nscenario,:,:,:]\n",
    "    slr_mmSLE_common_raw = ds_slr_endo_merged.slr_mmSLE_common_raw.values[nscenario,:,:,:]\n",
    "\n",
    "    # -- Exorheic --\n",
    "    exor_idx = np.where(endo_flag == 0)[0]\n",
    "    basin_ids_common_exor = basin_ids_common[exor_idx]\n",
    "    # slr adjusted for downstream hydrology\n",
    "    slr_mmSLE_exor_2015_2100 = slr_mmSLE_common[:, exor_idx, year_start_idx:].sum(2)\n",
    "    slr_mmSLE_exor_2015_2100_median = np.median(slr_mmSLE_exor_2015_2100, axis=0)\n",
    "    # slr unadjusted for downstream hydrology (assuming 100% contribution)\n",
    "    slr_mmSLE_exor_raw_2015_2100 = slr_mmSLE_common_raw[:, exor_idx, year_start_idx:].sum(2)\n",
    "    slr_mmSLE_exor_raw_2015_2100_median = np.median(slr_mmSLE_exor_raw_2015_2100, axis=0)\n",
    "    # Fraction remaining\n",
    "    slr_2015_2100_frac_remaining_exor = slr_mmSLE_exor_2015_2100 / slr_mmSLE_exor_raw_2015_2100\n",
    "    slr_2015_2100_frac_remaining_exor_median = np.median(slr_2015_2100_frac_remaining_exor, axis=0)\n",
    "    # Reduction \n",
    "    slr_2015_2100_dif_exor = slr_mmSLE_exor_raw_2015_2100 - slr_mmSLE_exor_2015_2100\n",
    "    slr_2015_2100_dif_exor_median = np.median(slr_2015_2100_dif_exor, axis=0)\n",
    "\n",
    "    # -- Endorheic --\n",
    "    endo_idx = np.where(endo_flag == 1)[0]\n",
    "    basin_ids_common_endo = basin_ids_common[endo_idx]\n",
    "    slr_mmSLE_endo_2015_2100 = slr_mmSLE_common_raw[:, endo_idx, year_start_idx:].sum(2)\n",
    "    slr_mmSLE_endo_2015_2100_median = np.median(slr_mmSLE_endo_2015_2100, axis=0)\n",
    "\n",
    "    # Fraction Reaching Ocean Dictionary\n",
    "    for basin_id in basin_ids_common_endo:\n",
    "        basin_frac_dict[scenario][basin_id] = 0\n",
    "    for nid, basin_id in enumerate(basin_ids_common_exor):\n",
    "        basin_frac_dict[scenario][basin_id] = slr_2015_2100_frac_remaining_exor_median[nid]\n",
    "\n",
    "    # Uncommon basins\n",
    "    basin_ids_greenland_extra = list(df_basins.loc[df_basins['SubContinentName'] == 'Greenland', 'ID'])\n",
    "    for basin_id in basin_ids_uncommon:\n",
    "        if basin_id in basin_ids_greenland or basin_id in basin_ids_greenland_extra:\n",
    "            basin_frac_dict[scenario][basin_id] = np.nan\n",
    "        else:\n",
    "            basin_frac_dict[scenario][basin_id] = 1\n",
    "\n",
    "    # ----- Reduction & Contribution Dictionaries -----\n",
    "    for nid, basin_id in enumerate(basin_ids_common_exor):\n",
    "        basin_reduction_dict[scenario][basin_id] = slr_2015_2100_dif_exor_median[nid]\n",
    "        basin_contribution_dict[scenario][basin_id] = slr_mmSLE_exor_raw_2015_2100_median[nid]\n",
    "    for nid, basin_id in enumerate(basin_ids_common_endo):\n",
    "        basin_reduction_dict[scenario][basin_id] = slr_mmSLE_endo_2015_2100_median[nid]\n",
    "        basin_contribution_dict[scenario][basin_id] = slr_mmSLE_endo_2015_2100_median[nid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d906c-50db-4e8d-b675-dd8e5c9bfa40",
   "metadata": {},
   "source": [
    "#### Shapefile Created in QGIS\n",
    "Raster to Polygon function of QGIS was used to convert Basin IDs into a shapefile to more easily plot global maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ae4c2-640d-4e84-a6aa-d1a41c05b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = 'MERIT_plus_15min_v1_IDs_wcrs-vector-v3_endo_merged.shp'\n",
    "gdf_all = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Add endorheic flag to the dataset\n",
    "endo_flag_dict = {}\n",
    "# Endorheic Dictionary\n",
    "for nid, basin_id in enumerate(basin_ids_common):\n",
    "    endo_flag_dict[basin_id] = endo_flag[nid]\n",
    "\n",
    "# Add onto basin shapefile\n",
    "gdf_all = gdf_all.sort_values('DN')\n",
    "gdf_all.reset_index(inplace=True, drop=True)\n",
    "\n",
    "gdf_all['endo_flag'] = gdf_all['DN'].map(endo_flag_dict)\n",
    "gdf_all.loc[np.isnan(gdf_all['endo_flag']),'endo_flag'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2873c28-aeed-4df4-b01a-1ea6638707d4",
   "metadata": {},
   "source": [
    "#### Remove small glacierized regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99fb5d-9022-4131-8927-c05589ab4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_ids_glacier_area_gt30km2 = list(basin_glacier_area_df.loc[basin_glacier_area_df['Area']>30, 'MeritID'])\n",
    "\n",
    "# Remove small glacierized regions\n",
    "gdf_gt30km2 = gdf_all[gdf_all['DN'].isin(basin_ids_glacier_area_gt30km2)].copy()\n",
    "gdf_gt30km2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969f094-99c8-4e09-bfd8-0f8a9590ef2d",
   "metadata": {},
   "source": [
    "#### Basins of interest for subplots - discharge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7354ad0-a272-41e8-b815-aa6c598944b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_ids_subplot = [226, 54, 23]\n",
    "\n",
    "basin_ids_subplot_glmelt_median = {}\n",
    "basin_ids_subplot_glmelt_95 = {}\n",
    "basin_ids_subplot_q_pg_median = {}\n",
    "basin_ids_subplot_q_pg_95 = {}\n",
    "for scenario in scenarios:\n",
    "    basin_ids_subplot_glmelt_median[scenario] = {}\n",
    "    basin_ids_subplot_glmelt_95[scenario] = {}\n",
    "    basin_ids_subplot_q_pg_median[scenario] = {}\n",
    "    basin_ids_subplot_q_pg_95[scenario] = {}\n",
    "    \n",
    "    # ----- Individual Basin Discharge Data -----\n",
    "    ds_wbm_fp = 'wbm_processed/basin_components_yearly-wibt-common/'\n",
    "    ds_wbm_fn = 'wbm_' + scenario + '_2016_2099_v4-basin_components_yearly-wibt-common.nc'\n",
    "    ds_wbm = xr.open_dataset(ds_wbm_fp + ds_wbm_fn)\n",
    "\n",
    "    basin_ids_ds_wbm = list(ds_wbm.basin_ids.values)\n",
    "    basin_ids_first_ds_wbm = [x.split('-')[0] for x in basin_ids_ds_wbm]\n",
    "    \n",
    "    for basin_id in basin_ids_subplot:\n",
    "        basin_idx_wbm = basin_ids_first_ds_wbm.index(str(basin_id))\n",
    "    \n",
    "        basin_glmelt = ds_wbm.glmelt.values[:,basin_idx_wbm,:]\n",
    "        basin_glmelt_median = np.median(basin_glmelt, axis=0)\n",
    "        basin_glmelt_95 = 1.96*np.std(basin_glmelt, axis=0)\n",
    "    \n",
    "        basin_q_pg = ds_wbm.discharge_pg.values[:,basin_idx_wbm,:]\n",
    "        basin_q_pg_median = np.median(basin_q_pg, axis=0)\n",
    "        basin_q_pg_95 = 1.96*np.std(basin_q_pg, axis=0)\n",
    "\n",
    "        basin_ids_subplot_glmelt_median[scenario][basin_id] = basin_glmelt_median\n",
    "        basin_ids_subplot_glmelt_95[scenario][basin_id] = basin_glmelt_95\n",
    "        basin_ids_subplot_q_pg_median[scenario][basin_id] = basin_q_pg_median\n",
    "        basin_ids_subplot_q_pg_95[scenario][basin_id] = basin_q_pg_95\n",
    "\n",
    "    years = ds_wbm.year.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50b075-5343-41dc-9aa4-2a55ce9400c4",
   "metadata": {},
   "source": [
    "# Univariate Map: SLE contribution (hatched) + Fraction to ocean (colored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690de79a-c0ce-4797-8d68-0d1d9323aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ds_wbm.year.values\n",
    "\n",
    "for scenario in scenarios:\n",
    "# for scenario in ['ssp245']:\n",
    "\n",
    "    # Load data\n",
    "    gdf = gdf_gt30km2.copy()\n",
    "    gdf['slr_mmSLE_reduction'] = gdf['DN'].map(basin_reduction_dict[scenario])\n",
    "    gdf['slr_mmSLE'] = gdf['DN'].map(basin_contribution_dict[scenario])\n",
    "    gdf['frac2ocean'] = gdf['DN'].map(basin_frac_dict[scenario])\n",
    "    gdf = gdf[gdf['slr_mmSLE_reduction'].notna()]\n",
    "    gdf['perc2ocean'] = 100*gdf['frac2ocean']\n",
    "\n",
    "    gdf_bi = gdf.to_crs(ccrs.PlateCarree().proj4_init)\n",
    "    gdf_bi = gdf_bi.loc[gdf_bi['slr_mmSLE'] > 0.01]\n",
    "    gdf_bi_endo = gdf_bi.loc[gdf_bi['endo_flag']==1]\n",
    "    gdf_bi_exor = gdf_bi.loc[gdf_bi['endo_flag']==0]\n",
    "\n",
    "    # ----- Time Series Scenarios ----\n",
    "    scenarios2plot = [scenario]\n",
    "\n",
    "    # ----- Thresholds -----\n",
    "    mmSLE_1 = 0.01\n",
    "    mmSLE_2 = 0.1\n",
    "    mmSLE_3 = 0.5\n",
    "    \n",
    "    # ===== FIGURE =====\n",
    "    color_water = 'gainsboro'\n",
    "    color_land = 'white'\n",
    "    lw_main = 0.7\n",
    "    fontsize = 7\n",
    "\n",
    "    continuous_cmap = cm.GnBu\n",
    "    discrete_colors = continuous_cmap(np.linspace(0.1, 0.9, 8))\n",
    "    cmap = ListedColormap(discrete_colors)\n",
    "    perc_min = 60\n",
    "    perc_max = 100\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    gs = fig.add_gridspec(nrows=100, ncols=100, hspace=0, wspace=0)\n",
    "    \n",
    "    # ----- Regions -----\n",
    "    # ax1 = plt.subplot(gs[0:41,5:65], projection=ccrs.PlateCarree()) # North America\n",
    "    # ax2 = plt.subplot(gs[40:,0:30], projection=ccrs.PlateCarree()) # South America\n",
    "    # ax3 = plt.subplot(gs[36:,26:99], projection=ccrs.PlateCarree()) # Europe + High Mountain Asia\n",
    "    # ax4 = plt.subplot(gs[38:62,88:], projection=ccrs.PlateCarree()) # New Zealand\n",
    "    \n",
    "    ax2 = plt.subplot(gs[35:,0:26], projection=ccrs.PlateCarree()) # South America\n",
    "    # ax2 = plt.subplot(gs[30:,0:26], projection=ccrs.PlateCarree()) # South America\n",
    "    ax3 = plt.subplot(gs[31:,26:], projection=ccrs.PlateCarree()) # Europe + High Mountain Asia\n",
    "    # ax4 = plt.subplot(gs[39:63,88:], projection=ccrs.PlateCarree()) # New Zealand\n",
    "    ax4 = plt.subplot(gs[81:,91:], projection=ccrs.PlateCarree()) # New Zealand\n",
    "    ax1 = plt.subplot(gs[0:40,0:55], projection=ccrs.PlateCarree()) # North America\n",
    "    # ax1 = plt.subplot(gs[0:40,0:60], projection=ccrs.PlateCarree()) # North America\n",
    "    \n",
    "    bounds_na = [-170,-95, 32, 67]\n",
    "    bounds_sa=[-82,-40,13,-57]\n",
    "    bounds_hma=[0, 120, 59.5, 10]\n",
    "    bounds_nz=[166, 179, -48, -34]\n",
    "    \n",
    "    def add_inset(ax, bounds, gdf, color_water=None, color_land=None, add_legend=False, \n",
    "                  subfig_str=None, subfig_posx=0.02, subfig_posy=0.98, subfig_textsize=8, linewidth=0.5):\n",
    "        ax.set_extent(bounds, ccrs.Geodetic())\n",
    "        ax.spines['geo'].set_edgecolor('k')\n",
    "        ax.spines['geo'].set_linewidth(0.7)\n",
    "        ax.add_feature(cfeature.NaturalEarthFeature('physical', 'ocean', '50m', facecolor=color_water))\n",
    "        ax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', facecolor=color_land))\n",
    "\n",
    "        # Hatch based on mm SLE contribution\n",
    "        gdf_bi_exor_subset1 = gdf_bi_exor[(gdf_bi_exor['slr_mmSLE'] >= mmSLE_1) & (gdf_bi_exor['slr_mmSLE'] < mmSLE_2)]\n",
    "        gdf_bi_exor_subset1.plot(ax=ax, column='perc2ocean', hatch='ooooo',\n",
    "                                 cmap=cmap, vmin=perc_min, vmax=perc_max, edgecolor='k', linewidth=linewidth, zorder=2)\n",
    "\n",
    "        gdf_bi_exor_subset2 = gdf_bi_exor[(gdf_bi_exor['slr_mmSLE'] >= mmSLE_2) & (gdf_bi_exor['slr_mmSLE'] < mmSLE_3)]\n",
    "        gdf_bi_exor_subset2.plot(ax=ax, column='perc2ocean', hatch='xxxxxxx',\n",
    "                                 cmap=cmap, vmin=perc_min, vmax=perc_max, edgecolor='k', linewidth=linewidth, zorder=2)\n",
    "\n",
    "        gdf_bi_exor_subset3 = gdf_bi_exor[gdf_bi_exor['slr_mmSLE'] >= mmSLE_3]\n",
    "        gdf_bi_exor_subset3.plot(ax=ax, column='perc2ocean',\n",
    "                                 cmap=cmap, vmin=perc_min, vmax=perc_max, edgecolor='k', linewidth=linewidth, zorder=2)\n",
    "\n",
    "        gdf_bi_endo_subset1 = gdf_bi_endo[(gdf_bi_endo['slr_mmSLE'] >= mmSLE_1) & (gdf_bi_endo['slr_mmSLE'] < mmSLE_2)]\n",
    "        gdf_bi_endo_subset1.plot(ax=ax, color='forestgreen', hatch='ooooo', edgecolor='k', linewidth=linewidth, zorder=2)\n",
    "\n",
    "        gdf_bi_endo_subset2 = gdf_bi_endo[(gdf_bi_endo['slr_mmSLE'] >= mmSLE_2) & (gdf_bi_endo['slr_mmSLE'] < mmSLE_3)]\n",
    "        gdf_bi_endo_subset2.plot(ax=ax, color='forestgreen', hatch='xxxxxxx', edgecolor='k', linewidth=linewidth, zorder=2)\n",
    "\n",
    "        gdf_bi_endo_subset3 = gdf_bi_endo[gdf_bi_endo['slr_mmSLE'] >= mmSLE_3]\n",
    "        gdf_bi_endo_subset3.plot(ax=ax, color='forestgreen', edgecolor='k', linewidth=linewidth, zorder=2)\n",
    "        # gdf_bi_endo_minsle.plot(ax=ax, hatch='////////', color='none', edgecolor='k', linewidth=0.5, zorder=4)\n",
    "        plt.rcParams['hatch.linewidth'] = 0.15\n",
    "\n",
    "        # Add subfigure label\n",
    "        ax.text(subfig_posx, subfig_posy, subfig_str, size=subfig_textsize, weight='bold', ha='left', va='top', \n",
    "                transform=ax.transAxes)\n",
    "    \n",
    "    add_inset(ax1, bounds_na, gdf, color_water=color_water, color_land=color_land, \n",
    "              subfig_str='a', subfig_posx=0.02, subfig_posy=0.98, linewidth=0.5)\n",
    "    add_inset(ax2, bounds_sa, gdf, color_water=color_water, color_land=color_land, \n",
    "              subfig_str='b', subfig_posx=0.04, subfig_posy=0.953, linewidth=0.5)\n",
    "    add_inset(ax3, bounds_hma, gdf, color_water=color_water, color_land=color_land, \n",
    "              subfig_str='c', subfig_posx=0.02, subfig_posy=0.98, linewidth=0.5)\n",
    "    add_inset(ax4, bounds_nz, gdf, color_water=color_water, color_land=color_land, \n",
    "              subfig_str='d', subfig_posx=0.04, subfig_posy=0.95, linewidth=0.5)\n",
    "    \n",
    "    # ----- Time Series -----\n",
    "    def add_timeseries(gs_bounds, basin_id, scenarios, ssp_colordict,\n",
    "                       basin_ids_subplot_glmelt_median, basin_ids_subplot_q_pg_median, \n",
    "                       lw_main=0.5, basin_name=None, pointerline_bounds=None, \n",
    "                       subfig_str=None):\n",
    "        gs_y1 = gs_bounds[0]\n",
    "        gs_y2 = gs_bounds[1]\n",
    "        gs_x1 = gs_bounds[2]\n",
    "        gs_x2 = gs_bounds[3]\n",
    "\n",
    "        # Background box\n",
    "        # ax1b_background = plt.subplot(gs[gs_y1-4:gs_y2+4, gs_x1-7:gs_x2+3])\n",
    "        ax1b_background = plt.subplot(gs[gs_y1-4:gs_y2+4, gs_x1-6:gs_x2+1])\n",
    "        ax1b_background.xaxis.set_visible(False)\n",
    "        ax1b_background.yaxis.set_visible(False)\n",
    "        ax1b_background.patch.set_color('white')\n",
    "        for spine in ax1b_background.spines.values():\n",
    "            spine.set_color('grey')\n",
    "            spine.set_linewidth(0.5)\n",
    "\n",
    "        # Time series plot\n",
    "        ax1b = plt.subplot(gs[gs_y1:gs_y2, gs_x1:gs_x2])\n",
    "        \n",
    "        # normalize by 2016-2035 mean\n",
    "        basin_glmelt_mean_2015_2035_all = []\n",
    "        for scenario_subplot in scenarios:\n",
    "            basin_glmelt_mean_2015_2035_all.append(np.mean(basin_ids_subplot_glmelt_median[scenario_subplot][basin_id][0:20]))\n",
    "        basin_glmelt_mean_2015_2035 = np.mean(basin_glmelt_mean_2015_2035_all)\n",
    "\n",
    "                \n",
    "        basin_glmelt_mean_2015_2035_str = str(int(np.round(basin_glmelt_mean_2015_2035, 0))) + ' m$^{3}$/s'\n",
    "\n",
    "        # plot normalized values\n",
    "        for scenario_subplot in scenarios:\n",
    "            basin_glmelt_median = (uniform_filter(basin_ids_subplot_glmelt_median[scenario_subplot][basin_id], size=(11)) \n",
    "                                   / basin_glmelt_mean_2015_2035)\n",
    "            basin_q_pg_median = (uniform_filter(basin_ids_subplot_q_pg_median[scenario_subplot][basin_id], size=(11)) \n",
    "                                 / basin_glmelt_mean_2015_2035)\n",
    "\n",
    "            if len(scenarios) == 1:\n",
    "                color_ts = 'k'\n",
    "            else:\n",
    "                color_ts = ssp_colordict[scenario_subplot]\n",
    "    \n",
    "            ax1b.plot(years, 100*basin_glmelt_median, color=color_ts, \n",
    "                      linestyle='--', linewidth=lw_main, zorder=2, label=None)\n",
    "            ax1b.plot(years, 100*basin_q_pg_median, color=color_ts, \n",
    "                      linestyle='-', linewidth=lw_main, zorder=3, label=None)\n",
    "\n",
    "        # Labels\n",
    "        ax1b.set_ylabel('Runoff (%)', size=fontsize, labelpad=1)\n",
    "        ax1b.set_ylim(0,130)\n",
    "        ax1b.tick_params(axis='y', labelsize=fontsize-1)\n",
    "        ax1b.tick_params(axis='x', labelsize=fontsize-1)\n",
    "        ax1b.yaxis.set_major_locator(MultipleLocator(50))\n",
    "        ax1b.yaxis.set_minor_locator(MultipleLocator(10))\n",
    "        ax1b.set_xlim(2015,2100)\n",
    "        ax1b.xaxis.set_major_locator(MultipleLocator(20))\n",
    "        ax1b.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "        ax1b.set_xticks([2040, 2080], ['2040', '2080'])\n",
    "        # ax1b.xaxis.set_major_locator(MultipleLocator(20))\n",
    "        # ax1b.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "        ax1b.tick_params(axis='both', which='major', direction='in', right=True, pad=1, width=0.3, length=3)\n",
    "        ax1b.tick_params(axis='both', which='minor', direction='in', right=True, width=0.15, length=2)\n",
    "\n",
    "        # Reduce the linewidth of the spines\n",
    "        for spine in ax1b.spines.values():\n",
    "            spine.set_linewidth(0.5)  # Set the desired linewidth (default is typically 1.5)\n",
    "\n",
    "        # Add Basin Name\n",
    "        if basin_name is None:\n",
    "            basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "        ax1b.text(0.5, 1.01, basin_name, size=fontsize, horizontalalignment='center', va='bottom', transform=ax1b.transAxes)\n",
    "\n",
    "        # Add Basin melt\n",
    "        ax1b.text(0.05, 0.085, basin_glmelt_mean_2015_2035_str, size=fontsize-1, horizontalalignment='left', va='bottom', \n",
    "                  transform=ax1b.transAxes)\n",
    "\n",
    "        # Add Basin Frac2Ocean\n",
    "        basin_frac2ocean_str = str(int(np.round(100*basin_frac_dict[scenario][basin_id],0))) + '%'\n",
    "        if scenario in ['ssp370', 'ssp585'] and basin_id in [226]:\n",
    "            text_y = 0.70\n",
    "        else:\n",
    "            text_y = 0.97\n",
    "        ax1b.text(0.95, text_y, basin_frac2ocean_str, size=fontsize-1, horizontalalignment='right', va='top', \n",
    "                  transform=ax1b.transAxes)\n",
    "        # Add pointer to basin\n",
    "        if not pointerline_bounds is None:\n",
    "                ax1b.plot([pointerline_bounds[0], pointerline_bounds[1]], [pointerline_bounds[2], pointerline_bounds[3]], \n",
    "                          color='grey', linewidth=0.75, clip_on=False)\n",
    "\n",
    "        # Add subfigure label\n",
    "        ax1b_background.text(0.98, 0.98, subfig_str, size=fontsize, weight='bold', ha='right', va='top', \n",
    "                             transform=ax1b_background.transAxes)\n",
    "\n",
    "    # COPPER RIVER\n",
    "    size_x = 17\n",
    "    size_y = 14\n",
    "    \n",
    "    basin_id = 226\n",
    "    gs_y1 = 20\n",
    "    gs_x1 = 7\n",
    "    gs_y2 = gs_y1 + size_y\n",
    "    gs_x2 = gs_x1 + size_x\n",
    "    gs_bounds = [gs_y1, gs_y2, gs_x1, gs_x2]\n",
    "    pointerline_bounds = [2046, 2070, 193, 260]\n",
    "    add_timeseries(gs_bounds, basin_id, scenarios2plot, ssp_colordict,\n",
    "                   basin_ids_subplot_glmelt_median, basin_ids_subplot_q_pg_median,\n",
    "                   lw_main=lw_main, pointerline_bounds=pointerline_bounds, subfig_str='e')\n",
    "\n",
    "    # COLORADO RIVER, ARGENTINA\n",
    "    basin_id = 54\n",
    "    basin_name = 'Colorado'\n",
    "    gs_y1 = 77\n",
    "    gs_x1 = 21\n",
    "    gs_y2 = gs_y1 + size_y\n",
    "    gs_x2 = gs_x1 + size_x\n",
    "    gs_bounds = [gs_y1, gs_y2, gs_x1, gs_x2]\n",
    "    pointerline_bounds = [1953, 1985, 150, 80]\n",
    "    add_timeseries(gs_bounds, basin_id, scenarios2plot, ssp_colordict,\n",
    "                   basin_ids_subplot_glmelt_median, basin_ids_subplot_q_pg_median,\n",
    "                   lw_main=lw_main, basin_name=basin_name, pointerline_bounds=pointerline_bounds, subfig_str='f')\n",
    "\n",
    "    # INDUS RIVER\n",
    "    basin_id = 23\n",
    "    gs_y1 = 80\n",
    "    gs_x1 = 47\n",
    "    gs_y2 = gs_y1 + size_y\n",
    "    gs_x2 = gs_x1 + size_x\n",
    "    gs_bounds = [gs_y1, gs_y2, gs_x1, gs_x2]\n",
    "    pointerline_bounds = [2105, 2123, 150, 200]\n",
    "    add_timeseries(gs_bounds, basin_id, scenarios2plot, ssp_colordict,\n",
    "                   basin_ids_subplot_glmelt_median, basin_ids_subplot_q_pg_median,\n",
    "                   lw_main=lw_main, pointerline_bounds=pointerline_bounds, subfig_str='g')\n",
    "\n",
    "    # ----- LEGENDS -----\n",
    "    fontsize_leg = 8\n",
    "    ax_leg1 = plt.subplot(gs[14:34,58:78])\n",
    "    ax_leg1.axis('off')\n",
    "    rect1 = patches.Rectangle((0.1, 0), 0.15, 0.15, facecolor='lightgray', edgecolor='k', clip_on=False)\n",
    "    rect2 = patches.Rectangle((0.1, 0.2), 0.15, 0.15, facecolor='lightgray', hatch='xxxxxxx', edgecolor='k', clip_on=False)\n",
    "    rect3 = patches.Rectangle((0.1, 0.4), 0.15, 0.15, facecolor='lightgray', hatch='ooooo', edgecolor='k', clip_on=False)\n",
    "    ax_leg1.add_patch(rect1)\n",
    "    ax_leg1.add_patch(rect2)\n",
    "    ax_leg1.add_patch(rect3)\n",
    "    \n",
    "    ax_leg1.text(0.4, 0.6, 'mm SLE', size=fontsize_leg, ha='center', va='bottom', transform=ax_leg1.transAxes)\n",
    "    ax_leg1.text(0.3, 0.075, '> 0.5', size=fontsize_leg, ha='left', va='center', transform=ax_leg1.transAxes)\n",
    "    ax_leg1.text(0.3, 0.275, '0.1 - 0.5', size=fontsize_leg, ha='left', va='center', transform=ax_leg1.transAxes)\n",
    "    ax_leg1.text(0.3, 0.475, '< 0.1', size=fontsize_leg, ha='left', va='center', transform=ax_leg1.transAxes)\n",
    "    \n",
    "    # TIMESERIES: LINE STYLE\n",
    "    lw_leg = 1.5\n",
    "    axleg = plt.subplot(gs[20:34, 80:])\n",
    "    axleg.plot([0,0],[0,0], color='k', linestyle='--', linewidth=lw_leg, label='at Terminus')\n",
    "    axleg.plot([0,0],[0,0], color='k', linestyle='-', linewidth=lw_leg, label='at Ocean')\n",
    "    axleg.legend(loc=(0.1, 0.1), fontsize=fontsize_leg-0.5, ncol=1, columnspacing=0.8, labelspacing=0.25, \n",
    "                 handlelength=1.8, handletextpad=0.4, borderpad=0, frameon=False)\n",
    "    axleg.set_axis_off()\n",
    "    axleg.text(0.5, 0.75, 'Time Series', size=fontsize_leg, ha='center', va='center', transform=axleg.transAxes)\n",
    "    rect_legbox = patches.Rectangle((0, 0), 20, 20, facecolor='none', edgecolor='grey', linewidth=0.75, clip_on=False)\n",
    "    axleg.add_patch(rect_legbox)\n",
    "\n",
    "    # ADD VALUES OF AMOUNT REDUCED OVER A GIVEN LEVEL\n",
    "    basins_onplot = list(gdf_bi.loc[gdf_bi['slr_mmSLE_reduction'] > 0.05, 'DN'].values)\n",
    "    basin_id_name = {3:'Caspian',\n",
    "                     14:'Ganges',\n",
    "                     23:'Indus',\n",
    "                     33:'W Kunlun Shan\\n- C Tien Shan', # 'W Kunlun Shan - Tien Shan'\n",
    "                     44:'Aral',\n",
    "                     47:'Syr Darya',\n",
    "                     132:'E Kunlun Shan\\n& Qilian Shan',\n",
    "                     226:'Copper',\n",
    "                     228:'E Tien Shan',\n",
    "                     317:'Tibetan\\nInterior\\nMountains',\n",
    "                    }\n",
    "    basin_text_xy = {3:[52, 55],\n",
    "                     14:[85.0,27.2],\n",
    "                     23:[73.7, 33.0],\n",
    "                     33:[105,55.5],\n",
    "                     44:[78,67],\n",
    "                     47:[60,70],\n",
    "                     132:[110, 46],\n",
    "                     226:[-144,57.5],\n",
    "                     228:[94,64],\n",
    "                     317:[90.5,17],\n",
    "                    }\n",
    "    \n",
    "    for basin_id in basins_onplot:\n",
    "        # basin_reduction_str = str(np.round(basin_reduction_dict[scenario][basin_id],2))\n",
    "        basin_reduction_str = \"{:.2f}\".format(basin_reduction_dict[scenario][basin_id])\n",
    "        basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "\n",
    "        linespacing=0.9\n",
    "        name_size = 6\n",
    "\n",
    "        if basin_id in [3, 14, 23]:\n",
    "            color_txt = 'white'\n",
    "        else:\n",
    "            color_txt = 'k'\n",
    "\n",
    "        if basin_id in [3, 14, 23, 33, 44, 47, 132, 228, 317]:\n",
    "            ax3.text(basin_text_xy[basin_id][0], basin_text_xy[basin_id][1], basin_id_name[basin_id] + '\\n(' + basin_reduction_str + ')', \n",
    "                     size=name_size, linespacing=linespacing, ha='center', va='center', color=color_txt, zorder=6)\n",
    "            if basin_id in [33]:\n",
    "                ax3.plot([88,98], [40.4,52.5], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [44]:\n",
    "                ax3.plot([76.5,77.5], [47,64.5], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [47]:\n",
    "                ax3.plot([66.3,60], [45,67.5], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [132]:\n",
    "                ax3.plot([98,104], [39.5,44], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [228]:\n",
    "                ax3.plot([82.8,90], [45.8,61], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [317]:\n",
    "                ax3.plot([90,92.5], [32,21], linewidth=0.5, color=color_txt, zorder=5)\n",
    "\n",
    "        elif basin_id in [226]:\n",
    "            ax1.text(basin_text_xy[basin_id][0], basin_text_xy[basin_id][1], basin_id_name[basin_id] + '\\n(' + basin_reduction_str + ')', \n",
    "                     size=name_size, linespacing=linespacing, ha='center', va='center')            \n",
    "\n",
    "\n",
    "    # Heat map colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=perc_min, vmax=perc_max))\n",
    "    sm._A = []\n",
    "    cax = plt.axes([0.62, 0.79, 0.22, 0.015])\n",
    "    cbar = plt.colorbar(sm, ax=ax, cax=cax, orientation='horizontal')\n",
    "    cax.xaxis.set_ticks_position('bottom')\n",
    "    cax.xaxis.set_tick_params(pad=2)\n",
    "    cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "    rect_hatch = patches.Rectangle((55, 0), 5, 0.98, facecolor='forestgreen', edgecolor='k', clip_on=False)\n",
    "    cax.add_patch(rect_hatch)\n",
    "    rect_marker = patches.Rectangle((55, -0.7), 0.03, 0.7, facecolor='k', edgecolor='k', lw=0.8, clip_on=False)\n",
    "    cax.add_patch(rect_marker)\n",
    "    cax.text(-0.15, -1.82, '0', size=fontsize_leg, ha='left', va='center', transform=cax.transAxes)\n",
    "    cax.text(0.5, 1.8, 'Glacier Runoff Reaching Ocean (%)', size=9, ha='center', va='bottom', transform=cax.transAxes)\n",
    "    \n",
    "    # ----- Saving -----\n",
    "    # fig.set_size_inches(6,4)\n",
    "    fig_fp = results_fp + '_figures/'\n",
    "    if not os.path.exists(fig_fp):\n",
    "        os.makedirs(fig_fp)\n",
    "    fig.savefig(fig_fp + 'Fig3_basins_slr_and_frac_' + scenario + '.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e81b44-3433-4ac0-90a9-2f36a80476ac",
   "metadata": {},
   "source": [
    "# Bivariate Map: SLE contribution + Fraction to Ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f844aa2-2d8e-4aa9-8a05-10051cbaf309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scenario in scenarios:\n",
    "for scenario in ['ssp245']:\n",
    "\n",
    "    # Load data\n",
    "    gdf = gdf_gt30km2.copy()\n",
    "    gdf['slr_mmSLE_reduction'] = gdf['DN'].map(basin_reduction_dict[scenario])\n",
    "    gdf['slr_mmSLE'] = gdf['DN'].map(basin_contribution_dict[scenario])\n",
    "    gdf['frac2ocean'] = gdf['DN'].map(basin_frac_dict[scenario])\n",
    "    gdf = gdf[gdf['slr_mmSLE_reduction'].notna()]\n",
    "    gdf['perc2ocean'] = 100*gdf['frac2ocean']\n",
    "\n",
    "    gdf_bi = gdf.to_crs(ccrs.PlateCarree().proj4_init)\n",
    "\n",
    "    # ----- Time Series Scenarios ----\n",
    "    scenarios2plot = [scenario]\n",
    "    # scenarios2plot = scenarios\n",
    "\n",
    "    # ----- Bivariate Thresholds -----\n",
    "    mmSLE_1 = 0.01\n",
    "    mmSLE_2 = 0.1\n",
    "    mmSLE_3 = 0.5\n",
    "\n",
    "    frac_1 = 0.0\n",
    "    frac_2 = 0.5\n",
    "    frac_3 = 0.95\n",
    "\n",
    "    # Colors selected from https://jakubnowosad.com/posts/2020-08-25-cbc-bp2/\n",
    "    # color_lowlow = '#E8E8E8'\n",
    "    # color_lowmid = '#CBB8D7'\n",
    "    # color_lowhigh = '#9973AF'\n",
    "    # color_midlow = '#E4D9AC'\n",
    "    # color_midmid = '#C8ADA0'\n",
    "    # color_midhigh = '#976B83'\n",
    "    # color_highlow = '#C8B35A'\n",
    "    # color_highmid = '#B08E53'\n",
    "    # color_highhigh = '#804D36'\n",
    "\n",
    "    # # alternative 1\n",
    "    # color_lowlow = '#F3F3F3'\n",
    "    # color_midlow = '#F3E6B3'\n",
    "    # color_highlow = '#F3B301'\n",
    "    # color_lowmid = '#B4D3E1'\n",
    "    # color_midmid = '#B3B3B3'\n",
    "    # color_highmid = '#B36600'\n",
    "    # color_lowhigh = '#509DC2'\n",
    "    # color_midhigh = '#376387'\n",
    "    # color_highhigh = '#2F282B'\n",
    "\n",
    "    # alternative 1 - custom\n",
    "    color_lowlow = '#FBFAD3' #'lightgoldenrodyellow'\n",
    "    color_midlow = '#EED67F' #'#F8DE7F'\n",
    "    color_highlow = '#DBA101' #'#F3B301'\n",
    "    color_lowmid = '#FFC8D5' #'#FAB6C2'\n",
    "    color_midmid = '#D68E88' #'#E69674'\n",
    "    color_highmid = '#9F5132' #'#C76600'\n",
    "    color_lowhigh = '#58ADD5' #'#509DC2'\n",
    "    color_midhigh = '#4B7B95' #'#376387'\n",
    "    color_highhigh = '#383034' #'#2F282B'\n",
    "\n",
    "    # # alternative 2\n",
    "    # color_lowlow = '#F7CE8B'\n",
    "    # color_midlow = '#E16439'\n",
    "    # color_highlow = '#B40000'\n",
    "    # color_lowmid = '#E6E6E6'\n",
    "    # color_midmid = '#BFBFBF'\n",
    "    # color_highmid = '#7F7F7F'\n",
    "    # color_lowhigh = '#C0B3D6'\n",
    "    # color_midhigh = '#7868A7'\n",
    "    # color_highhigh = '#210E5A'\n",
    "\n",
    "    # Lowest mmSLE + Lowest red\n",
    "    gdf_bi_subset1 = gdf_bi[(gdf_bi['slr_mmSLE'] >= mmSLE_1) & (gdf_bi['slr_mmSLE'] < mmSLE_2) & \n",
    "                            (gdf_bi['frac2ocean'] >= frac_1) & (gdf_bi['frac2ocean'] < frac_2)]\n",
    "    color_subset1 = color_lowlow\n",
    "    \n",
    "    # Lowest mmSLE + Middle red\n",
    "    gdf_bi_subset2 = gdf_bi[(gdf_bi['slr_mmSLE'] >= mmSLE_1) & (gdf_bi['slr_mmSLE'] < mmSLE_2) & \n",
    "                            (gdf_bi['frac2ocean'] >= frac_2) & (gdf_bi['frac2ocean'] < frac_3)]\n",
    "    color_subset2 = color_lowmid\n",
    "\n",
    "    # Lowest mmSLE + Upper red\n",
    "    gdf_bi_subset3 = gdf_bi[(gdf_bi['slr_mmSLE'] >= mmSLE_1) & (gdf_bi['slr_mmSLE'] < mmSLE_2) & \n",
    "                            (gdf_bi['frac2ocean'] >= frac_3)]\n",
    "    color_subset3 = color_lowhigh\n",
    "\n",
    "    # Middle mmSLE + Lowest red\n",
    "    gdf_bi_subset4 = gdf_bi[(gdf_bi['slr_mmSLE'] >= mmSLE_2) & (gdf_bi['slr_mmSLE'] < mmSLE_3) & \n",
    "                            (gdf_bi['frac2ocean'] >= frac_1) & (gdf_bi['frac2ocean'] < frac_2)]\n",
    "    color_subset4 = color_midlow\n",
    "\n",
    "    # Middle mmSLE + Middle red\n",
    "    gdf_bi_subset5 = gdf_bi[(gdf_bi['slr_mmSLE'] >= mmSLE_2) & (gdf_bi['slr_mmSLE'] < mmSLE_3) & \n",
    "                            (gdf_bi['frac2ocean'] >= frac_2) & (gdf_bi['frac2ocean'] < frac_3)]\n",
    "    color_subset5 = color_midmid\n",
    "\n",
    "    # Middle mmSLE + Upper red\n",
    "    gdf_bi_subset6 = gdf_bi[(gdf_bi['slr_mmSLE'] >= mmSLE_2) & (gdf_bi['slr_mmSLE'] < mmSLE_3) & \n",
    "                            (gdf_bi['frac2ocean'] >= frac_3)]\n",
    "    color_subset6 = color_midhigh\n",
    "\n",
    "    # Upper mmSLE + Lowest red\n",
    "    gdf_bi_subset7 = gdf_bi[(gdf_bi['slr_mmSLE'] >= mmSLE_3) & \n",
    "                            (gdf_bi['frac2ocean'] >= frac_1) & (gdf_bi['frac2ocean'] < frac_2)]\n",
    "    color_subset7 = color_highlow\n",
    "\n",
    "    # Upper mmSLE + Middle red\n",
    "    gdf_bi_subset8 = gdf_bi[(gdf_bi['slr_mmSLE'] >= mmSLE_3) & \n",
    "                            (gdf_bi['frac2ocean'] >= frac_2) & (gdf_bi['frac2ocean'] < frac_3)]\n",
    "    color_subset8 = color_highmid\n",
    "\n",
    "    # Upper mmSLE + Middle red\n",
    "    gdf_bi_subset9 = gdf_bi[(gdf_bi['slr_mmSLE'] >= mmSLE_3) & \n",
    "                            (gdf_bi['frac2ocean'] >= frac_3)]\n",
    "    color_subset9 = color_highhigh\n",
    "    \n",
    "    # ===== FIGURE =====\n",
    "    color_water = 'gainsboro'\n",
    "    color_land = 'white'\n",
    "    lw_main = 0.7\n",
    "    fontsize = 7\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    gs = fig.add_gridspec(nrows=100, ncols=100, hspace=0, wspace=0)\n",
    "    \n",
    "    # ----- Regions -----\n",
    "    # ax1 = plt.subplot(gs[0:41,5:65], projection=ccrs.PlateCarree()) # North America\n",
    "    # ax2 = plt.subplot(gs[40:,0:30], projection=ccrs.PlateCarree()) # South America\n",
    "    # ax3 = plt.subplot(gs[36:,26:99], projection=ccrs.PlateCarree()) # Europe + High Mountain Asia\n",
    "    # ax4 = plt.subplot(gs[38:62,88:], projection=ccrs.PlateCarree()) # New Zealand\n",
    "    \n",
    "    ax2 = plt.subplot(gs[35:,0:26], projection=ccrs.PlateCarree()) # South America\n",
    "    # ax2 = plt.subplot(gs[30:,0:26], projection=ccrs.PlateCarree()) # South America\n",
    "    ax3 = plt.subplot(gs[31:,26:], projection=ccrs.PlateCarree()) # Europe + High Mountain Asia\n",
    "    # ax4 = plt.subplot(gs[39:63,88:], projection=ccrs.PlateCarree()) # New Zealand\n",
    "    ax4 = plt.subplot(gs[81:,91:], projection=ccrs.PlateCarree()) # New Zealand\n",
    "    ax1 = plt.subplot(gs[0:40,0:55], projection=ccrs.PlateCarree()) # North America\n",
    "    # ax1 = plt.subplot(gs[0:40,0:60], projection=ccrs.PlateCarree()) # North America\n",
    "    \n",
    "    bounds_na = [-170,-95, 32, 67]\n",
    "    bounds_sa=[-82,-40,13,-57]\n",
    "    bounds_hma=[0, 120, 59.5, 10]\n",
    "    bounds_nz=[166, 179, -48, -34]\n",
    "    \n",
    "    def add_inset(ax, bounds, gdf, color_water=None, color_land=None, add_legend=False, \n",
    "                  subfig_str=None, subfig_posx=0.02, subfig_posy=0.98, subfig_textsize=8):\n",
    "        ax.set_extent(bounds, ccrs.Geodetic())\n",
    "        ax.spines['geo'].set_edgecolor('k')\n",
    "        ax.spines['geo'].set_linewidth(0.7)\n",
    "        ax.add_feature(cfeature.NaturalEarthFeature('physical', 'ocean', '50m', facecolor=color_water))\n",
    "        ax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', facecolor=color_land))\n",
    "        \n",
    "        # gdf_bi.plot(ax=ax, color='white', edgecolor='k', linewidth=0.5, zorder=2)\n",
    "        # gdf_bi.plot(ax=ax, hatch='////////', color='white', edgecolor='k', linewidth=0.5, zorder=2)\n",
    "        # plt.rcParams['hatch.linewidth'] = 0.25\n",
    "\n",
    "        # Bivariate subsets\n",
    "        gdf_bi_subset1.plot(ax=ax, color=color_subset1, edgecolor='k', linewidth=0.5, zorder=3)\n",
    "        gdf_bi_subset2.plot(ax=ax, color=color_subset2, edgecolor='k', linewidth=0.5, zorder=3)\n",
    "        gdf_bi_subset3.plot(ax=ax, color=color_subset3, edgecolor='k', linewidth=0.5, zorder=3)\n",
    "        gdf_bi_subset4.plot(ax=ax, color=color_subset4, edgecolor='k', linewidth=0.5, zorder=3)\n",
    "        gdf_bi_subset5.plot(ax=ax, color=color_subset5, edgecolor='k', linewidth=0.5, zorder=3)\n",
    "        gdf_bi_subset6.plot(ax=ax, color=color_subset6, edgecolor='k', linewidth=0.5, zorder=3)\n",
    "        gdf_bi_subset7.plot(ax=ax, color=color_subset7, edgecolor='k', linewidth=0.5, zorder=3)\n",
    "        gdf_bi_subset8.plot(ax=ax, color=color_subset8, edgecolor='k', linewidth=0.5, zorder=3)\n",
    "        gdf_bi_subset9.plot(ax=ax, color=color_subset9, edgecolor='k', linewidth=0.5, zorder=3)\n",
    "\n",
    "        gdf_bi_endo = gdf_bi.loc[gdf_bi['endo_flag']==1]\n",
    "        gdf_bi_endo_minsle = gdf_bi_endo.loc[gdf_bi_endo['slr_mmSLE'] >= mmSLE_1]\n",
    "        gdf_bi_endo_minsle.plot(ax=ax, hatch='////////', color='none', edgecolor='k', linewidth=0.5, zorder=4)\n",
    "        plt.rcParams['hatch.linewidth'] = 0.15\n",
    "\n",
    "        # Add subfigure label\n",
    "        ax.text(subfig_posx, subfig_posy, subfig_str, size=subfig_textsize, weight='bold', ha='left', va='top', \n",
    "                transform=ax.transAxes)\n",
    "    \n",
    "    add_inset(ax1, bounds_na, gdf, color_water=color_water, color_land=color_land, \n",
    "              subfig_str='a', subfig_posx=0.02, subfig_posy=0.98)\n",
    "    add_inset(ax2, bounds_sa, gdf, color_water=color_water, color_land=color_land, \n",
    "              subfig_str='b', subfig_posx=0.04, subfig_posy=0.953)\n",
    "    add_inset(ax3, bounds_hma, gdf, color_water=color_water, color_land=color_land, \n",
    "              subfig_str='c', subfig_posx=0.02, subfig_posy=0.98)\n",
    "    add_inset(ax4, bounds_nz, gdf, color_water=color_water, color_land=color_land, \n",
    "              subfig_str='d', subfig_posx=0.04, subfig_posy=0.95)\n",
    "    \n",
    "    # ----- Time Series -----\n",
    "    def add_timeseries(gs_bounds, basin_id, scenarios, ssp_colordict,\n",
    "                       basin_ids_subplot_glmelt_median, basin_ids_subplot_q_pg_median, \n",
    "                       lw_main=0.5, basin_name=None, pointerline_bounds=None, \n",
    "                       subfig_str=None):\n",
    "        gs_y1 = gs_bounds[0]\n",
    "        gs_y2 = gs_bounds[1]\n",
    "        gs_x1 = gs_bounds[2]\n",
    "        gs_x2 = gs_bounds[3]\n",
    "\n",
    "        # Background box\n",
    "        # ax1b_background = plt.subplot(gs[gs_y1-4:gs_y2+4, gs_x1-7:gs_x2+3])\n",
    "        ax1b_background = plt.subplot(gs[gs_y1-4:gs_y2+4, gs_x1-6:gs_x2+1])\n",
    "        ax1b_background.xaxis.set_visible(False)\n",
    "        ax1b_background.yaxis.set_visible(False)\n",
    "        ax1b_background.patch.set_color('white')\n",
    "        for spine in ax1b_background.spines.values():\n",
    "            spine.set_color('grey')\n",
    "            spine.set_linewidth(0.5)\n",
    "\n",
    "        # Time series plot\n",
    "        ax1b = plt.subplot(gs[gs_y1:gs_y2, gs_x1:gs_x2])\n",
    "        \n",
    "        # normalize by 2016-2035 mean\n",
    "        basin_glmelt_mean_2015_2035_all = []\n",
    "        for scenario_subplot in scenarios:\n",
    "            basin_glmelt_mean_2015_2035_all.append(np.mean(basin_ids_subplot_glmelt_median[scenario_subplot][basin_id][0:20]))\n",
    "        basin_glmelt_mean_2015_2035 = np.mean(basin_glmelt_mean_2015_2035_all)\n",
    "\n",
    "                \n",
    "        basin_glmelt_mean_2015_2035_str = str(int(np.round(basin_glmelt_mean_2015_2035, 0))) + ' m$^{3}$/s'\n",
    "\n",
    "        # plot normalized values\n",
    "        for scenario_subplot in scenarios:\n",
    "            basin_glmelt_median = (uniform_filter(basin_ids_subplot_glmelt_median[scenario_subplot][basin_id], size=(11)) \n",
    "                                   / basin_glmelt_mean_2015_2035)\n",
    "            basin_q_pg_median = (uniform_filter(basin_ids_subplot_q_pg_median[scenario_subplot][basin_id], size=(11)) \n",
    "                                 / basin_glmelt_mean_2015_2035)\n",
    "\n",
    "            if len(scenarios) == 1:\n",
    "                color_ts = 'k'\n",
    "            else:\n",
    "                color_ts = ssp_colordict[scenario_subplot]\n",
    "    \n",
    "            ax1b.plot(years, basin_glmelt_median, color=color_ts, \n",
    "                      linestyle='--', linewidth=lw_main, zorder=2, label=None)\n",
    "            ax1b.plot(years, basin_q_pg_median, color=color_ts, \n",
    "                      linestyle='-', linewidth=lw_main, zorder=3, label=None)\n",
    "\n",
    "        # Labels\n",
    "        ax1b.set_ylabel('Discharge (-)', size=fontsize, labelpad=1)\n",
    "        ax1b.set_ylim(0,1.5)\n",
    "        ax1b.tick_params(axis='y', labelsize=fontsize)\n",
    "        ax1b.tick_params(axis='x', labelsize=fontsize)\n",
    "        ax1b.yaxis.set_major_locator(MultipleLocator(0.5))\n",
    "        ax1b.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "        ax1b.set_xlim(2015,2100)\n",
    "        ax1b.xaxis.set_major_locator(MultipleLocator(20))\n",
    "        ax1b.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "        ax1b.set_xticks([2040, 2080], ['2040', '2080'])\n",
    "        # ax1b.xaxis.set_major_locator(MultipleLocator(20))\n",
    "        # ax1b.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "        ax1b.tick_params(axis='both', which='major', direction='in', right=True, pad=1, width=0.3, length=3)\n",
    "        ax1b.tick_params(axis='both', which='minor', direction='in', right=True, width=0.15, length=2)\n",
    "\n",
    "        # Reduce the linewidth of the spines\n",
    "        for spine in ax1b.spines.values():\n",
    "            spine.set_linewidth(0.5)  # Set the desired linewidth (default is typically 1.5)\n",
    "\n",
    "        # Add Basin Name\n",
    "        if basin_name is None:\n",
    "            basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "        ax1b.text(0.5, 1.01, basin_name, size=fontsize, horizontalalignment='center', va='bottom', transform=ax1b.transAxes)\n",
    "\n",
    "        # Add Basin melt\n",
    "        ax1b.text(0.05, 0.085, basin_glmelt_mean_2015_2035_str, size=fontsize, horizontalalignment='left', va='bottom', \n",
    "                  transform=ax1b.transAxes)\n",
    "\n",
    "        # Add Basin Frac2Ocean\n",
    "        basin_frac2ocean_str = str(int(np.round(100*basin_frac_dict[scenario][basin_id],0))) + '%'\n",
    "        ax1b.text(0.95, 0.97, basin_frac2ocean_str, size=fontsize, horizontalalignment='right', va='top', \n",
    "                  transform=ax1b.transAxes)\n",
    "        # Add pointer to basin\n",
    "        if not pointerline_bounds is None:\n",
    "                ax1b.plot([pointerline_bounds[0], pointerline_bounds[1]], [pointerline_bounds[2], pointerline_bounds[3]], \n",
    "                          color='grey', linewidth=0.75, clip_on=False)\n",
    "\n",
    "        # Add subfigure label\n",
    "        ax1b_background.text(0.98, 0.98, subfig_str, size=fontsize, weight='bold', ha='right', va='top', \n",
    "                             transform=ax1b_background.transAxes)\n",
    "\n",
    "    # COPPER RIVER\n",
    "    size_x = 17\n",
    "    size_y = 14\n",
    "    \n",
    "    basin_id = 226\n",
    "    gs_y1 = 20\n",
    "    gs_x1 = 7\n",
    "    gs_y2 = gs_y1 + size_y\n",
    "    gs_x2 = gs_x1 + size_x\n",
    "    gs_bounds = [gs_y1, gs_y2, gs_x1, gs_x2]\n",
    "    pointerline_bounds = [2046, 2070, 1.93, 2.6]\n",
    "    add_timeseries(gs_bounds, basin_id, scenarios2plot, ssp_colordict,\n",
    "                   basin_ids_subplot_glmelt_median, basin_ids_subplot_q_pg_median,\n",
    "                   lw_main=lw_main, pointerline_bounds=pointerline_bounds, subfig_str='e')\n",
    "\n",
    "    # COLORADO RIVER, ARGENTINA\n",
    "    basin_id = 54\n",
    "    basin_name = 'Colorado'\n",
    "    gs_y1 = 77\n",
    "    gs_x1 = 21\n",
    "    gs_y2 = gs_y1 + size_y\n",
    "    gs_x2 = gs_x1 + size_x\n",
    "    gs_bounds = [gs_y1, gs_y2, gs_x1, gs_x2]\n",
    "    pointerline_bounds = [1953, 1985, 1.5, 0.8]\n",
    "    add_timeseries(gs_bounds, basin_id, scenarios2plot, ssp_colordict,\n",
    "                   basin_ids_subplot_glmelt_median, basin_ids_subplot_q_pg_median,\n",
    "                   lw_main=lw_main, basin_name=basin_name, pointerline_bounds=pointerline_bounds, subfig_str='f')\n",
    "\n",
    "    # INDUS RIVER\n",
    "    basin_id = 23\n",
    "    gs_y1 = 80\n",
    "    gs_x1 = 47\n",
    "    gs_y2 = gs_y1 + size_y\n",
    "    gs_x2 = gs_x1 + size_x\n",
    "    gs_bounds = [gs_y1, gs_y2, gs_x1, gs_x2]\n",
    "    pointerline_bounds = [2105, 2123, 1.5, 2.0]\n",
    "    add_timeseries(gs_bounds, basin_id, scenarios2plot, ssp_colordict,\n",
    "                   basin_ids_subplot_glmelt_median, basin_ids_subplot_q_pg_median,\n",
    "                   lw_main=lw_main, pointerline_bounds=pointerline_bounds, subfig_str='g')\n",
    "\n",
    "    # ----- LEGENDS -----\n",
    "    fontsize_leg = 8\n",
    "    # ax_leg1 = plt.subplot(gs[3:27,67:82])\n",
    "    ax_leg1 = plt.subplot(gs[8:30,66:80])\n",
    "    rect1 = patches.Rectangle((0, 0), 0.33, 0.33, facecolor=color_lowlow, edgecolor='k', clip_on=False)\n",
    "    rect2 = patches.Rectangle((0.33, 0), 0.33, 0.33, facecolor=color_midlow, edgecolor='k', clip_on=False)\n",
    "    rect3 = patches.Rectangle((0.66, 0), 0.33, 0.33, facecolor=color_highlow, edgecolor='k', clip_on=False)\n",
    "    rect4 = patches.Rectangle((0, 0.33), 0.33, 0.33, facecolor=color_lowmid, edgecolor='k', clip_on=False)\n",
    "    rect5 = patches.Rectangle((0.33, 0.33), 0.33, 0.33, facecolor=color_midmid, edgecolor='k', clip_on=False)\n",
    "    rect6 = patches.Rectangle((0.66, 0.33), 0.33, 0.33, facecolor=color_highmid, edgecolor='k', clip_on=False)\n",
    "    rect7 = patches.Rectangle((0, 0.66), 0.33, 0.33, facecolor=color_lowhigh, edgecolor='k', clip_on=False)\n",
    "    rect8 = patches.Rectangle((0.33, 0.66), 0.33, 0.33, facecolor=color_midhigh, edgecolor='k', clip_on=False)\n",
    "    rect9 = patches.Rectangle((0.66, 0.66), 0.33, 0.33, facecolor=color_highhigh, edgecolor='k', clip_on=False)\n",
    "    ax_leg1.add_patch(rect1)\n",
    "    ax_leg1.add_patch(rect2)\n",
    "    ax_leg1.add_patch(rect3)\n",
    "    ax_leg1.add_patch(rect4)\n",
    "    ax_leg1.add_patch(rect5)\n",
    "    ax_leg1.add_patch(rect6)\n",
    "    ax_leg1.add_patch(rect7)\n",
    "    ax_leg1.add_patch(rect8)\n",
    "    ax_leg1.add_patch(rect9)\n",
    "    ax_leg1.set_ylabel(' Discharge to Ocean (%)', size=fontsize_leg, labelpad=2)\n",
    "    ax_leg1.set_xlabel(' Mass Loss (mm SLE)', size=fontsize_leg, labelpad=3)\n",
    "    ax_leg1.set_xlim(0,0.99)\n",
    "    ax_leg1.set_ylim(0,0.99)\n",
    "    ax_leg1.yaxis.set_major_locator(MultipleLocator(0.33))\n",
    "    ax_leg1.xaxis.set_major_locator(MultipleLocator(0.33))\n",
    "    ax_leg1.tick_params(axis='y', labelsize=fontsize_leg, pad=1)\n",
    "    ax_leg1.tick_params(axis='x', labelsize=fontsize_leg, pad=1)\n",
    "    ax_leg1.set_xticks([0, 0.33, 0.66, 0.99], [str(mmSLE_1), str(mmSLE_2), str(mmSLE_3), str(np.round(gdf_bi['slr_mmSLE'].max(),1))])\n",
    "    ax_leg1.set_yticks([0, 0.33, 0.66, 0.99], [str(int(np.round(100*frac_1,0))), str(int(np.round(100*frac_2,0))), str(int(np.round(100*frac_3,0))), '100'])\n",
    "\n",
    "    # rect_hatch = patches.Rectangle((-0.1, 1.15), 0.33, 0.12, hatch='////////', facecolor='white', edgecolor='k', clip_on=False)\n",
    "    # ax_leg1.text(0.28, 1.21, 'Endorheic', size=fontsize_leg, ha='left', va='center', transform=ax_leg1.transAxes)\n",
    "    rect_hatch = patches.Rectangle((1.25, 0), 0.25, 0.12, hatch='////////', facecolor='lightgrey', edgecolor='k', clip_on=False)\n",
    "    ax_leg1.text(1.62, 0.05, 'Endorheic', size=fontsize_leg, ha='left', va='center', transform=ax_leg1.transAxes)\n",
    "    ax_leg1.add_patch(rect_hatch)\n",
    "    \n",
    "    \n",
    "    # # TIMESERIES: LINE STYLE\n",
    "    # lw_leg = 1.5\n",
    "    # # axleg = plt.subplot(gs[13:27, 80:])\n",
    "    # axleg = plt.subplot(gs[8:22, 80:])\n",
    "    # axleg.plot([0,0],[0,0], color='k', linestyle='--', linewidth=lw_leg, label='at Terminus')\n",
    "    # axleg.plot([0,0],[0,0], color='k', linestyle='-', linewidth=lw_leg, label='at Ocean')\n",
    "    # axleg.legend(loc=(0.1, 0.1), fontsize=fontsize_leg-0.5, ncol=1, columnspacing=0.8, labelspacing=0.25, \n",
    "    #              handlelength=1.8, handletextpad=0.4, borderpad=0, frameon=False)\n",
    "    # axleg.set_axis_off()\n",
    "    # axleg.text(0.5, 0.75, 'Discharge Plots', size=fontsize_leg, ha='center', va='center', transform=axleg.transAxes)\n",
    "    # rect_legbox = patches.Rectangle((0, 0), 20, 20, facecolor='none', edgecolor='grey', linewidth=0.75, clip_on=False)\n",
    "    # axleg.add_patch(rect_legbox)\n",
    "\n",
    "    # ADD VALUES OF AMOUNT REDUCED OVER A GIVEN LEVEL\n",
    "    basins_onplot = list(gdf_bi.loc[gdf_bi['slr_mmSLE_reduction'] > 0.05, 'DN'].values)\n",
    "    basin_id_name = {3:'Caspian',\n",
    "                     14:'Ganges',\n",
    "                     23:'Indus',\n",
    "                     33:'W Kunlun Shan\\n- C Tien Shan', # 'W Kunlun Shan - Tien Shan'\n",
    "                     44:'Aral',\n",
    "                     47:'Syr Darya',\n",
    "                     132:'E Kunlun Shan\\n& Qilian Shan',\n",
    "                     226:'Copper',\n",
    "                     228:'E Tien Shan',\n",
    "                     317:'Tibetan\\nInterior\\nMountains',\n",
    "                    }\n",
    "    basin_text_xy = {3:[52, 55],\n",
    "                     14:[85.0,27.2],\n",
    "                     23:[73.7, 33.0],\n",
    "                     33:[105,55.5],\n",
    "                     44:[78,67],\n",
    "                     47:[60,70],\n",
    "                     132:[110, 46],\n",
    "                     226:[-144,57.5],\n",
    "                     228:[94,64],\n",
    "                     317:[90.5,17],\n",
    "                    }\n",
    "    \n",
    "    for basin_id in basins_onplot:\n",
    "        # basin_reduction_str = str(np.round(basin_reduction_dict[scenario][basin_id],2))\n",
    "        basin_reduction_str = \"{:.2f}\".format(basin_reduction_dict[scenario][basin_id])\n",
    "        basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "\n",
    "        linespacing=0.9\n",
    "        name_size = 6\n",
    "\n",
    "        if basin_id in [3, 14, 23]:\n",
    "            color_txt = 'white'\n",
    "        else:\n",
    "            color_txt = 'k'\n",
    "\n",
    "        if basin_id in [3, 14, 23, 33, 44, 47, 132, 228, 317]:\n",
    "            ax3.text(basin_text_xy[basin_id][0], basin_text_xy[basin_id][1], basin_id_name[basin_id] + '\\n(' + basin_reduction_str + ')', \n",
    "                     size=name_size, linespacing=linespacing, ha='center', va='center', color=color_txt, zorder=6)\n",
    "            if basin_id in [33]:\n",
    "                ax3.plot([88,98], [40.4,52.5], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [44]:\n",
    "                ax3.plot([76.5,77.5], [47,64.5], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [47]:\n",
    "                ax3.plot([66.3,60], [45,67.5], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [132]:\n",
    "                ax3.plot([98,104], [39.5,44], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [228]:\n",
    "                ax3.plot([82.8,90], [45.8,61], linewidth=0.5, color=color_txt, zorder=5)\n",
    "            elif basin_id in [317]:\n",
    "                ax3.plot([90,92.5], [32,21], linewidth=0.5, color=color_txt, zorder=5)\n",
    "\n",
    "        elif basin_id in [226]:\n",
    "            ax1.text(basin_text_xy[basin_id][0], basin_text_xy[basin_id][1], basin_id_name[basin_id] + '\\n(' + basin_reduction_str + ')', \n",
    "                     size=name_size, linespacing=linespacing, ha='center', va='center')            \n",
    "\n",
    "\n",
    "    # ----- Saving -----\n",
    "    # fig.set_size_inches(6,4)\n",
    "    fig_fp = results_fp + '_figures/'\n",
    "    if not os.path.exists(fig_fp):\n",
    "        os.makedirs(fig_fp)\n",
    "    fig.savefig(fig_fp + 'Fig3b_basins_bivariate_slr_and_frac_' + scenario + '.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6978b8-c7a1-458e-ba5e-e9fbe5761054",
   "metadata": {},
   "source": [
    "### Values of interest for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e55da-f9aa-4f3e-a77a-c2949415e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scenario in scenarios:\n",
    "for scenario in ['ssp245']:\n",
    "\n",
    "    # Load data\n",
    "    gdf = gdf_gt30km2.copy()\n",
    "    gdf['slr_mmSLE_reduction'] = gdf['DN'].map(basin_reduction_dict[scenario])\n",
    "    gdf['slr_mmSLE'] = gdf['DN'].map(basin_contribution_dict[scenario])\n",
    "    gdf['frac2ocean'] = gdf['DN'].map(basin_frac_dict[scenario])\n",
    "    gdf = gdf[gdf['slr_mmSLE_reduction'].notna()]\n",
    "    gdf['perc2ocean'] = 100*gdf['frac2ocean']\n",
    "\n",
    "print('basin_id, slr reduction (mmSLE), slr contribution (mmSLE), % to ocean')\n",
    "for basin_id in [23,14,226]:\n",
    "    print('  ', basin_id, \n",
    "          np.round(gdf.loc[gdf['DN']==basin_id, 'slr_mmSLE_reduction'].values[0],2),\n",
    "          np.round(gdf.loc[gdf['DN']==basin_id, 'slr_mmSLE'].values[0],2),\n",
    "          int(np.round(100*gdf.loc[gdf['DN']==basin_id, 'frac2ocean'].values[0],0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175953e-0726-424a-b96e-aec19f439bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_exor = gdf[gdf['endo_flag']==0]\n",
    "gdf_exor_minloss = gdf_exor[gdf_exor['slr_mmSLE'] > 0.01]\n",
    "gdf_exor_sort = gdf_exor_minloss.sort_values('perc2ocean')\n",
    "basin_ids_sorted_raw = gdf_exor_sort.DN.values\n",
    "\n",
    "basin_ids_sorted = []\n",
    "for basin_id in basin_ids_sorted_raw:\n",
    "    if not basin_id in basin_ids_sorted:\n",
    "        basin_ids_sorted.append(basin_id)\n",
    "\n",
    "for nid, basin_id in enumerate(basin_ids_sorted[0:10]):\n",
    "    print(nid, basin_id, \n",
    "          df_basins.loc[df_basins['ID']==basin_id,'Name'].values[0], \n",
    "          int(np.round(100*basin_frac_dict[scenario][basin_id],0)),\n",
    "          np.round(gdf.loc[gdf['DN']==basin_id, 'slr_mmSLE_reduction'].values[0],2),\n",
    "          np.round(gdf.loc[gdf['DN']==basin_id, 'slr_mmSLE'].values[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0344b95-3d5d-42a7-8d6b-dcb6ea5dd837",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_exor_sort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd2648-13bb-448b-8f2d-728e363a2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basins[df_basins['ID']==292]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f4439-0bf3-498e-a4e5-c1c310c0ed3e",
   "metadata": {},
   "source": [
    "#### Glacierized regions statistics for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de002c2d-4cd4-4e32-85f3-9af00ea22e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_print = [1, 2, 13, 14, 15]\n",
    "for reg in regions:\n",
    "    if reg in reg_print:\n",
    "        print('')\n",
    "    for scenario in scenarios:\n",
    "        reg_slr = reg_annual_slr_mmSLE_dict[scenario][reg][:,year_start_idx:]\n",
    "        reg_slr_raw = reg_annual_slr_mmSLE_raw_dict[scenario][reg][:,year_start_idx:]\n",
    "    \n",
    "        reg_slr_reduction = reg_slr_raw.sum(1) - reg_slr.sum(1)\n",
    "        reg_slr_reduction_median = np.median(reg_slr_reduction)\n",
    "        reg_slr_reduction_std = np.std(reg_slr_reduction)\n",
    "\n",
    "        if reg in reg_print:\n",
    "            print(reg, scenario, np.round(reg_slr_reduction_median,1), np.round(1.96*reg_slr_reduction_std,1))\n",
    "\n",
    "# Merge HMA together\n",
    "print(' ')\n",
    "for scenario in scenarios:\n",
    "    reg_slr = None\n",
    "    reg_slr_raw = None\n",
    "    for reg in [13, 14, 15]:\n",
    "        if reg_slr is None:\n",
    "            reg_slr = reg_annual_slr_mmSLE_dict[scenario][reg][:,year_start_idx:]\n",
    "            reg_slr_raw = reg_annual_slr_mmSLE_raw_dict[scenario][reg][:,year_start_idx:]\n",
    "        else:\n",
    "            reg_slr = reg_slr + reg_annual_slr_mmSLE_dict[scenario][reg][:,year_start_idx:]\n",
    "            reg_slr_raw = reg_slr_raw + reg_annual_slr_mmSLE_raw_dict[scenario][reg][:,year_start_idx:]\n",
    "\n",
    "    reg_slr_reduction = reg_slr_raw.sum(1) - reg_slr.sum(1)\n",
    "    reg_slr_reduction_median = np.median(reg_slr_reduction)\n",
    "    reg_slr_reduction_std = np.std(reg_slr_reduction)\n",
    "\n",
    "    print('HMA', scenario, np.round(reg_slr_reduction_median,1), np.round(1.96*reg_slr_reduction_std,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef120f8-b08e-4103-a9c8-0f2f9f05572d",
   "metadata": {},
   "source": [
    "### Endorheic glaciers by area in HMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df81c8-ae7a-4494-8181-3b6b1a44113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_df_wbasins = pd.read_csv(rgiids_wbasins_fn)\n",
    "group_dict = dict(zip(rgi_df_wbasins.RGIId, rgi_df_wbasins.MeritID))\n",
    "\n",
    "main_glac_rgi_hma = selectglaciersrgitable(rgi_regionsO1=[13,14,15])\n",
    "main_glac_rgi_hma['basin'] = [int(x) if ~np.isnan(x) else int(-9999) for x in main_glac_rgi_hma['RGIId'].map(group_dict)]\n",
    "basins_endorheic_dict = dict(zip(df_basins.ID.values, df_basins.Endorheic.values))\n",
    "main_glac_rgi_hma['endorheic'] = main_glac_rgi_hma['basin'].map(basins_endorheic_dict)\n",
    "print('Endorheic glaciers by area (HMA):', np.round(100*\n",
    "    main_glac_rgi_hma.loc[main_glac_rgi_hma['endorheic'] == 1, 'Area'].sum() / main_glac_rgi_hma['Area'].sum(),0), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57f01c-fe6a-459b-b422-788665a27241",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_glac_rgi_all = selectglaciersrgitable(rgi_regionsO1=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "main_glac_rgi_all['basin'] = [int(x) if ~np.isnan(x) else int(-9999) for x in main_glac_rgi_all['RGIId'].map(group_dict)]\n",
    "basins_endorheic_dict = dict(zip(df_basins.ID.values, df_basins.Endorheic.values))\n",
    "main_glac_rgi_all['endorheic'] = main_glac_rgi_all['basin'].map(basins_endorheic_dict)\n",
    "print('Endorheic glaciers by area (all):', np.round(100*\n",
    "    main_glac_rgi_all.loc[main_glac_rgi_all['endorheic'] == 1, 'Area'].sum() / main_glac_rgi_all['Area'].sum(),0), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fec660-6f86-4f53-990a-facd7244c255",
   "metadata": {},
   "source": [
    "#### Table S2: RGI Region Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fba112-46d1-4aeb-8751-5225132ac3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cns = ['Global'] + [rgi_reg_dict[reg] + ' (' + str(reg) + ')' for reg in regions]\n",
    "\n",
    "df_cns = []\n",
    "var_cns = ['SLR (mm SLE)', 'SLR lost (mm SLE)', '% SLR to ocean']\n",
    "for var_cn in var_cns:\n",
    "    for scenario in scenarios:\n",
    "        df_cns.append(var_cn + '-' + scenario)\n",
    "df_cns\n",
    "\n",
    "reg_supp_df = pd.DataFrame(np.zeros((len(reg_cns), len(df_cns))), columns=df_cns, index=reg_cns)\n",
    "\n",
    "# SLR stats\n",
    "for scenario in scenarios:\n",
    "    global_slr = None\n",
    "    global_slr_raw = None\n",
    "    for reg in regions:\n",
    "        reg_slr = reg_annual_slr_mmSLE_dict[scenario][reg][:,year_start_idx:]\n",
    "        reg_slr_raw = reg_annual_slr_mmSLE_raw_dict[scenario][reg][:,year_start_idx:]\n",
    "    \n",
    "        reg_slr_reduction = reg_slr_raw.sum(1) - reg_slr.sum(1)\n",
    "        reg_slr_reduction_median = np.median(reg_slr_reduction)\n",
    "        reg_slr_reduction_std = np.std(reg_slr_reduction)\n",
    "\n",
    "        # Record\n",
    "        reg_supp_df.loc[rgi_reg_dict[reg] + ' (' + str(reg) + ')' , 'SLR (mm SLE)' + '-' + scenario] = np.median(reg_slr_raw.sum(1))\n",
    "        reg_supp_df.loc[rgi_reg_dict[reg] + ' (' + str(reg) + ')' , 'SLR lost (mm SLE)' + '-' + scenario] = reg_slr_reduction_median\n",
    "        reg_supp_df.loc[rgi_reg_dict[reg] + ' (' + str(reg) + ')' , '% SLR to ocean' + '-' + scenario] = 100 - 100*(reg_slr_reduction_median / np.median(reg_slr_raw.sum(1)))\n",
    "        \n",
    "\n",
    "        # Global aggregation\n",
    "        if global_slr is None:\n",
    "            global_slr = reg_slr\n",
    "            global_slr_raw = reg_slr_raw\n",
    "        else:\n",
    "            global_slr = global_slr + reg_slr\n",
    "            global_slr_raw = global_slr_raw + reg_slr_raw\n",
    "\n",
    "    # Record Global\n",
    "    global_slr_reduction = global_slr_raw.sum(1) - global_slr.sum(1)\n",
    "    reg_supp_df.loc['Global', 'SLR (mm SLE)' + '-' + scenario] = np.median(global_slr_raw.sum(1))\n",
    "    reg_supp_df.loc['Global', 'SLR lost (mm SLE)' + '-' + scenario] = np.median(global_slr_reduction)\n",
    "    reg_supp_df.loc['Global', '% SLR to ocean' + '-' + scenario] = 100 - 100*(np.median(global_slr_reduction) / np.median(global_slr_raw.sum(1)))\n",
    "\n",
    "reg_supp_df.to_csv(results_fp + 'Table_S2_rgi_reg_stats.csv')\n",
    "reg_supp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c3e58-a56d-497c-8be1-c1d551c41c75",
   "metadata": {},
   "source": [
    "# Map Overview Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f466aab-3dc9-4246-a0eb-bc37bb1f36b0",
   "metadata": {},
   "source": [
    "### No Greenland And Antarctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96b15f-eb9d-4b26-8752-f65ea1f9c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ----- FIGURE: GLOBAL COMBINED -----\n",
    "scenario = 'ssp245'\n",
    "\n",
    "add_rgi_glaciers = True\n",
    "add_rgi_regions = True\n",
    "\n",
    "continuous_cmap = cm.GnBu\n",
    "discrete_colors = continuous_cmap(np.linspace(0.1, 0.9, 8))\n",
    "cmap = ListedColormap(discrete_colors)\n",
    "perc_min = 20\n",
    "perc_max = 100\n",
    "\n",
    "class MidpointNormalize(mpl.colors.Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        mpl.colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        # Note that I'm ignoring clipping and other edge cases here.\n",
    "        result, is_scalar = self.process_value(value)\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.array(np.interp(value, x, y), mask=result.mask, copy=False)\n",
    "\n",
    "rgi_reg_fig_dict = {'all':'Global (excl. A+G)',\n",
    "                    1:'Alaska',\n",
    "                    2:'W Canada & US',\n",
    "                    3:'Arctic Canada N',\n",
    "                    4:'Arctic Canada S',\n",
    "                    5:'Greenland Periphery',\n",
    "                    6:'Iceland',\n",
    "                    7:'Svalbard',\n",
    "                    8:'Scandinavia',\n",
    "                    9:'Russian Arctic',\n",
    "                    10:'North Asia',\n",
    "                    11:'Central Europe',\n",
    "                    12:'Caucasus &\\nMiddle East',\n",
    "                    13:'Central Asia',\n",
    "                    14:'South Asia W',\n",
    "                    15:'South Asia E',\n",
    "                    16:'Low Latitudes',\n",
    "                    17:'Southern\\nAndes',\n",
    "                    18:'New Zealand',\n",
    "                    19:'Antarctic & Subantarctic'\n",
    "                    }\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Add background image\n",
    "ax_background = fig.add_axes([0,0.15,1,0.7], projection=ccrs.Robinson())\n",
    "ax_background.patch.set_facecolor('lightblue')\n",
    "ax_background.get_yaxis().set_visible(False)\n",
    "ax_background.get_xaxis().set_visible(False)\n",
    "ax_background.add_feature(cartopy.feature.LAND, color='white')\n",
    "\n",
    "# Add global boundary\n",
    "ax_global_patch = fig.add_axes([0.05,0.14,0.15,0.28], facecolor='lightblue')\n",
    "ax_global_patch.get_yaxis().set_visible(False)\n",
    "ax_global_patch.get_xaxis().set_visible(False)\n",
    "\n",
    "# Add basins\n",
    "gdf_plot = gdf_gt30km2.copy().to_crs(ccrs.Robinson().proj4_init)\n",
    "gdf_plot['perc2ocean'] = 100*gdf_plot['DN'].map(basin_frac_dict[scenario])\n",
    "gdf_plot['slr_mmSLE'] = gdf_plot['DN'].map(basin_contribution_dict[scenario])\n",
    "gdf_plot = gdf_plot.loc[gdf_plot['slr_mmSLE'] > 0.01]\n",
    "gdf_plot.plot(ax=ax_background, column='perc2ocean', cmap=cmap, vmin=perc_min, vmax=perc_max, edgecolor='gray', linewidth=0.25)\n",
    "\n",
    "# Add RGI glacier outlines\n",
    "if add_rgi_glaciers:\n",
    "    glac_color = 'k'\n",
    "    shape_feature = ShapelyFeature(Reader(rgi_shp_fn).geometries(), ccrs.Robinson(),alpha=1,facecolor=glac_color,\n",
    "                                   linewidth=0.1,edgecolor=glac_color)\n",
    "    ax_background.add_feature(shape_feature)\n",
    "    \n",
    "if add_rgi_regions:\n",
    "    gdf_rgiregs = gpd.read_file(rgi_regions_fn)\n",
    "    gdf_rgiregs = gdf_rgiregs.to_crs(ccrs.Robinson().proj4_init)\n",
    "    regions2include = [1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "    regions2include_idxs = []\n",
    "    for x in np.arange(gdf_rgiregs.shape[0]):\n",
    "        if gdf_rgiregs.loc[x, 'RGI_CODE'] in regions2include:\n",
    "            regions2include_idxs.append(x)\n",
    "    gdf_rgiregs = gdf_rgiregs.loc[regions2include_idxs,:]\n",
    "    gdf_rgiregs.plot(ax=ax_background, facecolor='None', edgecolor='k', linewidth=0.7)\n",
    "\n",
    "regions_ordered = ['all',1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "reg_pie_sizes = []\n",
    "for reg in regions_ordered:\n",
    "\n",
    "    if reg in ['all']:\n",
    "        reg_supp_df_idxname = 'Global'\n",
    "    else:\n",
    "        reg_supp_df_idxname = rgi_reg_dict[reg] + ' (' + str(reg) + ')'\n",
    "    \n",
    "    reg_slr_cum_pie = reg_supp_df.loc[reg_supp_df_idxname, 'SLR (mm SLE)' + '-' + scenario]\n",
    "    \n",
    "    pie_size_min = 0.07\n",
    "    if reg_slr_cum_pie > 80:\n",
    "        pie_size = 0.22\n",
    "    elif reg_slr_cum_pie > 25:\n",
    "        pie_size = 0.17\n",
    "    elif reg_slr_cum_pie < 1:\n",
    "        pie_size = pie_size_min\n",
    "    else:\n",
    "        pie_size = pie_size_min + (reg_slr_cum_pie - 1) / (25-1) * (0.17 - pie_size_min)\n",
    "    reg_pie_sizes.append(pie_size)\n",
    "\n",
    "ax0 = fig.add_axes([0.095,0.18,0.06,0.03], facecolor='none')\n",
    "ax1 = fig.add_axes([0.08,0.745,0.06,0.03], facecolor='none')\n",
    "ax2 = fig.add_axes([0.137,0.592,0.06,0.03], facecolor='none')\n",
    "ax3 = fig.add_axes([0.252,0.875,0.06,0.03], facecolor='none')\n",
    "ax4 = fig.add_axes([0.29,0.63,0.06,0.03], facecolor='none')\n",
    "ax6 = fig.add_axes([0.39,0.658,0.06,0.03], facecolor='none')\n",
    "ax7 = fig.add_axes([0.44,0.875,0.06,0.03], facecolor='none')\n",
    "ax8 = fig.add_axes([0.535,0.875,0.06,0.03], facecolor='none')\n",
    "ax9 = fig.add_axes([0.646,0.875,0.06,0.03], facecolor='none')\n",
    "ax10 = fig.add_axes([0.8,0.79,0.06,0.03], facecolor='none')\n",
    "ax11 = fig.add_axes([0.445,0.58,0.06,0.03], facecolor='none')\n",
    "ax12 = fig.add_axes([0.543,0.547,0.06,0.03], facecolor='none')\n",
    "ax13 = fig.add_axes([0.78,0.574,0.06,0.03], facecolor='none')\n",
    "ax14 = fig.add_axes([0.62,0.476,0.06,0.03], facecolor='none')\n",
    "ax15 = fig.add_axes([0.715,0.438,0.06,0.03], facecolor='none')\n",
    "ax16 = fig.add_axes([0.40,0.435,0.06,0.03], facecolor='none')\n",
    "ax17 = fig.add_axes([0.36,0.245,0.06,0.03], facecolor='none')\n",
    "ax18 = fig.add_axes([0.775,0.213,0.06,0.03], facecolor='none')\n",
    "\n",
    "# Pie charts\n",
    "ax0b = fig.add_axes([0.016,0.201,reg_pie_sizes[0],reg_pie_sizes[0]], facecolor='none')\n",
    "ax1b = fig.add_axes([0.025,0.768,reg_pie_sizes[1],reg_pie_sizes[1]], facecolor='none')\n",
    "ax2b = fig.add_axes([0.13,0.619,reg_pie_sizes[2],reg_pie_sizes[2]], facecolor='none')\n",
    "ax3b = fig.add_axes([0.23,0.901,reg_pie_sizes[3],reg_pie_sizes[3]], facecolor='none')\n",
    "ax4b = fig.add_axes([0.268,0.656,reg_pie_sizes[4],reg_pie_sizes[4]], facecolor='none')\n",
    "ax6b = fig.add_axes([0.38,0.685,reg_pie_sizes[5],reg_pie_sizes[5]], facecolor='none')\n",
    "ax7b = fig.add_axes([0.425,0.901,reg_pie_sizes[6],reg_pie_sizes[6]], facecolor='none')\n",
    "ax8b = fig.add_axes([0.528,0.902,reg_pie_sizes[7],reg_pie_sizes[7]], facecolor='none')\n",
    "ax9b = fig.add_axes([0.63,0.90,reg_pie_sizes[8],reg_pie_sizes[8]], facecolor='none')\n",
    "ax10b = fig.add_axes([0.794,0.817,reg_pie_sizes[9],reg_pie_sizes[9]], facecolor='none')\n",
    "ax11b = fig.add_axes([0.44,0.607,reg_pie_sizes[10],reg_pie_sizes[10]], facecolor='none')\n",
    "ax12b = fig.add_axes([0.538,0.574,reg_pie_sizes[11],reg_pie_sizes[11]], facecolor='none')\n",
    "ax13b = fig.add_axes([0.766,0.60,reg_pie_sizes[12],reg_pie_sizes[12]], facecolor='none')\n",
    "ax14b = fig.add_axes([0.612,0.503,reg_pie_sizes[13],reg_pie_sizes[13]], facecolor='none')\n",
    "ax15b = fig.add_axes([0.71,0.465,reg_pie_sizes[14],reg_pie_sizes[14]], facecolor='none')\n",
    "ax16b = fig.add_axes([0.395,0.453,reg_pie_sizes[15],reg_pie_sizes[15]], facecolor='none')\n",
    "ax17b = fig.add_axes([0.346,0.271,reg_pie_sizes[16],reg_pie_sizes[16]], facecolor='none')\n",
    "ax18b = fig.add_axes([0.772,0.24,reg_pie_sizes[17],reg_pie_sizes[17]], facecolor='none')\n",
    "\n",
    "# ----- Heat map of fraction reaching ocean (2015 - 2100) -----\n",
    "for nax, ax in enumerate([ax0, ax1, ax2, ax3, ax4, ax6, ax7, ax8, ax9, ax10,\n",
    "                          ax11, ax12, ax13, ax14, ax15, ax16, ax17, ax18]):\n",
    "\n",
    "    reg = regions_ordered[nax]\n",
    "\n",
    "    mesh = None\n",
    "    for hm_scenario in ['ssp126', 'ssp585']:\n",
    "\n",
    "        # Regional Fraction of SLR that directly reaches ocean\n",
    "        reg_slr = reg_annual_slr_mmSLE_dict[hm_scenario][reg][:,year_start_idx:]\n",
    "        reg_slr_raw = reg_annual_slr_mmSLE_raw_dict[hm_scenario][reg][:,year_start_idx:]\n",
    "        reg_perc_med = 100*np.median(reg_slr / reg_slr_raw, axis=0)\n",
    "    \n",
    "        if mesh is None:\n",
    "            mesh = reg_perc_med[np.newaxis,:]\n",
    "        else:\n",
    "            mesh = np.concatenate((mesh, reg_perc_med[np.newaxis,:]), axis=0)\n",
    "        \n",
    "    ax.imshow(mesh, aspect='auto', cmap=cmap, vmin=perc_min, vmax=perc_max, interpolation='none')\n",
    "    ax.set_xlim(0,85)\n",
    "    ax.hlines(0.5,0,mesh.shape[1]-1, color='k', linewidth=0.5, zorder=2)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.set_xticks([35,85])\n",
    "    ax.set_xticks(np.arange(5,81,10), minor=True)\n",
    "    ax.tick_params(axis='both', which='major', direction='in', length=3, right=True, top=True)\n",
    "    ax.tick_params(axis='both', which='minor', direction='in', right=True, top=True)\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    # Add region label\n",
    "    ax.text(0.5, -0.22, rgi_reg_fig_dict[reg], size=8, horizontalalignment='center', \n",
    "            verticalalignment='top', transform=ax.transAxes, bbox=dict(facecolor='white', edgecolor='k', pad=2))\n",
    "    \n",
    "# ----- Pie Chart of Volume Remaining by end of century -----\n",
    "wedge_size = 0.22\n",
    "for nax, ax in enumerate([ax0b, ax1b, ax2b, ax3b, ax4b, ax6b, ax7b, ax8b, ax9b, ax10b,\n",
    "                          ax11b, ax12b, ax13b, ax14b, ax15b, ax16b, ax17b, ax18b]):\n",
    "    \n",
    "    reg = regions_ordered[nax]\n",
    "    \n",
    "    ssp_vol_remaining_pies = []\n",
    "    ssp_pie_radius = 1\n",
    "    scenario_groups = ['ssp126','ssp585']\n",
    "    for ngroup, pie_scenario in enumerate(scenario_groups[::-1]):\n",
    "        \n",
    "        # Mass remaining at 2100, relative to 2015\n",
    "        mass_year_startidx = np.where(mass_years == 2015)[0][0]\n",
    "        if reg not in ['all']:\n",
    "            reg_mass_annual_med = np.median(reg_mass_dict[pie_scenario][reg][:,mass_year_startidx:],axis=0)\n",
    "        else:\n",
    "            all_mass_annual = None\n",
    "            for globreg in [1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18]:\n",
    "                if all_mass_annual is None:\n",
    "                    all_mass_annual = reg_mass_dict[pie_scenario][globreg][:,mass_year_startidx:]\n",
    "                else:\n",
    "                    all_mass_annual = all_mass_annual + reg_mass_dict[pie_scenario][globreg][:,mass_year_startidx:]\n",
    "            reg_mass_annual_med = np.median(all_mass_annual, axis=0)\n",
    "                \n",
    "        mass_remaining = reg_mass_annual_med[-1] / reg_mass_annual_med[0]\n",
    "        ssp_vol_remaining_pies.append(mass_remaining)\n",
    "\n",
    "        if reg in ['all']:\n",
    "            reg_supp_df_idxname = 'Global'\n",
    "        else:\n",
    "            reg_supp_df_idxname = rgi_reg_dict[reg] + ' (' + str(reg) + ')'\n",
    "        reg_slr_sum = reg_supp_df.loc[reg_supp_df_idxname, 'SLR (mm SLE)' + '-' + pie_scenario]\n",
    "        reg_slr_red_sum = reg_supp_df.loc[reg_supp_df_idxname, 'SLR lost (mm SLE)' + '-' + pie_scenario]\n",
    "        frac_reduction = reg_slr_red_sum / reg_slr_sum\n",
    "        if frac_reduction < 0:\n",
    "            frac_reduction = 0\n",
    "        frac_remaining = 1 - frac_reduction\n",
    "\n",
    "        # Pie Charts\n",
    "        ssp_pies = [frac_remaining*(1-ssp_vol_remaining_pies[ngroup]),\n",
    "                    frac_reduction*(1-ssp_vol_remaining_pies[ngroup]),\n",
    "                    ssp_vol_remaining_pies[ngroup]]\n",
    "        ssp_pie_colors = [ssp_colordict[pie_scenario], ssp_colordict[pie_scenario], 'lightgray']\n",
    "        pie_slices, pie_labels = ax.pie(ssp_pies, radius=ssp_pie_radius,\n",
    "                                        counterclock=False, startangle=90, colors=ssp_pie_colors,\n",
    "                                        wedgeprops=dict(width=wedge_size, linewidth=0.1, edgecolor='gray')\n",
    "                                       )\n",
    "        for i, pie_slice in enumerate(pie_slices):\n",
    "            if i == 2:\n",
    "                pie_slice.set_edgecolor('gray')\n",
    "        for i, pie_slice in enumerate(pie_slices):\n",
    "            if i == 1:\n",
    "                pie_slice.set_hatch('xxxxxxxxxxxxxxxxx')\n",
    "                pie_slice.set_edgecolor('white')\n",
    "\n",
    "        ssp_pie_radius = ssp_pie_radius - wedge_size\n",
    "\n",
    "    ssp_pie_radius_fill = 1 - wedge_size*len(scenario_groups)\n",
    "    wedge_size_fill = ssp_pie_radius_fill\n",
    "    ssp_pies, ssp_pie_colors = [1], ['white'] # middle circle\n",
    "    pie_slices, pie_labels = ax.pie(ssp_pies, radius=ssp_pie_radius_fill,\n",
    "                                    counterclock=False, startangle=90, colors=ssp_pie_colors,\n",
    "                                    wedgeprops=dict(width=wedge_size_fill))\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    # CENTER TEXT\n",
    "    if reg in ['all']:\n",
    "        reg_supp_df_idxname = 'Global'\n",
    "    else:\n",
    "        reg_supp_df_idxname = rgi_reg_dict[reg] + ' (' + str(reg) + ')'\n",
    "\n",
    "    # # SLR contribution\n",
    "    # reg_slr_cum_pie = reg_supp_df.loc[reg_supp_df_idxname, 'SLR (mm SLE)' + '-' + scenario]\n",
    "    # # SLR lost\n",
    "    # reg_slr_cum_pie = reg_supp_df.loc[reg_supp_df_idxname, 'SLR lost (mm SLE)' + '-' + scenario]\n",
    "    # if reg_slr_cum_pie > 1:\n",
    "    #     reg_slr_str = str(int(np.round(reg_slr_cum_pie)))\n",
    "    # else:\n",
    "    #     reg_slr_str = ''\n",
    "    # if reg in ['all']:\n",
    "    #     reg_slr_str += '\\nmm SLE'\n",
    "    # ax.text(0.5, 0.5, reg_slr_str, size=10, color='k', horizontalalignment='center', \n",
    "    #         verticalalignment='center', transform=ax.transAxes)\n",
    "    \n",
    "    # Perc reaching ocean\n",
    "    reg_perc2ocean_pie = reg_supp_df.loc[reg_supp_df_idxname, '% SLR to ocean' + '-' + scenario]\n",
    "    reg_pie_str = str(int(np.round(reg_perc2ocean_pie)))\n",
    "    if not reg in ['all']:\n",
    "        ax.text(0.5, 0.5, reg_pie_str, size=8, color='k', horizontalalignment='center', \n",
    "                verticalalignment='center', transform=ax.transAxes)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, reg_pie_str + '%', size=10, color='k', horizontalalignment='center', \n",
    "                verticalalignment='center', transform=ax.transAxes)\n",
    "        \n",
    "    \n",
    "    # Add outer edge by adding new circle with desired properties\n",
    "    center = pie_slices[0].center\n",
    "    r = 1\n",
    "    circle = mpl.patches.Circle(center, r, fill=False, edgecolor=\"k\", linewidth=0.5)\n",
    "    ax.add_patch(circle)\n",
    "    \n",
    "    \n",
    "# ----- LEGEND -----\n",
    "# PIE LEGEND\n",
    "ax_circle3 = fig.add_axes([0.492,-0.02,0.13,0.13], facecolor='none')\n",
    "ssp_vol_remaining_pies = [0.31, 0.41]\n",
    "frac_lost = 0.2\n",
    "ssp_pie_radius = 1\n",
    "for ngroup, pie_scenario in enumerate(scenario_groups[::-1]):\n",
    "    # Nested Pie Charts\n",
    "    ssp_pies = [(1-frac_lost)*(1-ssp_vol_remaining_pies[ngroup]),\n",
    "                frac_lost*(1-ssp_vol_remaining_pies[ngroup]),\n",
    "                ssp_vol_remaining_pies[ngroup]]\n",
    "    ssp_pie_colors = [ssp_colordict[pie_scenario], ssp_colordict[pie_scenario], 'lightgray']\n",
    "    pie_slices, pie_labels = ax_circle3.pie(ssp_pies, radius=ssp_pie_radius, \n",
    "                                            counterclock=False, startangle=90, colors=ssp_pie_colors,\n",
    "                                            wedgeprops=dict(width=wedge_size, linewidth=0.2, edgecolor='k'))\n",
    "    ssp_pie_radius = ssp_pie_radius - wedge_size\n",
    "\n",
    "    for i, pie_slice in enumerate(pie_slices):\n",
    "        if i == 2:\n",
    "            pie_slice.set_edgecolor('gray')\n",
    "    for i, pie_slice in enumerate(pie_slices):\n",
    "        if i == 1:\n",
    "            pie_slice.set_hatch('xxxxxxxxxxxxxxxxx')\n",
    "            pie_slice.set_edgecolor('white')\n",
    "    \n",
    "ssp_pie_radius_fill = 1 - wedge_size*len(scenario_groups)\n",
    "wedge_size_fill = ssp_pie_radius_fill\n",
    "ssp_pies, ssp_pie_colors = [1], ['white']\n",
    "pie_slices, pie_labels = ax_circle3.pie(ssp_pies, radius=ssp_pie_radius_fill, \n",
    "                                        counterclock=False, startangle=90, colors=ssp_pie_colors,\n",
    "                                        wedgeprops=dict(width=wedge_size_fill))\n",
    "ax_circle3.axis('equal')\n",
    "\n",
    "center = pie_slices[0].center\n",
    "r = 1\n",
    "circle = mpl.patches.Circle(center, r, fill=False, edgecolor=\"k\", linewidth=1)\n",
    "ax_circle3.add_patch(circle)\n",
    "ax_circle3.text(0.2,0.21,'SSP5-8.5', color=ssp_colordict['ssp585'], size=8, \n",
    "                ha='right', transform=ax_circle3.transAxes)\n",
    "ax_circle3.text(0.26,0.01,'SSP1-2.6', color=ssp_colordict['ssp126'], size=8, \n",
    "                ha='right', transform=ax_circle3.transAxes)\n",
    "\n",
    "ax_background.text(0.63,-0.05,'Mass Loss at 2100 (rel. to 2015)', size=10, \n",
    "                   ha='center', transform=ax_background.transAxes)\n",
    "\n",
    "# hatch description\n",
    "ax_hatchleg = fig.add_axes([0.57,0.045,0.05,0.05], facecolor='none')\n",
    "hatch_rect1 = patches.Rectangle((0.6, 0.0), 0.4, 0.4, linewidth=0.5, edgecolor='k', facecolor=ssp_colordict['ssp126'], clip_on=False)\n",
    "hatch_rect2 = patches.Rectangle((0.8, 0.0), 0.2, 0.4, linewidth=0.5, edgecolor='k', facecolor=ssp_colordict['ssp585'], clip_on=False)\n",
    "hatch_rect3 = patches.Rectangle((0.62, 0.02), 0.36, 0.36, linewidth=0.5, edgecolor='white', facecolor='none', \n",
    "                                hatch='xxxxxxxx', clip_on=False)\n",
    "ax_hatchleg.add_patch(hatch_rect1)\n",
    "ax_hatchleg.add_patch(hatch_rect2)\n",
    "ax_hatchleg.add_patch(hatch_rect3)\n",
    "ax_hatchleg.set_axis_off()\n",
    "ax_hatchleg.text(1.05,0.18,'Not directly reaching ocean', size=8, ha='left', va='center', transform=ax_hatchleg.transAxes)\n",
    "\n",
    "solid_rect1 = patches.Rectangle((0.6, 0.5), 0.4, 0.4, linewidth=0.5, edgecolor='k', facecolor=ssp_colordict['ssp126'], clip_on=False)\n",
    "solid_rect2 = patches.Rectangle((0.8, 0.5), 0.2, 0.4, linewidth=0.5, edgecolor='k', facecolor=ssp_colordict['ssp585'], clip_on=False)\n",
    "ax_hatchleg.add_patch(solid_rect1)\n",
    "ax_hatchleg.add_patch(solid_rect2)\n",
    "ax_hatchleg.set_axis_off()\n",
    "ax_hatchleg.text(1.05,0.67,'Directly reaching ocean', size=8, ha='left', va='center', transform=ax_hatchleg.transAxes)\n",
    "\n",
    "ax_hatchleg.text(-1.1, 0.65,'Mass\\nremaining', color='gray', size=7, ha='right', va='center', transform=ax_hatchleg.transAxes)\n",
    "ax_pointer = fig.add_axes([0.565,0.045,0.05,0.05], facecolor='none')\n",
    "ax_pointer.plot([-0.7, -0.95], [0.5, 0.65], color='grey', linewidth=0.75, clip_on=False)\n",
    "ax_pointer.set_xlim(0,1)\n",
    "ax_pointer.set_ylim(0,1)\n",
    "ax_pointer.set_axis_off()\n",
    "\n",
    "# HEATMAP LEGEND\n",
    "ax_background.text(0.20,-0.06,'Glacier Runoff Directly Reaching Ocean (%)', size=10, \n",
    "                   horizontalalignment='center', transform=ax_background.transAxes)\n",
    "ax_heatmap = fig.add_axes([0.06,0.03,0.1,0.06], facecolor='none')\n",
    "ax_heatmap.hlines(0.5,2015,2100, color='k', linewidth=0.5, zorder=2)\n",
    "ax_heatmap.set_ylim(0,1)\n",
    "ax_heatmap.set_xlim(2015,2100)\n",
    "ax_heatmap.get_yaxis().set_visible(False)\n",
    "ax_heatmap.xaxis.set_major_locator(MultipleLocator(40))\n",
    "ax_heatmap.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "ax_heatmap.set_xticks(ticks=[2050, 2100])\n",
    "ax_heatmap.tick_params(axis='both', which='major', direction='inout', right=True, top=True)\n",
    "ax_heatmap.tick_params(axis='both', which='minor', direction='in', right=True, top=True)\n",
    "ax_heatmap.text(0.5,0.71,'SSP1-2.6', size=8, \n",
    "                horizontalalignment='center', verticalalignment='center', transform=ax_heatmap.transAxes)\n",
    "ax_heatmap.text(0.5,0.21,'SSP5-8.5', size=8, \n",
    "                horizontalalignment='center', verticalalignment='center', transform=ax_heatmap.transAxes)\n",
    "\n",
    "# Heat map colorbar\n",
    "# sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=perc_min, vmax=perc_max))\n",
    "sm._A = []\n",
    "cax = plt.axes([0.19, 0.06, 0.22, 0.015])\n",
    "cbar = plt.colorbar(sm, ax=ax, cax=cax, orientation='horizontal', extend='min')\n",
    "cax.xaxis.set_ticks_position('bottom')\n",
    "cax.xaxis.set_tick_params(pad=2)\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "# PIE SIZE LEGEND\n",
    "ax_background.text(0.9,-0.05,'mm SLE', size=10, \n",
    "                   ha='center', transform=ax_background.transAxes)\n",
    "pie_size = 0.128 # 15 mm SLE\n",
    "ax_piesizea = fig.add_axes([0.78,-0.025,pie_size,pie_size], facecolor='none')\n",
    "center = (0.5, 0)\n",
    "circle = mpl.patches.Circle(center, pie_size, fill=False, edgecolor=\"k\", linewidth=0.5, clip_on=False)\n",
    "ax_piesizea.add_patch(circle)\n",
    "ax_piesizea.axis('equal')\n",
    "ax_piesizea.set_axis_off()\n",
    "\n",
    "pie_size = pie_size_min # 1 mm SLE\n",
    "ax_piesizeb = fig.add_axes([0.809,-0.022,pie_size,pie_size], facecolor='none')\n",
    "center = (0.5, 0)\n",
    "circle = mpl.patches.Circle(center, pie_size, fill=False, edgecolor=\"k\", linewidth=0.5, clip_on=False)\n",
    "ax_piesizeb.add_patch(circle)\n",
    "ax_piesizeb.axis('equal')\n",
    "ax_piesizeb.set_axis_off()\n",
    "\n",
    "ax_pointer2 = fig.add_axes([0.78,-0.025,0.1,0.1], facecolor='none')\n",
    "ax_pointer2.plot([0.62, 1.2], [1.238, 1.238], color='gray', linewidth=0.75, clip_on=False)\n",
    "ax_pointer2.plot([0.62, 1.2], [0.72, 0.72], color='gray', linewidth=0.75, clip_on=False)\n",
    "ax_pointer2.text(1.2, 1.21, '15', size=8, ha='center', va='top', transform=ax_pointer2.transAxes)\n",
    "ax_pointer2.text(1.2, 0.70, '< 1', size=8, ha='center', va='top', transform=ax_pointer2.transAxes)\n",
    "ax_pointer2.set_xlim(0,1)\n",
    "ax_pointer2.set_ylim(0,1)\n",
    "ax_pointer2.set_axis_off()\n",
    "\n",
    "# Save figure\n",
    "fig.set_size_inches(8.5,5)\n",
    "fig.savefig(fig_fp + 'Fig2_map_slr_frac-' + scenario + '.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91751aba-4077-4ebd-af9f-53ae1bd67e9d",
   "metadata": {},
   "source": [
    "### Remove the pie charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe086d9-3aba-4e17-8480-80cc906f7618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ----- FIGURE: GLOBAL COMBINED -----\n",
    "scenario = 'ssp245'\n",
    "\n",
    "add_rgi_glaciers = True\n",
    "add_rgi_regions = True\n",
    "\n",
    "continuous_cmap = cm.GnBu\n",
    "discrete_colors = continuous_cmap(np.linspace(0.1, 0.9, 8))\n",
    "cmap = ListedColormap(discrete_colors)\n",
    "perc_min = 20\n",
    "perc_max = 100\n",
    "\n",
    "class MidpointNormalize(mpl.colors.Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        mpl.colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        # Note that I'm ignoring clipping and other edge cases here.\n",
    "        result, is_scalar = self.process_value(value)\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.array(np.interp(value, x, y), mask=result.mask, copy=False)\n",
    "\n",
    "rgi_reg_fig_dict = {'all':'Global (excl. A+G)',\n",
    "                    1:'Alaska',\n",
    "                    2:'W Canada & US',\n",
    "                    3:'Arctic Canada N',\n",
    "                    4:'Arctic Canada S',\n",
    "                    5:'Greenland Periphery',\n",
    "                    6:'Iceland',\n",
    "                    7:'Svalbard',\n",
    "                    8:'Scandinavia',\n",
    "                    9:'Russian Arctic',\n",
    "                    10:'North Asia',\n",
    "                    11:'Central Europe',\n",
    "                    12:'Caucasus &\\nMiddle East',\n",
    "                    13:'Central Asia',\n",
    "                    14:'South Asia W',\n",
    "                    15:'South Asia E',\n",
    "                    16:'Low Latitudes',\n",
    "                    17:'Southern\\nAndes',\n",
    "                    18:'New Zealand',\n",
    "                    19:'Antarctic & Subantarctic'\n",
    "                    }\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Add background image\n",
    "ax_background = fig.add_axes([0,0.15,1,0.7], projection=ccrs.Robinson())\n",
    "ax_background.patch.set_facecolor('lightblue')\n",
    "ax_background.get_yaxis().set_visible(False)\n",
    "ax_background.get_xaxis().set_visible(False)\n",
    "ax_background.add_feature(cartopy.feature.LAND, color='white')\n",
    "\n",
    "# # Add global boundary\n",
    "# ax_global_patch = fig.add_axes([0.05,0.14,0.15,0.28], facecolor='lightblue')\n",
    "# ax_global_patch.get_yaxis().set_visible(False)\n",
    "# ax_global_patch.get_xaxis().set_visible(False)\n",
    "\n",
    "# Add basins\n",
    "gdf_plot = gdf_gt30km2.copy().to_crs(ccrs.Robinson().proj4_init)\n",
    "gdf_plot['perc2ocean'] = 100*gdf_plot['DN'].map(basin_frac_dict[scenario])\n",
    "gdf_plot['slr_mmSLE'] = gdf_plot['DN'].map(basin_contribution_dict[scenario])\n",
    "gdf_plot = gdf_plot.loc[gdf_plot['slr_mmSLE'] > 0.01]\n",
    "gdf_plot.plot(ax=ax_background, column='perc2ocean', cmap=cmap, vmin=perc_min, vmax=perc_max, edgecolor='gray', linewidth=0.25)\n",
    "\n",
    "# Add RGI glacier outlines\n",
    "if add_rgi_glaciers:\n",
    "    glac_color = 'k'\n",
    "    shape_feature = ShapelyFeature(Reader(rgi_shp_fn).geometries(), ccrs.Robinson(),alpha=1,facecolor=glac_color,\n",
    "                                   linewidth=0.1,edgecolor=glac_color)\n",
    "    ax_background.add_feature(shape_feature)\n",
    "    \n",
    "if add_rgi_regions:\n",
    "    gdf_rgiregs = gpd.read_file(rgi_regions_fn)\n",
    "    gdf_rgiregs = gdf_rgiregs.to_crs(ccrs.Robinson().proj4_init)\n",
    "    regions2include = [1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "    regions2include_idxs = []\n",
    "    for x in np.arange(gdf_rgiregs.shape[0]):\n",
    "        if gdf_rgiregs.loc[x, 'RGI_CODE'] in regions2include:\n",
    "            regions2include_idxs.append(x)\n",
    "    gdf_rgiregs = gdf_rgiregs.loc[regions2include_idxs,:]\n",
    "    gdf_rgiregs.plot(ax=ax_background, facecolor='None', edgecolor='k', linewidth=0.7)\n",
    "\n",
    "regions_ordered = ['all',1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "reg_pie_sizes = []\n",
    "for reg in regions_ordered:\n",
    "\n",
    "    if reg in ['all']:\n",
    "        reg_supp_df_idxname = 'Global'\n",
    "    else:\n",
    "        reg_supp_df_idxname = rgi_reg_dict[reg] + ' (' + str(reg) + ')'\n",
    "    \n",
    "    reg_slr_cum_pie = reg_supp_df.loc[reg_supp_df_idxname, 'SLR (mm SLE)' + '-' + scenario]\n",
    "    \n",
    "    pie_size_min = 0.07\n",
    "    if reg_slr_cum_pie > 80:\n",
    "        pie_size = 0.22\n",
    "    elif reg_slr_cum_pie > 25:\n",
    "        pie_size = 0.17\n",
    "    elif reg_slr_cum_pie < 1:\n",
    "        pie_size = pie_size_min\n",
    "    else:\n",
    "        pie_size = pie_size_min + (reg_slr_cum_pie - 1) / (25-1) * (0.17 - pie_size_min)\n",
    "    reg_pie_sizes.append(pie_size)\n",
    "\n",
    "# Save figure\n",
    "fig.set_size_inches(8.5,5)\n",
    "fig.savefig(fig_fp + 'Fig2b_map_slr_frac-' + scenario + '-nopies.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b0e0d-f026-413c-a033-f3830d765ff0",
   "metadata": {},
   "source": [
    "### Remove just numbers in disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ecd521-2ef9-4be0-8c46-36986dc987d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ----- FIGURE: GLOBAL COMBINED -----\n",
    "scenario = 'ssp245'\n",
    "\n",
    "add_rgi_glaciers = True\n",
    "add_rgi_regions = True\n",
    "\n",
    "continuous_cmap = cm.GnBu\n",
    "discrete_colors = continuous_cmap(np.linspace(0.1, 0.9, 8))\n",
    "cmap = ListedColormap(discrete_colors)\n",
    "perc_min = 20\n",
    "perc_max = 100\n",
    "\n",
    "class MidpointNormalize(mpl.colors.Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        mpl.colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        # Note that I'm ignoring clipping and other edge cases here.\n",
    "        result, is_scalar = self.process_value(value)\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.array(np.interp(value, x, y), mask=result.mask, copy=False)\n",
    "\n",
    "rgi_reg_fig_dict = {'all':'Global (excl. A+G)',\n",
    "                    1:'Alaska',\n",
    "                    2:'W Canada & US',\n",
    "                    3:'Arctic Canada N',\n",
    "                    4:'Arctic Canada S',\n",
    "                    5:'Greenland Periphery',\n",
    "                    6:'Iceland',\n",
    "                    7:'Svalbard',\n",
    "                    8:'Scandinavia',\n",
    "                    9:'Russian Arctic',\n",
    "                    10:'North Asia',\n",
    "                    11:'Central Europe',\n",
    "                    12:'Caucasus &\\nMiddle East',\n",
    "                    13:'Central Asia',\n",
    "                    14:'South Asia W',\n",
    "                    15:'South Asia E',\n",
    "                    16:'Low Latitudes',\n",
    "                    17:'Southern\\nAndes',\n",
    "                    18:'New Zealand',\n",
    "                    19:'Antarctic & Subantarctic'\n",
    "                    }\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Add background image\n",
    "ax_background = fig.add_axes([0,0.15,1,0.7], projection=ccrs.Robinson())\n",
    "ax_background.patch.set_facecolor('lightblue')\n",
    "ax_background.get_yaxis().set_visible(False)\n",
    "ax_background.get_xaxis().set_visible(False)\n",
    "ax_background.add_feature(cartopy.feature.LAND, color='white')\n",
    "\n",
    "# Add global boundary\n",
    "ax_global_patch = fig.add_axes([0.05,0.14,0.15,0.28], facecolor='lightblue')\n",
    "ax_global_patch.get_yaxis().set_visible(False)\n",
    "ax_global_patch.get_xaxis().set_visible(False)\n",
    "\n",
    "# Add basins\n",
    "gdf_plot = gdf_gt30km2.copy().to_crs(ccrs.Robinson().proj4_init)\n",
    "gdf_plot['perc2ocean'] = 100*gdf_plot['DN'].map(basin_frac_dict[scenario])\n",
    "gdf_plot['slr_mmSLE'] = gdf_plot['DN'].map(basin_contribution_dict[scenario])\n",
    "gdf_plot = gdf_plot.loc[gdf_plot['slr_mmSLE'] > 0.01]\n",
    "gdf_plot.plot(ax=ax_background, column='perc2ocean', cmap=cmap, vmin=perc_min, vmax=perc_max, edgecolor='gray', linewidth=0.25)\n",
    "\n",
    "# Add RGI glacier outlines\n",
    "if add_rgi_glaciers:\n",
    "    glac_color = 'k'\n",
    "    shape_feature = ShapelyFeature(Reader(rgi_shp_fn).geometries(), ccrs.Robinson(),alpha=1,facecolor=glac_color,\n",
    "                                   linewidth=0.1,edgecolor=glac_color)\n",
    "    ax_background.add_feature(shape_feature)\n",
    "    \n",
    "if add_rgi_regions:\n",
    "    gdf_rgiregs = gpd.read_file(rgi_regions_fn)\n",
    "    gdf_rgiregs = gdf_rgiregs.to_crs(ccrs.Robinson().proj4_init)\n",
    "    regions2include = [1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "    regions2include_idxs = []\n",
    "    for x in np.arange(gdf_rgiregs.shape[0]):\n",
    "        if gdf_rgiregs.loc[x, 'RGI_CODE'] in regions2include:\n",
    "            regions2include_idxs.append(x)\n",
    "    gdf_rgiregs = gdf_rgiregs.loc[regions2include_idxs,:]\n",
    "    gdf_rgiregs.plot(ax=ax_background, facecolor='None', edgecolor='k', linewidth=0.7)\n",
    "\n",
    "regions_ordered = ['all',1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18]\n",
    "reg_pie_sizes = []\n",
    "for reg in regions_ordered:\n",
    "\n",
    "    if reg in ['all']:\n",
    "        reg_supp_df_idxname = 'Global'\n",
    "    else:\n",
    "        reg_supp_df_idxname = rgi_reg_dict[reg] + ' (' + str(reg) + ')'\n",
    "    \n",
    "    reg_slr_cum_pie = reg_supp_df.loc[reg_supp_df_idxname, 'SLR (mm SLE)' + '-' + scenario]\n",
    "    \n",
    "    pie_size_min = 0.07\n",
    "    if reg_slr_cum_pie > 80:\n",
    "        pie_size = 0.22\n",
    "    elif reg_slr_cum_pie > 25:\n",
    "        pie_size = 0.17\n",
    "    elif reg_slr_cum_pie < 1:\n",
    "        pie_size = pie_size_min\n",
    "    else:\n",
    "        pie_size = pie_size_min + (reg_slr_cum_pie - 1) / (25-1) * (0.17 - pie_size_min)\n",
    "    reg_pie_sizes.append(pie_size)\n",
    "\n",
    "# Pie charts\n",
    "ax0b = fig.add_axes([0.016,0.201,reg_pie_sizes[0],reg_pie_sizes[0]], facecolor='none')\n",
    "ax1b = fig.add_axes([0.025,0.768,reg_pie_sizes[1],reg_pie_sizes[1]], facecolor='none')\n",
    "ax2b = fig.add_axes([0.13,0.619,reg_pie_sizes[2],reg_pie_sizes[2]], facecolor='none')\n",
    "ax3b = fig.add_axes([0.23,0.901,reg_pie_sizes[3],reg_pie_sizes[3]], facecolor='none')\n",
    "ax4b = fig.add_axes([0.268,0.656,reg_pie_sizes[4],reg_pie_sizes[4]], facecolor='none')\n",
    "ax6b = fig.add_axes([0.38,0.685,reg_pie_sizes[5],reg_pie_sizes[5]], facecolor='none')\n",
    "ax7b = fig.add_axes([0.425,0.901,reg_pie_sizes[6],reg_pie_sizes[6]], facecolor='none')\n",
    "ax8b = fig.add_axes([0.528,0.902,reg_pie_sizes[7],reg_pie_sizes[7]], facecolor='none')\n",
    "ax9b = fig.add_axes([0.63,0.90,reg_pie_sizes[8],reg_pie_sizes[8]], facecolor='none')\n",
    "ax10b = fig.add_axes([0.794,0.817,reg_pie_sizes[9],reg_pie_sizes[9]], facecolor='none')\n",
    "ax11b = fig.add_axes([0.44,0.607,reg_pie_sizes[10],reg_pie_sizes[10]], facecolor='none')\n",
    "ax12b = fig.add_axes([0.538,0.574,reg_pie_sizes[11],reg_pie_sizes[11]], facecolor='none')\n",
    "ax13b = fig.add_axes([0.766,0.60,reg_pie_sizes[12],reg_pie_sizes[12]], facecolor='none')\n",
    "ax14b = fig.add_axes([0.612,0.503,reg_pie_sizes[13],reg_pie_sizes[13]], facecolor='none')\n",
    "ax15b = fig.add_axes([0.71,0.465,reg_pie_sizes[14],reg_pie_sizes[14]], facecolor='none')\n",
    "ax16b = fig.add_axes([0.395,0.453,reg_pie_sizes[15],reg_pie_sizes[15]], facecolor='none')\n",
    "ax17b = fig.add_axes([0.346,0.271,reg_pie_sizes[16],reg_pie_sizes[16]], facecolor='none')\n",
    "ax18b = fig.add_axes([0.772,0.24,reg_pie_sizes[17],reg_pie_sizes[17]], facecolor='none')\n",
    "    \n",
    "# ----- Pie Chart of Volume Remaining by end of century -----\n",
    "wedge_size = 0.22\n",
    "for nax, ax in enumerate([ax0b, ax1b, ax2b, ax3b, ax4b, ax6b, ax7b, ax8b, ax9b, ax10b,\n",
    "                          ax11b, ax12b, ax13b, ax14b, ax15b, ax16b, ax17b, ax18b]):\n",
    "    \n",
    "    reg = regions_ordered[nax]\n",
    "    \n",
    "    ssp_vol_remaining_pies = []\n",
    "    ssp_pie_radius = 1\n",
    "    scenario_groups = ['ssp126','ssp585']\n",
    "    for ngroup, pie_scenario in enumerate(scenario_groups[::-1]):\n",
    "        \n",
    "        # Mass remaining at 2100, relative to 2015\n",
    "        mass_year_startidx = np.where(mass_years == 2015)[0][0]\n",
    "        if reg not in ['all']:\n",
    "            reg_mass_annual_med = np.median(reg_mass_dict[pie_scenario][reg][:,mass_year_startidx:],axis=0)\n",
    "        else:\n",
    "            all_mass_annual = None\n",
    "            for globreg in [1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18]:\n",
    "                if all_mass_annual is None:\n",
    "                    all_mass_annual = reg_mass_dict[pie_scenario][globreg][:,mass_year_startidx:]\n",
    "                else:\n",
    "                    all_mass_annual = all_mass_annual + reg_mass_dict[pie_scenario][globreg][:,mass_year_startidx:]\n",
    "            reg_mass_annual_med = np.median(all_mass_annual, axis=0)\n",
    "                \n",
    "        mass_remaining = reg_mass_annual_med[-1] / reg_mass_annual_med[0]\n",
    "        ssp_vol_remaining_pies.append(mass_remaining)\n",
    "\n",
    "        if reg in ['all']:\n",
    "            reg_supp_df_idxname = 'Global'\n",
    "        else:\n",
    "            reg_supp_df_idxname = rgi_reg_dict[reg] + ' (' + str(reg) + ')'\n",
    "        reg_slr_sum = reg_supp_df.loc[reg_supp_df_idxname, 'SLR (mm SLE)' + '-' + pie_scenario]\n",
    "        reg_slr_red_sum = reg_supp_df.loc[reg_supp_df_idxname, 'SLR lost (mm SLE)' + '-' + pie_scenario]\n",
    "        frac_reduction = reg_slr_red_sum / reg_slr_sum\n",
    "        if frac_reduction < 0:\n",
    "            frac_reduction = 0\n",
    "        frac_remaining = 1 - frac_reduction\n",
    "\n",
    "        # Pie Charts\n",
    "        ssp_pies = [frac_remaining*(1-ssp_vol_remaining_pies[ngroup]),\n",
    "                    frac_reduction*(1-ssp_vol_remaining_pies[ngroup]),\n",
    "                    ssp_vol_remaining_pies[ngroup]]\n",
    "        ssp_pie_colors = [ssp_colordict[pie_scenario], ssp_colordict[pie_scenario], 'lightgray']\n",
    "        pie_slices, pie_labels = ax.pie(ssp_pies, radius=ssp_pie_radius,\n",
    "                                        counterclock=False, startangle=90, colors=ssp_pie_colors,\n",
    "                                        wedgeprops=dict(width=wedge_size, linewidth=0.1, edgecolor='gray')\n",
    "                                       )\n",
    "        for i, pie_slice in enumerate(pie_slices):\n",
    "            if i == 2:\n",
    "                pie_slice.set_edgecolor('gray')\n",
    "        for i, pie_slice in enumerate(pie_slices):\n",
    "            if i == 1:\n",
    "                pie_slice.set_hatch('xxxxxxxxxxxxxxxxx')\n",
    "                pie_slice.set_edgecolor('white')\n",
    "\n",
    "        ssp_pie_radius = ssp_pie_radius - wedge_size\n",
    "\n",
    "    ssp_pie_radius_fill = 1 - wedge_size*len(scenario_groups)\n",
    "    wedge_size_fill = ssp_pie_radius_fill\n",
    "    ssp_pies, ssp_pie_colors = [1], ['white'] # middle circle\n",
    "    pie_slices, pie_labels = ax.pie(ssp_pies, radius=ssp_pie_radius_fill,\n",
    "                                    counterclock=False, startangle=90, colors=ssp_pie_colors,\n",
    "                                    wedgeprops=dict(width=wedge_size_fill))\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    # CENTER TEXT\n",
    "    if reg in ['all']:\n",
    "        reg_supp_df_idxname = 'Global'\n",
    "    else:\n",
    "        reg_supp_df_idxname = rgi_reg_dict[reg] + ' (' + str(reg) + ')'\n",
    "\n",
    "    # # SLR contribution\n",
    "    # reg_slr_cum_pie = reg_supp_df.loc[reg_supp_df_idxname, 'SLR (mm SLE)' + '-' + scenario]\n",
    "    # # SLR lost\n",
    "    # reg_slr_cum_pie = reg_supp_df.loc[reg_supp_df_idxname, 'SLR lost (mm SLE)' + '-' + scenario]\n",
    "    # if reg_slr_cum_pie > 1:\n",
    "    #     reg_slr_str = str(int(np.round(reg_slr_cum_pie)))\n",
    "    # else:\n",
    "    #     reg_slr_str = ''\n",
    "    # if reg in ['all']:\n",
    "    #     reg_slr_str += '\\nmm SLE'\n",
    "    # ax.text(0.5, 0.5, reg_slr_str, size=10, color='k', horizontalalignment='center', \n",
    "    #         verticalalignment='center', transform=ax.transAxes)\n",
    "    \n",
    "    # Perc reaching ocean\n",
    "    reg_perc2ocean_pie = reg_supp_df.loc[reg_supp_df_idxname, '% SLR to ocean' + '-' + scenario]\n",
    "    reg_pie_str = str(int(np.round(reg_perc2ocean_pie)))\n",
    "    if not reg in ['all']:\n",
    "        ax.text(0.5, 0.5, reg_pie_str, size=8, color='k', horizontalalignment='center', \n",
    "                verticalalignment='center', transform=ax.transAxes)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, reg_pie_str + '%', size=10, color='k', horizontalalignment='center', \n",
    "                verticalalignment='center', transform=ax.transAxes)\n",
    "        \n",
    "    \n",
    "    # Add outer edge by adding new circle with desired properties\n",
    "    center = pie_slices[0].center\n",
    "    r = 1\n",
    "    circle = mpl.patches.Circle(center, r, fill=False, edgecolor=\"k\", linewidth=0.5)\n",
    "    ax.add_patch(circle)\n",
    "    \n",
    "    \n",
    "# ----- LEGEND -----\n",
    "# PIE LEGEND\n",
    "ax_circle3 = fig.add_axes([0.492,-0.02,0.13,0.13], facecolor='none')\n",
    "ssp_vol_remaining_pies = [0.31, 0.41]\n",
    "frac_lost = 0.2\n",
    "ssp_pie_radius = 1\n",
    "for ngroup, pie_scenario in enumerate(scenario_groups[::-1]):\n",
    "    # Nested Pie Charts\n",
    "    ssp_pies = [(1-frac_lost)*(1-ssp_vol_remaining_pies[ngroup]),\n",
    "                frac_lost*(1-ssp_vol_remaining_pies[ngroup]),\n",
    "                ssp_vol_remaining_pies[ngroup]]\n",
    "    ssp_pie_colors = [ssp_colordict[pie_scenario], ssp_colordict[pie_scenario], 'lightgray']\n",
    "    pie_slices, pie_labels = ax_circle3.pie(ssp_pies, radius=ssp_pie_radius, \n",
    "                                            counterclock=False, startangle=90, colors=ssp_pie_colors,\n",
    "                                            wedgeprops=dict(width=wedge_size, linewidth=0.2, edgecolor='k'))\n",
    "    ssp_pie_radius = ssp_pie_radius - wedge_size\n",
    "\n",
    "    for i, pie_slice in enumerate(pie_slices):\n",
    "        if i == 2:\n",
    "            pie_slice.set_edgecolor('gray')\n",
    "    for i, pie_slice in enumerate(pie_slices):\n",
    "        if i == 1:\n",
    "            pie_slice.set_hatch('xxxxxxxxxxxxxxxxx')\n",
    "            pie_slice.set_edgecolor('white')\n",
    "    \n",
    "ssp_pie_radius_fill = 1 - wedge_size*len(scenario_groups)\n",
    "wedge_size_fill = ssp_pie_radius_fill\n",
    "ssp_pies, ssp_pie_colors = [1], ['white']\n",
    "pie_slices, pie_labels = ax_circle3.pie(ssp_pies, radius=ssp_pie_radius_fill, \n",
    "                                        counterclock=False, startangle=90, colors=ssp_pie_colors,\n",
    "                                        wedgeprops=dict(width=wedge_size_fill))\n",
    "ax_circle3.axis('equal')\n",
    "\n",
    "center = pie_slices[0].center\n",
    "r = 1\n",
    "circle = mpl.patches.Circle(center, r, fill=False, edgecolor=\"k\", linewidth=1)\n",
    "ax_circle3.add_patch(circle)\n",
    "ax_circle3.text(0.2,0.21,'SSP5-8.5', color=ssp_colordict['ssp585'], size=8, \n",
    "                ha='right', transform=ax_circle3.transAxes)\n",
    "ax_circle3.text(0.26,0.01,'SSP1-2.6', color=ssp_colordict['ssp126'], size=8, \n",
    "                ha='right', transform=ax_circle3.transAxes)\n",
    "\n",
    "ax_background.text(0.63,-0.05,'Mass Loss at 2100 (rel. to 2015)', size=10, \n",
    "                   ha='center', transform=ax_background.transAxes)\n",
    "\n",
    "# hatch description\n",
    "ax_hatchleg = fig.add_axes([0.57,0.045,0.05,0.05], facecolor='none')\n",
    "hatch_rect1 = patches.Rectangle((0.6, 0.0), 0.4, 0.4, linewidth=0.5, edgecolor='k', facecolor=ssp_colordict['ssp126'], clip_on=False)\n",
    "hatch_rect2 = patches.Rectangle((0.8, 0.0), 0.2, 0.4, linewidth=0.5, edgecolor='k', facecolor=ssp_colordict['ssp585'], clip_on=False)\n",
    "hatch_rect3 = patches.Rectangle((0.62, 0.02), 0.36, 0.36, linewidth=0.5, edgecolor='white', facecolor='none', \n",
    "                                hatch='xxxxxxxx', clip_on=False)\n",
    "ax_hatchleg.add_patch(hatch_rect1)\n",
    "ax_hatchleg.add_patch(hatch_rect2)\n",
    "ax_hatchleg.add_patch(hatch_rect3)\n",
    "ax_hatchleg.set_axis_off()\n",
    "ax_hatchleg.text(1.05,0.18,'Not directly reaching ocean', size=8, ha='left', va='center', transform=ax_hatchleg.transAxes)\n",
    "\n",
    "solid_rect1 = patches.Rectangle((0.6, 0.5), 0.4, 0.4, linewidth=0.5, edgecolor='k', facecolor=ssp_colordict['ssp126'], clip_on=False)\n",
    "solid_rect2 = patches.Rectangle((0.8, 0.5), 0.2, 0.4, linewidth=0.5, edgecolor='k', facecolor=ssp_colordict['ssp585'], clip_on=False)\n",
    "ax_hatchleg.add_patch(solid_rect1)\n",
    "ax_hatchleg.add_patch(solid_rect2)\n",
    "ax_hatchleg.set_axis_off()\n",
    "ax_hatchleg.text(1.05,0.67,'Directly reaching ocean', size=8, ha='left', va='center', transform=ax_hatchleg.transAxes)\n",
    "\n",
    "ax_hatchleg.text(-1.1, 0.65,'Mass\\nremaining', color='gray', size=7, ha='right', va='center', transform=ax_hatchleg.transAxes)\n",
    "ax_pointer = fig.add_axes([0.565,0.045,0.05,0.05], facecolor='none')\n",
    "ax_pointer.plot([-0.7, -0.95], [0.5, 0.65], color='grey', linewidth=0.75, clip_on=False)\n",
    "ax_pointer.set_xlim(0,1)\n",
    "ax_pointer.set_ylim(0,1)\n",
    "ax_pointer.set_axis_off()\n",
    "\n",
    "# # HEATMAP LEGEND\n",
    "# ax_background.text(0.20,-0.06,'Glacier Runoff Directly Reaching Ocean (%)', size=10, \n",
    "#                    horizontalalignment='center', transform=ax_background.transAxes)\n",
    "# ax_heatmap = fig.add_axes([0.06,0.03,0.1,0.06], facecolor='none')\n",
    "# ax_heatmap.hlines(0.5,2015,2100, color='k', linewidth=0.5, zorder=2)\n",
    "# ax_heatmap.set_ylim(0,1)\n",
    "# ax_heatmap.set_xlim(2015,2100)\n",
    "# ax_heatmap.get_yaxis().set_visible(False)\n",
    "# ax_heatmap.xaxis.set_major_locator(MultipleLocator(40))\n",
    "# ax_heatmap.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "# ax_heatmap.set_xticks(ticks=[2050, 2100])\n",
    "# ax_heatmap.tick_params(axis='both', which='major', direction='inout', right=True, top=True)\n",
    "# ax_heatmap.tick_params(axis='both', which='minor', direction='in', right=True, top=True)\n",
    "# ax_heatmap.text(0.5,0.71,'SSP1-2.6', size=8, \n",
    "#                 horizontalalignment='center', verticalalignment='center', transform=ax_heatmap.transAxes)\n",
    "# ax_heatmap.text(0.5,0.21,'SSP5-8.5', size=8, \n",
    "#                 horizontalalignment='center', verticalalignment='center', transform=ax_heatmap.transAxes)\n",
    "\n",
    "# # Heat map colorbar\n",
    "# # sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "# sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=perc_min, vmax=perc_max))\n",
    "# sm._A = []\n",
    "# cax = plt.axes([0.19, 0.06, 0.22, 0.015])\n",
    "# cbar = plt.colorbar(sm, ax=ax, cax=cax, orientation='horizontal', extend='min')\n",
    "# cax.xaxis.set_ticks_position('bottom')\n",
    "# cax.xaxis.set_tick_params(pad=2)\n",
    "# cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "# PIE SIZE LEGEND\n",
    "ax_background.text(0.9,-0.05,'mm SLE', size=10, \n",
    "                   ha='center', transform=ax_background.transAxes)\n",
    "pie_size = 0.128 # 15 mm SLE\n",
    "ax_piesizea = fig.add_axes([0.78,-0.025,pie_size,pie_size], facecolor='none')\n",
    "center = (0.5, 0)\n",
    "circle = mpl.patches.Circle(center, pie_size, fill=False, edgecolor=\"k\", linewidth=0.5, clip_on=False)\n",
    "ax_piesizea.add_patch(circle)\n",
    "ax_piesizea.axis('equal')\n",
    "ax_piesizea.set_axis_off()\n",
    "\n",
    "pie_size = pie_size_min # 1 mm SLE\n",
    "ax_piesizeb = fig.add_axes([0.809,-0.022,pie_size,pie_size], facecolor='none')\n",
    "center = (0.5, 0)\n",
    "circle = mpl.patches.Circle(center, pie_size, fill=False, edgecolor=\"k\", linewidth=0.5, clip_on=False)\n",
    "ax_piesizeb.add_patch(circle)\n",
    "ax_piesizeb.axis('equal')\n",
    "ax_piesizeb.set_axis_off()\n",
    "\n",
    "ax_pointer2 = fig.add_axes([0.78,-0.025,0.1,0.1], facecolor='none')\n",
    "ax_pointer2.plot([0.62, 1.2], [1.238, 1.238], color='gray', linewidth=0.75, clip_on=False)\n",
    "ax_pointer2.plot([0.62, 1.2], [0.72, 0.72], color='gray', linewidth=0.75, clip_on=False)\n",
    "ax_pointer2.text(1.2, 1.21, '15', size=8, ha='center', va='top', transform=ax_pointer2.transAxes)\n",
    "ax_pointer2.text(1.2, 0.70, '< 1', size=8, ha='center', va='top', transform=ax_pointer2.transAxes)\n",
    "ax_pointer2.set_xlim(0,1)\n",
    "ax_pointer2.set_ylim(0,1)\n",
    "ax_pointer2.set_axis_off()\n",
    "\n",
    "# Save figure\n",
    "fig.set_size_inches(8.5,5)\n",
    "fig.savefig(fig_fp + 'Fig2c_map_slr_frac-' + scenario + '-noheatmap.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41375a-839e-4f21-8925-d5a9f8835cb5",
   "metadata": {},
   "source": [
    "# Percentage Reaching Ocean by Basin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd4b44-11d0-4d35-bf5f-b6b8e9b90df1",
   "metadata": {},
   "source": [
    "#### Select basins that will be included\n",
    "- named basins\n",
    "- greater than 30 km2 of glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576cf46-429a-4d92-bebf-7e8cbe592a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_ids_gt30km2 = list(np.unique(gdf_gt30km2.DN.values))\n",
    "df_basins_ids = list(df_basins.ID.values)\n",
    "basin_ids_gt30km2_idx = [df_basins_ids.index(x) for x in basin_ids_gt30km2]\n",
    "df_basins_gt30km2 = df_basins.loc[basin_ids_gt30km2_idx,:]\n",
    "df_basins_gt30km2_exor = df_basins_gt30km2.loc[df_basins_gt30km2['Endorheic']==0,:].copy()\n",
    "df_basins_gt30km2_exor['slr_mmSLE-ssp245'] = df_basins_gt30km2_exor['ID'].map(basin_contribution_dict['ssp245'])\n",
    "# Select only named basins\n",
    "name_boolean = []\n",
    "for x in list(df_basins_gt30km2_exor.Name.values):\n",
    "    try:\n",
    "        if np.isnan(x):\n",
    "            name_boolean.append(False)\n",
    "    except:\n",
    "        name_boolean.append(True)\n",
    "df_basins_gt30km2_exor_named = df_basins_gt30km2_exor.loc[name_boolean,:]\n",
    "\n",
    "df_basins_gt30km2_exor_named_minsle = df_basins_gt30km2_exor_named.loc[df_basins_gt30km2_exor_named['slr_mmSLE-ssp245']>0.01,:]\n",
    "df_basins_gt30km2_exor_named_minsle = df_basins_gt30km2_exor_named_minsle.sort_values('slr_mmSLE-ssp245', ascending=False)\n",
    "\n",
    "# Subset by continent\n",
    "df_basins_NA = df_basins_gt30km2_exor_named_minsle.loc[df_basins_gt30km2_exor_named_minsle['SubContinentName']=='North America']\n",
    "print('North America:', df_basins_NA.shape[0])\n",
    "\n",
    "df_basins_SA = df_basins_gt30km2_exor_named_minsle.loc[df_basins_gt30km2_exor_named_minsle['SubContinentName']=='South America']\n",
    "print('South America:', df_basins_SA.shape[0])\n",
    "\n",
    "df_basins_Europe = df_basins_gt30km2_exor_named_minsle.loc[df_basins_gt30km2_exor_named_minsle['SubContinentName']=='Europe']\n",
    "print('Europe:', df_basins_Europe.shape[0])\n",
    "\n",
    "df_basins_asia = df_basins_gt30km2_exor_named_minsle.loc[df_basins_gt30km2_exor_named_minsle['SubContinentName']=='Asia']\n",
    "print('Asia:', df_basins_asia.shape[0])\n",
    "\n",
    "df_basins_NZ = df_basins_gt30km2_exor_named_minsle.loc[df_basins_gt30km2_exor_named_minsle['SubContinentName']=='New Zealand']\n",
    "print('New Zealand:', df_basins_NZ.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd1add-b39c-4b61-8277-7a6e407e6bb8",
   "metadata": {},
   "source": [
    "## Plot North America and South America together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05617a70-3651-40cf-aad1-993f9db23943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basin_ids_subplot_raw = (list(df_basins_NA.ID.values) + list(df_basins_SA.ID.values) + list(df_basins_Europe.ID.values) + \n",
    "#                          list(df_basins_asia.ID.values) + list(df_basins_NZ.ID.values))\n",
    "basin_ids_subplot_raw = list(df_basins_NA.ID.values) + list(df_basins_SA.ID.values)\n",
    "# basin_ids_subplot_raw = list(df_basins_Europe.ID.values) + list(df_basins_asia.ID.values) + list(df_basins_NZ.ID.values)\n",
    "\n",
    "basin_ids_common = list(ds_slr_endo_merged.basin_ids_common.values)\n",
    "\n",
    "basin_ids_subplot = []\n",
    "for basin_id in basin_ids_subplot_raw:\n",
    "    if basin_id in basin_ids_common:\n",
    "        basin_ids_subplot.append(basin_id)\n",
    "\n",
    "len(basin_ids_subplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21734a62-0b63-4f9c-9542-7a65aea4faec",
   "metadata": {},
   "source": [
    "# Plot both the percent reaching ocean and the glacier runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de0421-f686-4635-b26f-44d8bc38214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 5, 4\n",
    "fontsize = 10\n",
    "fig, ax = plt.subplots(nrows,ncols, squeeze=False, sharex=False, sharey=False,\n",
    "                       gridspec_kw={'wspace':0.35, 'hspace':0.4})\n",
    "\n",
    "# ----- Load Data -----\n",
    "basin_ids_common = list(ds_slr_endo_merged.basin_ids_common.values)\n",
    "slr_mmSLE_common = ds_slr_endo_merged.slr_mmSLE_common.values[:,:,:,:]\n",
    "slr_mmSLE_common_raw = ds_slr_endo_merged.slr_mmSLE_common_raw.values[:,:,:,:]\n",
    "years = ds_slr_endo_merged.year.values\n",
    "\n",
    "\n",
    "\n",
    "nrow, ncol = 0, 0\n",
    "for nbasin, basin_id in enumerate(basin_ids_subplot):\n",
    "    \n",
    "    basin_idx = basin_ids_common.index(basin_id)\n",
    "    # ax2 = ax[nrow,ncol].twinx()\n",
    "    \n",
    "    for scenario_subplot in scenarios:\n",
    "        nscenario = scenarios.index(scenario_subplot)\n",
    "        \n",
    "        slr_mmSLE = slr_mmSLE_common[nscenario, :, basin_idx, :]\n",
    "        slr_mmSLE_raw = slr_mmSLE_common_raw[nscenario, :, basin_idx, :]\n",
    "        slr_mmSLE_frac = slr_mmSLE / slr_mmSLE_raw\n",
    "        slr_mmSLE_frac_median = uniform_filter(np.median(slr_mmSLE_frac, axis=0), size=(11))\n",
    "        \n",
    "        ax[nrow,ncol].plot(years, 100*slr_mmSLE_frac_median, color=ssp_colordict[scenario_subplot], \n",
    "                           linestyle='-', linewidth=1, zorder=2, label=None)\n",
    "\n",
    "    for scenario_subplot in scenarios:\n",
    "        # Add runoff\n",
    "        ds_wbm_fp = 'wbm_processed/basin_components_yearly-wibt-common/'\n",
    "        ds_wbm_fn = 'wbm_' + scenario_subplot + '_2016_2099_v4-basin_components_yearly-wibt-common.nc'\n",
    "        ds_wbm = xr.open_dataset(ds_wbm_fp + ds_wbm_fn)\n",
    "        years_wbm = ds_wbm.year.values\n",
    "        \n",
    "        basin_ids_single = [int(x.split('-')[0]) for x in ds_wbm.basin_ids.values]\n",
    "        basin_wbm_idx = basin_ids_single.index(basin_id)\n",
    "        \n",
    "        glac_runoff = ds_wbm.glmelt.values[:,basin_wbm_idx,:]\n",
    "        glac_runoff_med = uniform_filter(np.median(glac_runoff, axis=0), size=(11))\n",
    "        glac_runoff_norm = 100*glac_runoff_med/glac_runoff_med[:20].mean()\n",
    "\n",
    "        ax[nrow,ncol].plot(years_wbm, glac_runoff_norm, color=ssp_colordict[scenario_subplot], \n",
    "                           linestyle=':', linewidth=1, zorder=1, label=None)\n",
    "        ds_wbm.close()\n",
    "\n",
    "    # Labels\n",
    "    # ax1b.set_ylabel('Discharge (-)', size=fontsize, labelpad=1)\n",
    "    ymin, ymax = 0, 150\n",
    "    ax[nrow,ncol].set_ylim(ymin, ymax)\n",
    "    ax[nrow,ncol].tick_params(axis='y', labelsize=fontsize-1)\n",
    "    ax[nrow,ncol].tick_params(axis='x', labelsize=fontsize-1)\n",
    "    ax[nrow,ncol].yaxis.set_major_locator(MultipleLocator(50))\n",
    "    ax[nrow,ncol].yaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax[nrow,ncol].set_xlim(2015,2100)\n",
    "    ax[nrow,ncol].xaxis.set_major_locator(MultipleLocator(20))\n",
    "    ax[nrow,ncol].xaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax[nrow,ncol].set_xticks([2040, 2080], ['2040', '2080'])\n",
    "    ax[nrow,ncol].tick_params(axis='both', which='major', direction='in', right=True, pad=2)\n",
    "    ax[nrow,ncol].tick_params(axis='both', which='minor', direction='in', right=True)\n",
    "\n",
    "    # # Add Basin Name\n",
    "    basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "    if basin_name in ['Colorado (Argentina)']:\n",
    "        basin_name = 'Colorado (ARG)'\n",
    "    elif basin_name in ['Baker (Chile)']:\n",
    "        basin_name = 'Baker (CL)'\n",
    "    if basin_id in list(df_basins_NA.ID.values):\n",
    "        text_color = '#756bb1'\n",
    "    elif basin_id in list(df_basins_SA.ID.values):\n",
    "        text_color = '#31a354'\n",
    "    ax[nrow,ncol].text(1.0, 1.01, basin_name, size=fontsize, color=text_color, \n",
    "                       horizontalalignment='right', va='bottom', transform=ax[nrow,ncol].transAxes)\n",
    "\n",
    "    # Add Contribution (SSP2-4.5)\n",
    "    basin_contribution = basin_contribution_dict['ssp245'][basin_id]\n",
    "    basin_contribution_str = \"{:.2f}\".format(np.round(basin_contribution,2))\n",
    "    ax[nrow,ncol].text(0.06, 0.085, basin_contribution_str, size=fontsize-2, ha='left', va='bottom', \n",
    "                       transform=ax[nrow,ncol].transAxes)\n",
    "\n",
    "    # # Add Basin Frac2Ocean\n",
    "    # basin_frac2ocean_str = str(int(np.round(100*basin_frac_dict[scenario][basin_id],0))) + '%'\n",
    "    # ax1b.text(0.95, 0.97, basin_frac2ocean_str, size=fontsize, horizontalalignment='right', va='top', \n",
    "    #           transform=ax1b.transAxes)\n",
    "                    \n",
    "        \n",
    "    ncol += 1\n",
    "    if ncol%ncols == 0:\n",
    "        ncol = 0\n",
    "        nrow += 1\n",
    "\n",
    "while nbasin < nrows*ncols-1:\n",
    "    print(nbasin, nrow, ncol)\n",
    "    ax[nrow,ncol].set_axis_off()\n",
    "    # ax[nrow, ncol].axis('off')\n",
    "    nbasin += 1\n",
    "    ncol += 1\n",
    "    if ncol%ncols == 0:\n",
    "        ncol = 0\n",
    "        nrow += 1\n",
    "\n",
    "# # Legend\n",
    "# labels = ['SSP1-2.6', 'SSP2-4.5', 'SSP3-7.0', 'SSP5-8.5']\n",
    "# ax[4,2].legend(loc=(1.4,0.15), labels=labels, fontsize=fontsize-1, ncol=1, columnspacing=0.5, labelspacing=0.25, \n",
    "#                handlelength=1, handletextpad=0.25, borderpad=0, frameon=False\n",
    "#                )\n",
    "\n",
    "leg_lines = []\n",
    "leg_labels = []\n",
    "\n",
    "# add percent & runoff\n",
    "line = Line2D([0],[0], color='grey', linestyle='-', linewidth=1)\n",
    "leg_lines.append(line)\n",
    "leg_labels.append('Mass loss\\nreaching ocean')\n",
    "line = Line2D([0],[0], color='grey', linestyle=':', linewidth=1)\n",
    "leg_lines.append(line)\n",
    "leg_labels.append('Glacier Runoff')\n",
    "ax[4,2].legend(leg_lines, leg_labels, loc=(1.3,0.65), fontsize=8, labelspacing=0.25, handlelength=1, \n",
    "               handletextpad=0.25, borderpad=0, ncol=1, columnspacing=0.5, frameon=False)\n",
    "\n",
    "# # append dummy extras (for column spacing if needed)\n",
    "# line = Line2D([0],[0], color='grey', linestyle='-', linewidth=0)\n",
    "# leg_lines.append(line)\n",
    "# leg_labels.append(' ')\n",
    "\n",
    "leg2_lines = []\n",
    "leg2_labels = []\n",
    "for scenario_subplot in scenarios:\n",
    "    line = Line2D([0],[0], color=ssp_colordict[scenario_subplot], linewidth=1)\n",
    "    leg2_lines.append(line)\n",
    "    leg2_labels.append(ssp_name_dict[scenario_subplot])\n",
    "\n",
    "\n",
    "\n",
    "ax[4,1].legend(leg2_lines, leg2_labels, loc=(2.8,-0.05), fontsize=8, labelspacing=0.25, handlelength=1, \n",
    "               handletextpad=0.25, borderpad=0, ncol=1, columnspacing=0.5, frameon=False)\n",
    "\n",
    "fig.text(0.05,0.5,'Percentage of Mass Loss Reaching Ocean (%)', \n",
    "         size=11, horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "fig.text(0.95,0.5,'Glacier Runoff (rel. to 2015-2035, %)', size=11, horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "\n",
    "# Save figure\n",
    "fig_fn = 'FigS3-basins_percent2ocean_NA_SA.png'\n",
    "fig_fp = results_fp + '_figures/'\n",
    "if not os.path.exists(fig_fp):\n",
    "    os.makedirs(fig_fp)\n",
    "fig.set_size_inches(6.5,7)\n",
    "fig.savefig(fig_fp + fig_fn, bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955cbea3-28dc-444f-a576-0679e88ca3d8",
   "metadata": {},
   "source": [
    "## Plot Europe, Asia, and New Zealand together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89848310-72e6-4ff7-b6d0-4bd09c441137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basin_ids_subplot_raw = (list(df_basins_NA.ID.values) + list(df_basins_SA.ID.values) + list(df_basins_Europe.ID.values) + \n",
    "#                          list(df_basins_asia.ID.values) + list(df_basins_NZ.ID.values))\n",
    "# basin_ids_subplot_raw = list(df_basins_NA.ID.values) + list(df_basins_SA.ID.values)\n",
    "basin_ids_subplot_raw = list(df_basins_Europe.ID.values) + list(df_basins_asia.ID.values) + list(df_basins_NZ.ID.values)\n",
    "\n",
    "basin_ids_common = list(ds_slr_endo_merged.basin_ids_common.values)\n",
    "\n",
    "basin_ids_subplot = []\n",
    "for basin_id in basin_ids_subplot_raw:\n",
    "    if basin_id in basin_ids_common:\n",
    "        basin_ids_subplot.append(basin_id)\n",
    "\n",
    "len(basin_ids_subplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b062aa-828e-4756-9a3c-cba3913abf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 6, 3\n",
    "fontsize = 10\n",
    "fig, ax = plt.subplots(nrows, ncols, squeeze=False, sharex=False, sharey=False,\n",
    "                       gridspec_kw={'wspace':0.3, 'hspace':0.5})\n",
    "\n",
    "# ----- Load Data -----\n",
    "basin_ids_common = list(ds_slr_endo_merged.basin_ids_common.values)\n",
    "slr_mmSLE_common = ds_slr_endo_merged.slr_mmSLE_common.values[:,:,:,:]\n",
    "slr_mmSLE_common_raw = ds_slr_endo_merged.slr_mmSLE_common_raw.values[:,:,:,:]\n",
    "years = ds_slr_endo_merged.year.values\n",
    "\n",
    "nrow, ncol = 0, 0\n",
    "for nbasin, basin_id in enumerate(basin_ids_subplot):\n",
    "    \n",
    "    basin_idx = basin_ids_common.index(basin_id)\n",
    "    \n",
    "    for scenario_subplot in scenarios:\n",
    "        nscenario = scenarios.index(scenario_subplot)\n",
    "        \n",
    "        slr_mmSLE = slr_mmSLE_common[nscenario, :, basin_idx, :]\n",
    "        slr_mmSLE_raw = slr_mmSLE_common_raw[nscenario, :, basin_idx, :]\n",
    "        slr_mmSLE_frac = slr_mmSLE / slr_mmSLE_raw\n",
    "        slr_mmSLE_frac_median = uniform_filter(np.median(slr_mmSLE_frac, axis=0), size=(11))\n",
    "        \n",
    "        ax[nrow,ncol].plot(years, 100*slr_mmSLE_frac_median, color=ssp_colordict[scenario_subplot], \n",
    "                           linestyle='-', linewidth=1, zorder=2, label=None)\n",
    "\n",
    "    for scenario_subplot in scenarios:\n",
    "        # Add runoff\n",
    "        ds_wbm_fp = 'wbm_processed/basin_components_yearly-wibt-common/'\n",
    "        ds_wbm_fn = 'wbm_' + scenario_subplot + '_2016_2099_v4-basin_components_yearly-wibt-common.nc'\n",
    "        ds_wbm = xr.open_dataset(ds_wbm_fp + ds_wbm_fn)\n",
    "        years_wbm = ds_wbm.year.values\n",
    "        \n",
    "        basin_ids_single = [int(x.split('-')[0]) for x in ds_wbm.basin_ids.values]\n",
    "        basin_wbm_idx = basin_ids_single.index(basin_id)\n",
    "        \n",
    "        glac_runoff = ds_wbm.glmelt.values[:,basin_wbm_idx,:]\n",
    "        glac_runoff_med = uniform_filter(np.median(glac_runoff, axis=0), size=(11))\n",
    "        glac_runoff_norm = 100*glac_runoff_med/glac_runoff_med[:20].mean()\n",
    "\n",
    "        ax[nrow,ncol].plot(years_wbm, glac_runoff_norm, color=ssp_colordict[scenario_subplot], \n",
    "                           linestyle=':', linewidth=1, zorder=1, label=None)\n",
    "        ds_wbm.close()\n",
    "\n",
    "    # Labels\n",
    "    # ax1b.set_ylabel('Discharge (-)', size=fontsize, labelpad=1)\n",
    "    ymin, ymax = 0, 150\n",
    "    ax[nrow,ncol].set_ylim(ymin, ymax)\n",
    "    ax[nrow,ncol].tick_params(axis='y', labelsize=fontsize-1)\n",
    "    ax[nrow,ncol].tick_params(axis='x', labelsize=fontsize-1)\n",
    "    ax[nrow,ncol].yaxis.set_major_locator(MultipleLocator(50))\n",
    "    ax[nrow,ncol].yaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax[nrow,ncol].set_xlim(2015,2100)\n",
    "    ax[nrow,ncol].xaxis.set_major_locator(MultipleLocator(20))\n",
    "    ax[nrow,ncol].xaxis.set_minor_locator(MultipleLocator(10))\n",
    "    ax[nrow,ncol].set_xticks([2040, 2080], ['2040', '2080'])\n",
    "    ax[nrow,ncol].tick_params(axis='both', which='major', direction='in', right=True, pad=2)\n",
    "    ax[nrow,ncol].tick_params(axis='both', which='minor', direction='in', right=True)\n",
    "\n",
    "    # Add Basin Name\n",
    "    basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "    print(basin_id, basin_name)\n",
    "    if basin_id in list(df_basins_Europe.ID.values):\n",
    "        text_color = '#756bb1'\n",
    "    elif basin_id in list(df_basins_asia.ID.values):\n",
    "        text_color = '#31a354'\n",
    "    else:\n",
    "        text_color = '#636363'\n",
    "    ax[nrow,ncol].text(1.0, 1.01, basin_name, size=fontsize, color=text_color, \n",
    "                       horizontalalignment='right', va='bottom', transform=ax[nrow,ncol].transAxes)\n",
    "\n",
    "    # Add Contribution (SSP2-4.5)\n",
    "    basin_contribution = basin_contribution_dict['ssp245'][basin_id]\n",
    "    basin_contribution_str = \"{:.2f}\".format(np.round(basin_contribution,2))\n",
    "    ax[nrow,ncol].text(0.06, 0.085, basin_contribution_str, size=fontsize-2, horizontalalignment='left', va='bottom', \n",
    "                       transform=ax[nrow,ncol].transAxes)\n",
    "                    \n",
    "        \n",
    "    ncol += 1\n",
    "    if ncol%ncols == 0:\n",
    "        ncol = 0\n",
    "        nrow += 1\n",
    "\n",
    "while nbasin < nrows*ncols-1:\n",
    "    print(nbasin, nrow, ncol)\n",
    "    ax[nrow,ncol].set_axis_off()\n",
    "    # ax[nrow, ncol].axis('off')\n",
    "    nbasin += 1\n",
    "    ncol += 1\n",
    "    if ncol%ncols == 0:\n",
    "        ncol = 0\n",
    "        nrow += 1\n",
    "\n",
    "# # Legend\n",
    "# labels = ['SSP1-2.6', 'SSP2-4.5', 'SSP3-7.0', 'SSP5-8.5']\n",
    "# ax[4,2].legend(loc=(1.4,0.15), labels=labels, fontsize=fontsize-1, ncol=1, columnspacing=0.5, labelspacing=0.25, \n",
    "#                handlelength=1, handletextpad=0.25, borderpad=0, frameon=False\n",
    "#                )\n",
    "\n",
    "leg_lines = []\n",
    "leg_labels = []\n",
    "\n",
    "# add percent & runoff\n",
    "line = Line2D([0],[0], color='grey', linestyle=':', linewidth=1)\n",
    "leg_lines.append(line)\n",
    "leg_labels.append('Glacier Runoff')\n",
    "line = Line2D([0],[0], color='grey', linestyle='-', linewidth=1)\n",
    "leg_lines.append(line)\n",
    "leg_labels.append('Mass loss\\nreaching ocean')\n",
    "\n",
    "# append dummy extras (for column spacing if needed)\n",
    "line = Line2D([0],[0], color='grey', linestyle='-', linewidth=0)\n",
    "leg_lines.append(line)\n",
    "leg_labels.append(' ')\n",
    "line = Line2D([0],[0], color='grey', linestyle='-', linewidth=0)\n",
    "leg_lines.append(line)\n",
    "leg_labels.append(' ')\n",
    "\n",
    "for scenario_subplot in scenarios:\n",
    "    line = Line2D([0],[0], color=ssp_colordict[scenario_subplot], linewidth=1)\n",
    "    leg_lines.append(line)\n",
    "    leg_labels.append(ssp_name_dict[scenario_subplot])\n",
    "\n",
    "\n",
    "\n",
    "ax[5,0].legend(leg_lines, leg_labels, loc=(1.3,-0.1), fontsize=8, labelspacing=0.25, handlelength=1, \n",
    "               handletextpad=0.25, borderpad=0, ncol=2, columnspacing=0.5, frameon=False)\n",
    "\n",
    "fig.text(0.05,0.5,'Percentage of Mass Loss Reaching Ocean (%)', \n",
    "         size=11, horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "fig.text(0.95,0.5,'Glacier Runoff (rel. to 2015-2035, %)', size=11, horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "# Save figure\n",
    "fig_fn = 'FigS4-basins_percent2ocean_Europe_Asia_NZ.png'\n",
    "fig_fp = results_fp + '_figures/'\n",
    "if not os.path.exists(fig_fp):\n",
    "    os.makedirs(fig_fp)\n",
    "fig.set_size_inches(6.5,7)\n",
    "fig.savefig(fig_fp + fig_fn, bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abde70-ac4d-4115-9bca-22adea23284d",
   "metadata": {},
   "source": [
    "### Statistics needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d54595-4004-418d-ba4e-6d1c8c7e935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_id = 14\n",
    "basin_idx = basin_ids_common.index(basin_id)\n",
    "scenario_subplot = 'ssp245'\n",
    "    \n",
    "nscenario = scenarios.index(scenario_subplot)\n",
    "slr_mmSLE = slr_mmSLE_common[nscenario, :, basin_idx, :]\n",
    "slr_mmSLE_raw = slr_mmSLE_common_raw[nscenario, :, basin_idx, :]\n",
    "slr_mmSLE_frac = slr_mmSLE / slr_mmSLE_raw\n",
    "slr_mmSLE_frac_median = uniform_filter(np.median(slr_mmSLE_frac, axis=0), size=(11))\n",
    "\n",
    "slr_mmSLE_2015_2100_Ganges_pt92 = (np.median(slr_mmSLE_raw[:,year_start_idx:], axis=0) * 0.92).sum()\n",
    "print('Assume frac remains at 0.92 (mm SLE 2015-2100):', np.round(slr_mmSLE_2015_2100_Ganges_pt92,2))\n",
    "slr_mmSLE_2015_2100_Ganges = (np.median(slr_mmSLE[:,year_start_idx:], axis=0)).sum()\n",
    "print('With speculative IBTs (mm SLE 2015-2100):', np.round(slr_mmSLE_2015_2100_Ganges,2))\n",
    "print('Difference:', np.round(slr_mmSLE_2015_2100_Ganges_pt92 - slr_mmSLE_2015_2100_Ganges,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8663b85-2c2c-4b30-8051-7a79d2b1e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.median(slr_mmSLE_raw[:,year_start_idx:], axis=0) * 0.92).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0d310-d4e3-4e85-9699-0029fb5d9d0c",
   "metadata": {},
   "source": [
    "# Individual Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef57d1ae-2a49-4b3d-b643-1ce37daa9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basins_asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42572d20-ab35-4756-affb-8969238b8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_ids_of_interest_exor = [28]\n",
    "for scenario in scenarios:\n",
    "\n",
    "    # ----- Load Data -----\n",
    "    ds_wbm_fp = 'wbm_processed/basin_components_yearly-wibt-common/'\n",
    "    ds_wbm_fn = 'wbm_' + scenario + '_2016_2099_v4-basin_components_yearly-wibt-common.nc'\n",
    "    ds_wbm = xr.open_dataset(ds_wbm_fp + ds_wbm_fn)\n",
    "\n",
    "    basin_ids_ds_wbm = list(ds_wbm.basin_ids.values)\n",
    "    basin_ids_first_ds_wbm = [x.split('-')[0] for x in basin_ids_ds_wbm]\n",
    "    \n",
    "    for basin_id in basin_ids_of_interest_exor:\n",
    "        basin_idx_wbm = basin_ids_first_ds_wbm.index(str(basin_id))\n",
    "    \n",
    "        basin_glmelt = ds_wbm.glmelt.values[:,basin_idx_wbm,:]\n",
    "        basin_glmelt_med = np.median(basin_glmelt, axis=0)\n",
    "    \n",
    "        basin_q_pg = ds_wbm.discharge_pg.values[:,basin_idx_wbm,:]\n",
    "        basin_q_pg_med = np.median(basin_q_pg, axis=0)\n",
    "    \n",
    "        years = ds_wbm.year.values\n",
    "    \n",
    "        # ----- PLOT TIME SERIES -----\n",
    "        fig, ax = plt.subplots(1,1, squeeze=False, sharex=False, sharey=False,\n",
    "                              gridspec_kw={'wspace':0.5, 'hspace':0.3})\n",
    "    \n",
    "        # Plot individual values\n",
    "        for ngcm in np.arange(basin_glmelt.shape[0]):\n",
    "            basin_glmelt_single = basin_glmelt[ngcm,:]\n",
    "            ax[0,0].plot(years, basin_glmelt_single, color='lightblue', lw=0.5, zorder=1)\n",
    "            basin_q_pg_single = basin_q_pg[ngcm,:]\n",
    "            ax[0,0].plot(years, basin_q_pg_single, color='springgreen', lw=0.5, zorder=1)\n",
    "            \n",
    "        ax[0,0].plot(years, basin_glmelt_med, color='navy', lw=1, zorder=2, label='Glacier Runoff')\n",
    "        ax[0,0].plot(years, basin_q_pg_med, color='darkgreen', lw=1, zorder=1, label='Glacier Runoff at Ocean')\n",
    "    \n",
    "        # Labels\n",
    "        ax[0,0].set_ylabel('Discharge (m3/s)', size=12)\n",
    "\n",
    "        ax[0,0].text(0.22, 1.01, ssp_name_dict[scenario], size=11, horizontalalignment='right', \n",
    "                     verticalalignment='bottom', transform=ax[0,0].transAxes)\n",
    "        \n",
    "        basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "        if basin_name in [np.nan]:\n",
    "            basin_name = str(basin_id)\n",
    "        ax[0,0].text(1.0, 1.01, basin_name, size=12, horizontalalignment='right', \n",
    "                     verticalalignment='bottom', transform=ax[0,0].transAxes)\n",
    "    \n",
    "        frac_med_str = str(int(np.round(np.median(basin_q_pg.sum(1) / basin_glmelt.sum(1))*100,0))) + '%'\n",
    "        ax[0,0].text(0.99, 0.98, frac_med_str, size=12, horizontalalignment='right', \n",
    "                     verticalalignment='top', transform=ax[0,0].transAxes)\n",
    "    \n",
    "        # Legend\n",
    "        ax[0,0].legend(loc=(0.03,0.83), fontsize=10, ncol=1, columnspacing=0.5, labelspacing=0.25, \n",
    "                       handlelength=1, handletextpad=0.25, borderpad=0, frameon=False)\n",
    "        \n",
    "        fig.set_size_inches(4,3)\n",
    "\n",
    "        fig_fp = results_fp + '_figures/_individual_basins/'\n",
    "        if not os.path.exists(fig_fp):\n",
    "            os.makedirs(fig_fp)\n",
    "        fig.savefig(fig_fp + basin_name + '_discharge_timeseries_' + scenario + '.png', dpi=500)\n",
    "        \n",
    "        # plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced437a-fd0e-4968-92d0-c2ab36de7787",
   "metadata": {},
   "source": [
    "## Supplementary Table S3: SLR contribution and reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfe2e0-bff0-405b-a44e-d3359df534cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_ids_subplot_raw = (list(df_basins_NA.ID.values) + list(df_basins_SA.ID.values) + list(df_basins_Europe.ID.values) + \n",
    "                         list(df_basins_asia.ID.values) + list(df_basins_NZ.ID.values))\n",
    "basin_ids_common = list(ds_slr_endo_merged.basin_ids_common.values)\n",
    "\n",
    "basin_ids_subplot = []\n",
    "for basin_id in basin_ids_subplot_raw:\n",
    "    if basin_id in basin_ids_common:\n",
    "        basin_ids_subplot.append(basin_id)\n",
    "\n",
    "print(basin_ids_subplot)\n",
    "\n",
    "basin_names_subplot = []\n",
    "for nbasin, basin_id in enumerate(basin_ids_subplot):\n",
    "    \n",
    "    basin_idx = basin_ids_common.index(basin_id)\n",
    "    basin_name = df_basins.loc[df_basins.ID == basin_id, 'Name'].values[0]\n",
    "    basin_names_subplot.append(basin_name)\n",
    "\n",
    "basin_ids_common = list(ds_slr_endo_merged.basin_ids_common.values)\n",
    "slr_mmSLE_common = ds_slr_endo_merged.slr_mmSLE_common.values[:,:,:,:]\n",
    "slr_mmSLE_common_raw = ds_slr_endo_merged.slr_mmSLE_common_raw.values[:,:,:,:]\n",
    "years = ds_slr_endo_merged.year.values\n",
    "\n",
    "year_start = 2016\n",
    "year_start_idx = np.where(years_mass == year_start)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b98e98-aa01-4c84-9a42-f5499f8e2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cns = []\n",
    "var_cns = ['SLR (mm SLE)', 'SLR lost (mm SLE)', '% SLR to ocean']\n",
    "for var_cn in var_cns:\n",
    "    for scenario in scenarios:\n",
    "        df_cns.append(var_cn + '-' + scenario)\n",
    "df_cns\n",
    "\n",
    "reg_supp_df = pd.DataFrame(np.zeros((len(basin_names_subplot), len(df_cns))), columns=df_cns, index=basin_names_subplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81631f18-5f25-4094-91a2-5884ad0a8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLR stats\n",
    "for nscenario, scenario in enumerate(scenarios):\n",
    "    for nbasin, basin_id in enumerate(basin_ids_subplot):\n",
    "        basin_name = basin_names_subplot[nbasin]\n",
    "        basin_idx = basin_ids_common.index(basin_id)\n",
    "\n",
    "        reg_slr = slr_mmSLE_common[nscenario, :, basin_idx, year_start_idx:]\n",
    "        reg_slr_raw = slr_mmSLE_common_raw[nscenario, :, basin_idx, year_start_idx:]\n",
    "\n",
    "        reg_slr_reduction = reg_slr_raw.sum(1) - reg_slr.sum(1)\n",
    "        reg_slr_reduction_median = np.median(reg_slr_reduction)\n",
    "        reg_slr_reduction_std = np.std(reg_slr_reduction)\n",
    "\n",
    "        # Record\n",
    "        reg_supp_df.loc[basin_name, 'SLR (mm SLE)' + '-' + scenario] = np.median(reg_slr_raw.sum(1))\n",
    "        reg_supp_df.loc[basin_name, 'SLR lost (mm SLE)' + '-' + scenario] = reg_slr_reduction_median\n",
    "        reg_supp_df.loc[basin_name, '% SLR to ocean' + '-' + scenario] = 100 - 100*(reg_slr_reduction_median / np.median(reg_slr_raw.sum(1)))\n",
    "        \n",
    "reg_supp_df.to_csv(results_fp + 'Table_S3_basin_stats.csv')\n",
    "reg_supp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aac7d0-4d89-4204-875d-1020580debd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
